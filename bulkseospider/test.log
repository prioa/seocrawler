2023-05-02 12:04:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:04:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:04:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:04:44 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:04:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:04:44 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:04:44 [scrapy.extensions.telnet] INFO: Telnet Password: dda586dcaea1c64e
2023-05-02 12:04:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:04:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:04:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:04:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:04:44 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:04:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:04:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://lisaglauer.com/robots.txt> from <GET http://lisaglauer.de/robots.txt>
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfs-europe.de/robots.txt> from <GET http://pfs-europe.de/robots.txt>
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.be/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-wohnzimmer.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mgvliederkranz-asbach.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sprachlos-ev-beratung.de/robots.txt> from <GET http://sprachlos-ev-beratung.de/robots.txt>
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://kaffeeundservice.at/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lahres.com/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hausegger.net/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.be> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mgvliederkranz-asbach.de> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lahres.com> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hausegger.net> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://efwe-art.at/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://bayern-immobilie.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://steigertaxi.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:04:44 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET http://lifttaxi.com> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2023-05-02 12:04:44 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "pfs-europe.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'pfs-europe.de'))])
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://karo-stimme.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lifttaxi.com/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mind-holiday.com/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mixable.media/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfs-europe.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-wohnzimmer.de> (referer: None)
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfs-europe.de/> from <GET http://pfs-europe.de>
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://kaliner-yoga.de/robots.txt> from <GET http://kaliner-yoga.de/robots.txt>
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.be>
{'id': '7', 'url': 'http://biotikon.be', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mixable.media> (referer: None)
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mgvliederkranz-asbach.de>
{'id': '11', 'url': 'http://mgvliederkranz-asbach.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 28 Jan 2009 10:53:37 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lahres.com>
{'id': '15', 'url': 'http://lahres.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 25 Dec 2019 08:39:49 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-teamblog.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hausegger.net>
{'id': '12', 'url': 'http://hausegger.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 12 Sep 2012 11:18:42 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://kaffeeundservice.at/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://kaffeeundservice.at/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfs-europe.de/> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-teamblog.de> (referer: None)
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://photos.hr-photo.derobots.txt> from <GET http://hr-photo.de/robots.txt>
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://efwe-art.at> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gbv-grosskarolinenfeld.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bayern-immobilie.de> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://coaching-im-alltag.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://steigertaxi.de> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 463 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 469 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 472 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 507 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 513 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://karo-stimme.de> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mind-holiday.com> (referer: None)
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-wohnzimmer.de>
{'id': '13', 'url': 'http://massivholz-wohnzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lifttaxi.com/> (referer: None)
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mixable.media>
{'id': '17', 'url': 'http://mixable.media', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.wassersportcenter-heiligenhafen.de/> from <GET http://wassersportcenter-heiligenhafen.de>
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.dienstleistung-solar.de/robots.txt> from <GET http://dienstleistung-solar.de/robots.txt>
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://pfs-europe.de/>
{'id': '3', 'url': 'https://pfs-europe.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 03 Aug 2019 10:52:54 GMT', 'tableLayout': True, 'ssl_name': 'fun4-u.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon-teamblog.de>
{'id': '23', 'url': 'http://biotikon-teamblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://billardcafe-suedpark.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://efwe-art.at>
{'id': '5', 'url': 'http://efwe-art.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bayern-immobilie.de>
{'id': '6', 'url': 'http://bayern-immobilie.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gbv-grosskarolinenfeld.de> (referer: None)
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://steigertaxi.de>
{'id': '8', 'url': 'http://steigertaxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET http://www.purpur.eatbu.com/?lang=de> from <GET http://billardcafe-suedpark.de>
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.com/robots.txt> (referer: None)
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '25', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://karo-stimme.de>
{'id': '9', 'url': 'http://karo-stimme.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> (referer: None)
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 463 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 469 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 472 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 507 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 513 without any user agent to enforce it on.
2023-05-02 12:04:44 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coaching-im-alltag.de> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.com> (referer: None)
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mind-holiday.com>
{'id': '4', 'url': 'http://mind-holiday.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lifttaxi.com/>
{'id': '1', 'url': 'http://lifttaxi.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaukus.org/robots.txt> (referer: None)
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/> from <GET http://www.wassersportcenter-heiligenhafen.de/>
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://lisaglauer.com/> from <GET http://lisaglauer.de>
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://seelenfeder.at/robots.txt> (referer: None)
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gbv-grosskarolinenfeld.de>
{'id': '19', 'url': 'http://gbv-grosskarolinenfeld.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET http://kaukus.org/index2.html> from <GET http://kaukus.org>
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kilthau.tech/robots.txt> (referer: None)
2023-05-02 12:04:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.info>
{'id': '31', 'url': 'http://wasistopc.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/> (referer: None)
2023-05-02 12:04:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://seelenfeder.at> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaukus.org/index2.html> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://illger.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://drey.info/robots.txt> from <GET http://drey.info/robots.txt>
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coaching-im-alltag.de>
{'id': '18', 'url': 'http://coaching-im-alltag.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://illger.de> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativ-opc.com>
{'id': '33', 'url': 'http://3nativ-opc.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 1 times): 503 Service Unavailable
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-star.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://byzickl.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dobbrunz.com/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gartengestaltung-brandner.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 781: invalid start byte
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kilthau.tech> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dobbrunz.com> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://byzickl.de> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.wassersportcenter-heiligenhafen.de/>
{'id': '16', 'url': 'https://www.wassersportcenter-heiligenhafen.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'wassersportcenter-heiligenhafen.de', 'ssl_start': '20230719235959Z', 'ssl_expire': '20230719235959Z'}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://seelenfeder.at>
{'id': '35', 'url': 'http://seelenfeder.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 10 Mar 2018 15:51:03 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fmz-frankershausen.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://drey.info/robots.txt> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kaukus.org/index2.html>
{'id': '32', 'url': 'http://kaukus.org/index2.html', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2009 20:14:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://illger.de>
{'id': '37', 'url': 'http://illger.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 07 Nov 2011 11:18:19 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 2 times): 503 Service Unavailable
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.sprachlos-ev-beratung.de/robots.txt> from <GET https://sprachlos-ev-beratung.de/robots.txt>
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://magnonics.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/> (referer: None)
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://drey.info/> from <GET http://drey.info>
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.dienstleistung-solar.de/> from <GET http://dienstleistung-solar.de>
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cellhomoeostasis.com/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://magnonics.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://drey.info/> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.purpur.eatbu.com/robots.txt> (referer: None)
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essen-auf-raedern-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://diekicktipper.de/robots.txt> from <GET http://diekicktipper.de/robots.txt>
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cellhomoeostasis.com> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-star.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ts-it-service.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kilthau.tech>
{'id': '34', 'url': 'http://kilthau.tech', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dobbrunz.com>
{'id': '40', 'url': 'http://dobbrunz.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 03 Jul 2016 18:53:33 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://byzickl.de>
{'id': '36', 'url': 'http://byzickl.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.purpur.eatbu.com/?lang=de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gesundheitsforum-norderstedt.com/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schoolscout24.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lisaglauer.com/>
{'id': '2', 'url': 'http://lisaglauer.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hgwimmobilien.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://magnonics.de>
{'id': '42', 'url': 'http://magnonics.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 26 Aug 2009 08:33:56 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dj-sb.com/robots.txt> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://drey.info/>
{'id': '38', 'url': 'https://drey.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 17 Mar 2014 08:12:11 GMT', 'tableLayout': True, 'ssl_name': 'drey.info', 'ssl_start': '20230705235959Z', 'ssl_expire': '20230705235959Z'}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gesundheitsforum-norderstedt.com> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schoolscout24.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-flurmoebel.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://malttec.net/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 1 times): 503 Service Unavailable
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cellhomoeostasis.com>
{'id': '41', 'url': 'http://cellhomoeostasis.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kanzlei-sauerwein.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dj-sb.com> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-star.de>
{'id': '39', 'url': 'http://yoga-star.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET https://teamtrzweb.de> from <GET http://malttec.net>
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-flurmoebel.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://diekicktipper.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yogashop-paderborn.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kanzlei-sauerwein.de> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.purpur.eatbu.com/?lang=de>
{'id': '30', 'url': 'http://www.purpur.eatbu.com/?lang=de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://diekicktipper.de/> from <GET http://diekicktipper.de>
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.net/robots.txt> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gesundheitsforum-norderstedt.com>
{'id': '48', 'url': 'http://gesundheitsforum-norderstedt.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 26 Mar 2012 15:32:11 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schoolscout24.de>
{'id': '46', 'url': 'http://schoolscout24.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 03 May 2011 19:33:17 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 2 times): 503 Service Unavailable
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gartengestaltung-brandner.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hgwimmobilien.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essen-auf-raedern-willebadessen.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.net> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ts-it-service.de> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dj-sb.com>
{'id': '47', 'url': 'http://dj-sb.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 28 Dec 2014 13:42:08 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-flurmoebel.de>
{'id': '50', 'url': 'http://massivholz-flurmoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://buehnen-taxi.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kanzlei-sauerwein.de>
{'id': '51', 'url': 'http://kanzlei-sauerwein.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 03 Jan 2010 02:56:47 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wetzel.work/robots.txt> (referer: None)
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 213 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 214 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-05-02 12:04:45 [protego] DEBUG: Rule at line 234 without any user agent to enforce it on.
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://teamtrzweb.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://diekicktipper.de/> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wetzel.work> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-bueromoebel.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yogashop-paderborn.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-online-fragen-ktb.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://goting-kliff53.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/> (failed 1 times): 503 Service Unavailable
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-bueromoebel.de> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gartengestaltung-brandner.de>
{'id': '28', 'url': 'http://gartengestaltung-brandner.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-online-fragen-ktb.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-pilavas.com/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fmz-frankershausen.de> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hgwimmobilien.de>
{'id': '43', 'url': 'http://hgwimmobilien.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://essen-auf-raedern-willebadessen.de>
{'id': '27', 'url': 'http://essen-auf-raedern-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.net>
{'id': '55', 'url': 'http://biotikon.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ts-it-service.de>
{'id': '45', 'url': 'http://ts-it-service.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://goting-kliff53.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bce-clan.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://buehnen-taxi.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenheim-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://diekicktipper.de/>
{'id': '44', 'url': 'https://diekicktipper.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'diekicktipper.de', 'ssl_start': '20230527235959Z', 'ssl_expire': '20230527235959Z'}
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wetzel.work>
{'id': '56', 'url': 'http://wetzel.work', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 14 Aug 2017 17:23:07 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sprachlos-ev-beratung.de/> from <GET http://sprachlos-ev-beratung.de>
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yogashop-paderborn.de>
{'id': '52', 'url': 'http://yogashop-paderborn.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://bavarian-starlights.com/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://xn--edelstahlscheckkartenhlle-0wc.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-makler.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-bueromoebel.de>
{'id': '59', 'url': 'http://massivholz-bueromoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzte-online-fragen-ktb.de>
{'id': '61', 'url': 'http://xn--rzte-online-fragen-ktb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fmz-frankershausen.de>
{'id': '20', 'url': 'http://fmz-frankershausen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bauelemente-riede.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.info/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ciber-man.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://goting-kliff53.de>
{'id': '60', 'url': 'http://goting-kliff53.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 27 Jul 2022 11:59:55 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bce-clan.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bauelemente-riede.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-pilavas.com> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.info> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ciber-man.de> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://buehnen-taxi.de>
{'id': '53', 'url': 'http://buehnen-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--edelstahlscheckkartenhlle-0wc.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://xn--holzmbel-experte-qwb.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://buehnentaxi.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://schluesselvereinzler.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenheim-willebadessen.de> (referer: None)
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/robots.txt> from <GET http://cyberfiber.me/robots.txt>
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mobile-mietstation.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 774: invalid start byte
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bce-clan.de>
{'id': '64', 'url': 'http://bce-clan.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bauelemente-riede.de>
{'id': '69', 'url': 'http://bauelemente-riede.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 16 May 2018 20:42:31 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/> (failed 2 times): 503 Service Unavailable
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--holzmbel-experte-qwb.de> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-pilavas.com>
{'id': '57', 'url': 'http://ouzo-pilavas.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://was-ist-opc.info>
{'id': '65', 'url': 'http://was-ist-opc.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ciber-man.de>
{'id': '70', 'url': 'http://ciber-man.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 20 Jan 2013 06:17:41 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-makler.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-markt.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 776: invalid start byte
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://huckbros.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://style-and-smile.com/robots.txt> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--edelstahlscheckkartenhlle-0wc.de>
{'id': '67', 'url': 'http://xn--edelstahlscheckkartenhlle-0wc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.sprachlos-ev-beratung.de/> from <GET https://sprachlos-ev-beratung.de/>
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-qigong-master.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://da365.de/robots.txt> from <GET http://da365.de/robots.txt>
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-jugendzimmer.de/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://huckbros.de> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://altenheim-willebadessen.de>
{'id': '58', 'url': 'http://altenheim-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://buehnentaxi.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-qigong-master.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ignaz-schwarzbach.eu/robots.txt> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schluesselvereinzler.de> (referer: None)
2023-05-02 12:04:45 [dubdev] ERROR: HttpError on https://www.dienstleistung-solar.de/
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-jugendzimmer.de> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--holzmbel-experte-qwb.de>
{'id': '74', 'url': 'http://xn--holzmbel-experte-qwb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mobile-mietstation.de> (referer: None)
2023-05-02 12:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ignaz-schwarzbach.eu> (referer: None)
2023-05-02 12:04:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-makler.de>
{'id': '63', 'url': 'http://arbeitsbuehnen-makler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://marco-k.com/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teamtrzweb.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gzlw.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-markt.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://marco-k.com> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://huckbros.de>
{'id': '78', 'url': 'http://huckbros.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Feb 2023 08:29:22 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.net/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://no-flush-niacin.com/robots.txt> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://buehnentaxi.de>
{'id': '68', 'url': 'http://buehnentaxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taichichuan-qigong-master.de>
{'id': '79', 'url': 'http://taichichuan-qigong-master.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schluesselvereinzler.de>
{'id': '76', 'url': 'http://schluesselvereinzler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-kinderzimmer.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gzlw.de> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-jugendzimmer.de>
{'id': '80', 'url': 'http://massivholz-jugendzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mobile-mietstation.de>
{'id': '71', 'url': 'http://mobile-mietstation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ignaz-schwarzbach.eu>
{'id': '81', 'url': 'http://ignaz-schwarzbach.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 04 Dec 2022 16:28:01 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://no-flush-niacin.com> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-kinderzimmer.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila.info/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobila24.com/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ganzheitlichesheilen.eu/robots.txt> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://teamtrzweb.de>
{'id': '49', 'url': 'https://teamtrzweb.de', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'biergartenteam.de', 'ssl_start': '20230609051948Z', 'ssl_expire': '20230609051948Z'}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://openlate.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila.info> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ganzheitlichesheilen.eu> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-markt.de>
{'id': '72', 'url': 'http://arbeitsbuehnen-markt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://marco-k.com>
{'id': '82', 'url': 'http://marco-k.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 23 Jul 2022 22:42:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobila24.com> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmedmichalzik.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.bavarian-starlights.com/> from <GET http://bavarian-starlights.com>
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gzlw.de>
{'id': '85', 'url': 'http://gzlw.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 28 Jan 2009 09:42:45 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kaliner-yoga.de/robots.txt> from <GET https://kaliner-yoga.de/robots.txt>
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://no-flush-niacin.com>
{'id': '86', 'url': 'http://no-flush-niacin.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmedmichalzik.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmichalzik.eu/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaffeeundservice.at> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://boncomputa.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-kinderzimmer.de>
{'id': '87', 'url': 'http://massivholz-kinderzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rztefragenonline-unb.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmichalzik.eu> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://www.bavarian-starlights.com/robots.txt> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila.info>
{'id': '83', 'url': 'http://casamobila.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rztefragenonline-unb.de> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ganzheitlichesheilen.eu>
{'id': '90', 'url': 'http://ganzheitlichesheilen.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobila24.com>
{'id': '89', 'url': 'http://casademobila24.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.org/robots.txt> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://drmedmichalzik.de>
{'id': '91', 'url': 'http://drmedmichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kaffeeundservice.at>
{'id': '14', 'url': 'http://kaffeeundservice.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.org> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://boncomputa.de> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://drmichalzik.eu>
{'id': '93', 'url': 'http://drmichalzik.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nows.tk/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hgwimmobilien.org/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://da365.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rztefragenonline-unb.de>
{'id': '95', 'url': 'http://xn--rztefragenonline-unb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nows.tk> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-nektar.com/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://edelstahlscheckkartenhuelle.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lift-taxi.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://westside-linedance.com/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wwb1.net/robots.txt> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.org>
{'id': '97', 'url': 'http://biotikon.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://boncomputa.de>
{'id': '88', 'url': 'http://boncomputa.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-friedrichwipfel.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.net> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westside-linedance.com> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://doc-blog.org/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wwb1.net> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-alexander-michalzik.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nows.tk>
{'id': '94', 'url': 'http://nows.tk', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 06 Jun 2018 13:57:15 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-friedrichwipfel.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://doc-blog.org> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hgwimmobilien.org> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-alexander-michalzik.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://strukturplatten.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://edelstahlscheckkartenhuelle.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-nektar.com> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tibetanische-klangschalen.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://openlate.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobilashop.eu/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://wvm-branchenportal.com/robots.txt> from <GET http://wvm-branchenportal.com/robots.txt>
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.net>
{'id': '75', 'url': 'http://govido.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://westside-linedance.com>
{'id': '99', 'url': 'http://westside-linedance.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 29 Jan 2020 19:02:17 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://strukturplatten.de> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wwb1.net>
{'id': '102', 'url': 'http://wwb1.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 30 Sep 2019 13:35:47 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobilashop.eu> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.sprachlos-ev-beratung.de/>
{'id': '10', 'url': 'https://www.sprachlos-ev-beratung.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'sprachlos-ev-beratung.de', 'ssl_start': '20230711235959Z', 'ssl_expire': '20230711235959Z'}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://sulden-apresski.com/robots.txt> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taichichuan-friedrichwipfel.de>
{'id': '103', 'url': 'http://taichichuan-friedrichwipfel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://doc-blog.org>
{'id': '104', 'url': 'http://doc-blog.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hgwimmobilien.org>
{'id': '92', 'url': 'http://hgwimmobilien.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dr-alexander-michalzik.de>
{'id': '105', 'url': 'http://dr-alexander-michalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://impax.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lift-taxi.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sulden-apresski.com> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://edelstahlscheckkartenhuelle.de>
{'id': '101', 'url': 'http://edelstahlscheckkartenhuelle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-nektar.com>
{'id': '96', 'url': 'http://ouzo-nektar.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ateliereichner.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tibetanische-klangschalen.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ateliereichner.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mastershaft.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://openlate.de>
{'id': '54', 'url': 'http://openlate.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://impax.de> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://strukturplatten.de>
{'id': '106', 'url': 'http://strukturplatten.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 04 Aug 2021 09:44:09 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobilashop.eu>
{'id': '110', 'url': 'http://casademobilashop.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lift-taxi.de>
{'id': '84', 'url': 'http://lift-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://sulden-apresski.com>
{'id': '111', 'url': 'http://sulden-apresski.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 13 Mar 2018 16:25:36 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wvm-branchenportal.com/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://da365.de/> from <GET http://da365.de>
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-esszimmer.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://marcodesigner.com/robots.txt> from <GET http://marcodesigner.com/robots.txt>
2023-05-02 12:04:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://wvm-branchenportal.com/> from <GET http://wvm-branchenportal.com>
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.bavarian-starlights.com/> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wvm-branchenportal.com/> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-esszimmer.de> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tibetanische-klangschalen.de>
{'id': '107', 'url': 'http://tibetanische-klangschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ateliereichner.de>
{'id': '114', 'url': 'http://ateliereichner.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://impax.de>
{'id': '100', 'url': 'http://impax.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tibetanischeklagschalen.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.info/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mastershaft.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "marcodesigner.com"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'marcodesigner.com'))])
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.bavarian-starlights.com/>
{'id': '66', 'url': 'http://www.bavarian-starlights.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-schlafzimmer.com/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://carcal.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://wvm-branchenportal.com/>
{'id': '108', 'url': 'https://wvm-branchenportal.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 14 Oct 2020 22:24:24 GMT', 'tableLayout': True, 'ssl_name': 'wvm-branchenportal.com', 'ssl_start': '20240307235959Z', 'ssl_expire': '20240307235959Z'}
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-esszimmer.de>
{'id': '118', 'url': 'http://massivholz-esszimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-schlafzimmer.com> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nowikow.org/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dr-michalzik.at/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://wahnfriet.de/robots.txt> (referer: None)
2023-05-02 12:04:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:04:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mastershaft.de>
{'id': '113', 'url': 'http://mastershaft.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tibetanischeklagschalen.de> (referer: None)
2023-05-02 12:04:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://heueu.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://heueu.de> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://djmirco.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-schlafzimmer.com>
{'id': '122', 'url': 'http://massivholz-schlafzimmer.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://style-and-smile.com> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://carcal.de> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://marcodesigner.com/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nowikow.org> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tibetanischeklagschalen.de>
{'id': '121', 'url': 'http://tibetanischeklagschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wahnfriet.de> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.org/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://marcodesigner.com/> from <GET http://marcodesigner.com>
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://el-toque-latino.com/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.info> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://heueu.de>
{'id': '125', 'url': 'http://heueu.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 12 Sep 2009 08:57:13 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.org> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hilcura.com/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.info/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://el-toque-latino.com> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://style-and-smile.com>
{'id': '77', 'url': 'http://style-and-smile.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://djmirco.de> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-michalzik.at> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://carcal.de>
{'id': '123', 'url': 'http://carcal.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://biotikon.es/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.info> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://da365.de/> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nowikow.org>
{'id': '119', 'url': 'http://nowikow.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexander-michalzik.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wahnfriet.de>
{'id': '120', 'url': 'http://wahnfriet.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.info>
{'id': '112', 'url': 'http://govido.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-online-fragen.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-alpha-ketoglutarat.org>
{'id': '126', 'url': 'http://arginin-alpha-ketoglutarat.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://die-goldene-cd.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://aufklebershop-geislingen.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexander-michalzik.de> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://el-toque-latino.com>
{'id': '129', 'url': 'http://el-toque-latino.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 07 Sep 2007 23:41:14 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-online-fragen.de> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://djmirco.de>
{'id': '127', 'url': 'http://djmirco.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nevaton.eu/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronamitarbeiterschutz.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://aufklebershop-geislingen.de> (referer: None)
2023-05-02 12:04:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> from <GET http://xn--wohlfhlwerkstatt-nzb.de/robots.txt>
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dr-michalzik.at>
{'id': '117', 'url': 'http://dr-michalzik.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.info>
{'id': '130', 'url': 'http://biotikon.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://da365.de/>
{'id': '62', 'url': 'https://da365.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'da365.de', 'ssl_start': '20230708235959Z', 'ssl_expire': '20230708235959Z'}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.es> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronamitarbeiterschutz.de> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://marcodesigner.com/> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://springblade.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://edv-frenzel.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://die-goldene-cd.de> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://alexander-michalzik.de>
{'id': '133', 'url': 'http://alexander-michalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tribulus-saponine.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arzt-online-fragen.de>
{'id': '134', 'url': 'http://arzt-online-fragen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://aufklebershop-geislingen.de>
{'id': '135', 'url': 'http://aufklebershop-geislingen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Jul 2010 08:21:56 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tribulus-saponine.de> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.es>
{'id': '128', 'url': 'http://biotikon.es', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinoline-quinone.org/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://delta-mb.de/robots.txt> from <GET http://delta-mb.de/robots.txt>
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-dosierung.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coronamitarbeiterschutz.de>
{'id': '137', 'url': 'http://coronamitarbeiterschutz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://marcodesigner.com/>
{'id': '115', 'url': 'https://marcodesigner.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'die-rohrreinigung.com', 'ssl_start': '20210714235959Z', 'ssl_expire': '20210714235959Z'}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinoline-quinone.org> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-dosierung.de> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://die-goldene-cd.de>
{'id': '131', 'url': 'http://die-goldene-cd.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://springblade.de> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://edv-frenzel.de> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tribulus-saponine.de>
{'id': '142', 'url': 'http://tribulus-saponine.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://streamify.tv/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> (referer: None)
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://formergy.biz/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:04:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> from <GET http://xn--wohlfhlwerkstatt-nzb.de>
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://patcy.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pyrroloquinoline-quinone.org>
{'id': '140', 'url': 'http://pyrroloquinoline-quinone.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-blog.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://resveratrol-dosierung.de>
{'id': '145', 'url': 'http://resveratrol-dosierung.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://springblade.de>
{'id': '139', 'url': 'http://springblade.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://edv-frenzel.de>
{'id': '141', 'url': 'http://edv-frenzel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-blog.de> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.health/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hilcura.de/> from <GET http://hilcura.com>
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://patcy.de> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.health> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://formergy.biz> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nevaton.eu> (referer: None)
2023-05-02 12:04:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://huckbros.com/robots.txt> from <GET http://huckbros.com/robots.txt>
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon-blog.de>
{'id': '149', 'url': 'http://biotikon-blog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://kaliner-yoga.de/> from <GET http://kaliner-yoga.de>
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://crazybulls.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://patcy.de>
{'id': '148', 'url': 'http://patcy.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.health>
{'id': '150', 'url': 'http://biotikon.health', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://formergy.biz>
{'id': '143', 'url': 'http://formergy.biz', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://crazybulls.de> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fusionstreet.com/robots.txt> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nevaton.eu>
{'id': '98', 'url': 'http://nevaton.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://huckbros.com/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kindz.de/robots.txt> (referer: None)
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://pixxpress.com/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lifttaxi.eu/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:04:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://huckbros.com/> from <GET http://huckbros.com>
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www6.xn--wohlfhlwerkstatt-nzb.de/>
{'id': '138', 'url': 'http://www6.xn--wohlfhlwerkstatt-nzb.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfotenabdruck.info/robots.txt> from <GET http://pfotenabdruck.info/robots.txt>
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pixxpress.com> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://huckbros.com/> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://crazybulls.de>
{'id': '153', 'url': 'http://crazybulls.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Dec 2022 21:55:50 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://abandonedzone.com/robots.txt> (referer: None)
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:47 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://abandonedzone.com> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.com/robots.txt> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pixxpress.com>
{'id': '155', 'url': 'http://pixxpress.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 26 Aug 2011 20:30:29 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://pfotenabdruck.info/robots.txt> (referer: None)
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fusionstreet.com> (referer: None)
2023-05-02 12:04:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://huckbros.com/>
{'id': '151', 'url': 'https://huckbros.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Feb 2023 08:29:22 GMT', 'tableLayout': False, 'ssl_name': 'huckbros.com', 'ssl_start': '20230715235959Z', 'ssl_expire': '20230715235959Z'}
2023-05-02 12:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.com> (referer: None)
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfotenabdruck.info/> from <GET http://pfotenabdruck.info>
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pflegeplatz-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://klappstuhl51.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfotenabdruck.info/> (referer: None)
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://rustikal-lecker.com/robots.txt> from <GET http://rustikal-lecker.com/robots.txt>
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dirkmeineke.de/robots.txt> from <GET http://dirkmeineke.de/robots.txt>
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://abandonedzone.com>
{'id': '146', 'url': 'http://abandonedzone.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://manwe.net/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/robots.txt?doing_wp_cron=1683029088.0616788864135742187500> from <GET https://cyberfiber.me/robots.txt>
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fusionstreet.com>
{'id': '154', 'url': 'http://fusionstreet.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativopc.com>
{'id': '160', 'url': 'http://3nativopc.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinolinequinone.org/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "rustikal-lecker.com"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'rustikal-lecker.com'))])
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinolinequinone.org> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://pfotenabdruck.info/>
{'id': '156', 'url': 'https://pfotenabdruck.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 21 Apr 2020 09:28:05 GMT', 'tableLayout': True, 'ssl_name': 'pfotenabdruck.info', 'ssl_start': '20230609235959Z', 'ssl_expire': '20230609235959Z'}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hilcura.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://dirkmeineke.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kaliner-yoga.de/> from <GET https://kaliner-yoga.de/>
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://klappstuhl51.de> (referer: None)
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dirkmeineke.de/> from <GET http://dirkmeineke.de>
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pflegeplatz-willebadessen.de> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://delta-mb.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://delta-mb.de/> from <GET http://delta-mb.de>
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://efggp.de/robots.txt> from <GET http://efggp.de/robots.txt>
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dirkmeineke.de/> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pyrroloquinolinequinone.org>
{'id': '164', 'url': 'http://pyrroloquinolinequinone.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://jagdgruppe-bad-aibling.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnendiscount.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.pfotenabdruck.inforobots.txt> from <GET http://pfoten-abdruck.de/robots.txt>
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://klappstuhl51.de>
{'id': '159', 'url': 'http://klappstuhl51.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pflegeplatz-willebadessen.de>
{'id': '158', 'url': 'http://pflegeplatz-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://jagdgruppe-bad-aibling.de> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://manwe.net> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dirkmeineke.de/>
{'id': '163', 'url': 'https://dirkmeineke.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 13 Mar 2023 20:03:35 GMT', 'tableLayout': True, 'ssl_name': 'dirkmeineke.de', 'ssl_start': '20230611205738Z', 'ssl_expire': '20230611205738Z'}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yuriol.com/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobilashop.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thomasludwig.net/robots.txt> (referer: None)
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnendiscount.de> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holistic-medicine.ch/robots.txt> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://jagdgruppe-bad-aibling.de>
{'id': '168', 'url': 'http://jagdgruppe-bad-aibling.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 27 Apr 2023 12:47:45 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobilashop.de> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://rustikal-lecker.com/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://manwe.net>
{'id': '157', 'url': 'http://manwe.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holistic-medicine.ch> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://go-paps.com/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://rustikal-lecker.com/> from <GET http://rustikal-lecker.com>
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://go-paps.com> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://discounterticket.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnendiscount.de>
{'id': '165', 'url': 'http://arbeitsbuehnendiscount.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lifttaxi.eu> (referer: None)
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/robots.txt> from <GET http://sememo.de/robots.txt>
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobilashop.de>
{'id': '169', 'url': 'http://casamobilashop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://holistic-medicine.ch>
{'id': '171', 'url': 'http://holistic-medicine.ch', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gukrause.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://delta-mb.de/> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gukrause.de> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://go-paps.com>
{'id': '172', 'url': 'http://go-paps.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 14 Apr 2022 14:49:32 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dukartstein.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thomasludwig.net> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lifttaxi.eu>
{'id': '132', 'url': 'http://lifttaxi.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://discounterticket.de> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenpflege-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wikistudien.org/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://victoria-einrichtungen.eu/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rustikal-lecker.com/> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wikistudien.org> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://delta-mb.de/>
{'id': '144', 'url': 'https://delta-mb.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'ascon-systems.de', 'ssl_start': '20230706235959Z', 'ssl_expire': '20230706235959Z'}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://cyberfiber.me/robots.txt?doing_wp_cron=1683029088.0616788864135742187500> (referer: None)
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:04:48 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://victoria-einrichtungen.eu> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gukrause.de>
{'id': '175', 'url': 'http://gukrause.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hotcoconut.eu/robots.txt> from <GET http://hotcoconut.eu/robots.txt>
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.eu/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 762: invalid start byte
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/> from <GET http://cyberfiber.me>
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-nebenwirkungen.info/robots.txt> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thomasludwig.net>
{'id': '162', 'url': 'http://thomasludwig.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://discounterticket.de>
{'id': '170', 'url': 'http://discounterticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-nebenwirkungen.info> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://rustikal-lecker.com/>
{'id': '161', 'url': 'https://rustikal-lecker.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wikistudien.org>
{'id': '177', 'url': 'http://wikistudien.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotcoconut.eu/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.de> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kley-net.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://victoria-einrichtungen.eu>
{'id': '176', 'url': 'http://victoria-einrichtungen.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hotcoconut.eu/> from <GET http://hotcoconut.eu>
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kley-net.de> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenpflege-willebadessen.de> (referer: None)
2023-05-02 12:04:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://yuriol.com/> from <GET http://yuriol.com>
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotcoconut.eu/> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://christian-kilthau.com/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fabian-klemt.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://stein-und-soehne.de/robots.txt> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://resveratrol-nebenwirkungen.info>
{'id': '179', 'url': 'http://resveratrol-nebenwirkungen.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fabian-klemt.de> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.de>
{'id': '182', 'url': 'http://wasistopc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hilcura.de/> (referer: None)
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://stein-und-soehne.de> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kley-net.de>
{'id': '183', 'url': 'http://kley-net.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 01 Mar 2022 16:42:42 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://cyberfiber.me/> (referer: None)
2023-05-02 12:04:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://altenpflege-willebadessen.de>
{'id': '173', 'url': 'http://altenpflege-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://erp-ratschlag.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hotcoconut.eu/>
{'id': '181', 'url': 'https://hotcoconut.eu/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'hotcoconut.eu', 'ssl_start': '20230518232429Z', 'ssl_expire': '20230518232429Z'}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://christian-kilthau.com> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://erp-ratschlag.de> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fabian-klemt.de>
{'id': '185', 'url': 'http://fabian-klemt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 11 Dec 2008 11:42:32 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hilcura.de/>
{'id': '109', 'url': 'https://hilcura.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'hilcura.de', 'ssl_start': '20240329235959Z', 'ssl_expire': '20240329235959Z'}
2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://stein-und-soehne.de>
{'id': '187', 'url': 'http://stein-und-soehne.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 16 Nov 2022 23:48:49 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-fragen-online-ktb.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lilier.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kindz.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yuriol.com/> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://cyberfiber.me/>
{'id': '73', 'url': 'https://cyberfiber.me/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'cyberfiber.me', 'ssl_start': '20220206235959Z', 'ssl_expire': '20220206235959Z'}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://streamify.tv> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-fragen-online-ktb.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lilier.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gracic.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mitochondriale-medizin.org/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 779: invalid start byte
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.eu> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://christian-kilthau.com>
{'id': '186', 'url': 'http://christian-kilthau.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kinderpartyfun.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://chronisch-untervoegelt.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gracic.de> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://erp-ratschlag.de>
{'id': '189', 'url': 'http://erp-ratschlag.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Jul 2010 08:53:51 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kindz.de>
{'id': '26', 'url': 'http://kindz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://yuriol.com/>
{'id': '116', 'url': 'https://yuriol.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 27 Apr 2023 09:59:24 GMT', 'tableLayout': False, 'ssl_name': 'yuriol.com', 'ssl_start': '20230531103008Z', 'ssl_expire': '20230531103008Z'}
2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://streamify.tv>
{'id': '124', 'url': 'http://streamify.tv', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzte-fragen-online-ktb.de>
{'id': '192', 'url': 'http://xn--rzte-fragen-online-ktb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lilier.de>
{'id': '190', 'url': 'http://lilier.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 30 Dec 2013 12:30:38 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kopfstandstuhl.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.formativmedia.atrobots.txt> from <GET http://formativ-print.at/robots.txt>
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-plus.org/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://losmenombak-mentawai.com/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.eu>
{'id': '136', 'url': 'http://govido.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.kaliner-yoga.de/>
{'id': '21', 'url': 'https://www.kaliner-yoga.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'kaliner-yoga.de', 'ssl_start': '20230615235959Z', 'ssl_expire': '20230615235959Z'}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-plus.org> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gracic.de>
{'id': '193', 'url': 'http://gracic.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dralexandermichalzik.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://fuerdiesache.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dralexandermichalzik.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://himalayaklangschalen.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.exelant.derobots.txt> from <GET http://exelant.de/robots.txt>
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexandermichalzik.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexandermichalzik.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondriale-medizin.org> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-plus.org>
{'id': '195', 'url': 'http://arginin-plus.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://webmarktplatz24.de/robots.txt> from <GET http://webmarktplatz24.de/robots.txt>
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://wengkbuehle.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dralexandermichalzik.de>
{'id': '199', 'url': 'http://dralexandermichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://alexandermichalzik.de>
{'id': '202', 'url': 'http://alexandermichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://himalayaklangschalen.de> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mitochondriale-medizin.org>
{'id': '184', 'url': 'http://mitochondriale-medizin.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://chronisch-untervoegelt.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lokomotor.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:49 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://webmarktplatz24.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://secstate.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fuerdiesache.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wengkbuehle.de> (referer: None)
2023-05-02 12:04:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://webmarktplatz24.de/> from <GET http://webmarktplatz24.de>
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lokomotor.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://secstate.de> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://himalayaklangschalen.de>
{'id': '200', 'url': 'http://himalayaklangschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kulturbeutelhamburg.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://webmarktplatz24.de/> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://chronisch-untervoegelt.de>
{'id': '188', 'url': 'http://chronisch-untervoegelt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kopfstandstuhl.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cnc-fanpage.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dukartstein.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://osd2000.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trendsandtechnik.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kinderpartyfun.de/> from <GET http://kinderpartyfun.de>
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://osd2000.de> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fuerdiesache.de>
{'id': '194', 'url': 'http://fuerdiesache.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cnc-fanpage.de> (referer: None)
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lingk-shops.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wengkbuehle.de>
{'id': '198', 'url': 'http://wengkbuehle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lokomotor.de>
{'id': '204', 'url': 'http://lokomotor.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://secstate.de>
{'id': '207', 'url': 'http://secstate.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 15 Mar 2010 17:15:22 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://efggp.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://webmarktplatz24.de/>
{'id': '205', 'url': 'https://webmarktplatz24.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'webmarktplatz24.de', 'ssl_start': '20230523235959Z', 'ssl_expire': '20230523235959Z'}
2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kopfstandstuhl.de>
{'id': '197', 'url': 'http://kopfstandstuhl.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lingk-shops.de> (referer: None)
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dukartstein.de>
{'id': '152', 'url': 'http://dukartstein.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://maik-wiege.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://efggp.de/> from <GET http://efggp.de>
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://osd2000.de>
{'id': '209', 'url': 'http://osd2000.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 20 Feb 2011 15:04:02 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:49 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cnc-fanpage.de>
{'id': '210', 'url': 'http://cnc-fanpage.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:49 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.kinderpartyfun.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.kinderpartyfun.de'))])
2023-05-02 12:04:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nrw-zwerge.de/robots.txt> (referer: None)
2023-05-02 12:04:49 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 766: invalid start byte
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lingk-shops.de>
{'id': '211', 'url': 'http://lingk-shops.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 28 Sep 2021 13:22:44 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dinner-ticket.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trendsandtechnik.de> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://johannes-doerfler.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobilashop.com/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://blass-net.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://maik-wiege.de> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.kinderpartyfun.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobilashop.com> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://blass-net.de> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dinner-ticket.de> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://trendsandtechnik.de>
{'id': '208', 'url': 'http://trendsandtechnik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nrw-zwerge.de> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://johannes-doerfler.de> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yogamaster.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://krop.at/robots.txt> (referer: None)
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://peter-flaspoehler.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://maik-wiege.de>
{'id': '213', 'url': 'http://maik-wiege.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://peter-flaspoehler.de> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobilashop.com>
{'id': '217', 'url': 'http://casademobilashop.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://blass-net.de>
{'id': '218', 'url': 'http://blass-net.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 23 May 2018 16:59:30 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dinner-ticket.de>
{'id': '214', 'url': 'http://dinner-ticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronaschutzhilfe.com/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kinderpartyfun.de/> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nrw-zwerge.de>
{'id': '212', 'url': 'http://nrw-zwerge.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://johannes-doerfler.de>
{'id': '216', 'url': 'http://johannes-doerfler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronaschutzhilfe.com> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://peter-flaspoehler.de>
{'id': '220', 'url': 'http://peter-flaspoehler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 04 Mar 2023 14:19:35 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.kinderpartyfun.de/>
{'id': '178', 'url': 'https://www.kinderpartyfun.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mobilemietstation.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://steiger-taxi.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://krop.at> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila-shop.info/robots.txt> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coronaschutzhilfe.com>
{'id': '223', 'url': 'http://coronaschutzhilfe.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila-shop.info> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://serenafate.com/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hotland.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://krop.at>
{'id': '219', 'url': 'http://krop.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://richard-deutsch.com/robots.txt/> from <GET http://richard-deutsch.com/robots.txt>
2023-05-02 12:04:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2Frobots.txt&bp-auth=1&action=bpnoaccess> from <GET https://sememo.de/robots.txt>
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mobilemietstation.de> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://steiger-taxi.de> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila-shop.info>
{'id': '227', 'url': 'http://casamobila-shop.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://urbaczek.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://birtekaufmann-photography.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 59 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 61 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 79 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 105 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 113 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 117 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 133 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 138 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 167 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 175 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 183 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 187 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 188 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 191 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 203 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 213 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 227 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 229 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 233 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 235 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 237 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 240 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 242 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 246 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 248 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 253 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 255 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 259 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 263 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 264 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 265 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 268 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 269 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 270 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 272 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 274 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 275 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 278 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 281 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 284 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 289 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 297 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 298 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 302 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 304 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 306 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 310 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 313 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 317 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 320 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 322 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 327 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 328 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 332 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 333 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 338 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 342 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 346 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 350 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 358 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 368 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 376 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 380 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 383 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 384 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 386 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 387 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 391 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 396 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 412 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 429 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 477 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 481 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 483 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 484 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 486 without any user agent to enforce it on.
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://urbaczek.de> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grothues8.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://serenafate.com> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mobilemietstation.de>
{'id': '224', 'url': 'http://mobilemietstation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://atlantis-magazin.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://steiger-taxi.de>
{'id': '225', 'url': 'http://steiger-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://fam-rauch.com/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hotland.de> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hyaluronsaeure-kapseln.org/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.medizinrechtler.inforobots.txt> from <GET http://grad-der-behinderung.de/robots.txt>
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fam-rauch.com> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://richard-deutsch.com/robots.txt/> (referer: None)
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 53 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 61 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 93 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 97 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 104 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 115 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 123 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 131 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 149 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 151 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 153 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 205 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 235 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 242 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 254 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 273 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 280 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 311 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 318 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 348 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 355 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 374 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 382 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 383 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 391 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 399 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 400 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 407 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 408 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 409 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 416 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 417 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 418 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 425 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 426 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 427 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 434 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 435 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 436 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 447 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 448 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 449 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 451 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 452 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 453 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 454 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 482 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 495 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 496 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 502 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 532 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 552 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 553 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 554 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 555 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 556 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 558 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 559 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 560 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 561 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 562 without any user agent to enforce it on.
2023-05-02 12:04:50 [protego] DEBUG: Rule at line 563 without any user agent to enforce it on.
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hyaluronsaeure-kapseln.org> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://urbaczek.de>
{'id': '230', 'url': 'http://urbaczek.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 05 Feb 2023 15:42:17 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://serenafate.com>
{'id': '228', 'url': 'http://serenafate.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://weihrauchforum.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hotland.de>
{'id': '229', 'url': 'http://hotland.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://atlantis-magazin.de> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://herzinfarkt-vorbeugen-herzgesundheit.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://kulturbeutelhamburg.de/> from <GET http://kulturbeutelhamburg.de>
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://weihrauchforum.de> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fam-rauch.com>
{'id': '232', 'url': 'http://fam-rauch.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 19 Jun 2011 18:13:24 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://fun4-u.info/robots.txt> from <GET http://fun4-u.info/robots.txt>
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://herzinfarkt-vorbeugen-herzgesundheit.de> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hyaluronsaeure-kapseln.org>
{'id': '233', 'url': 'http://hyaluronsaeure-kapseln.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://atlantis-magazin.de>
{'id': '231', 'url': 'http://atlantis-magazin.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "fun4-u.info"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'fun4-u.info'))])
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://santakruz.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://weihrauchforum.de>
{'id': '235', 'url': 'http://weihrauchforum.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://felgenfreddy.de/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://richard-deutsch.com> (referer: None)
2023-05-02 12:04:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://herzinfarkt-vorbeugen-herzgesundheit.de>
{'id': '236', 'url': 'http://herzinfarkt-vorbeugen-herzgesundheit.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://fun4-u.info/robots.txt> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://santakruz.de> (referer: None)
2023-05-02 12:04:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://fun4-u.info/> from <GET http://fun4-u.info>
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://fun4-u.info/> (referer: None)
2023-05-02 12:04:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://essenonpaper.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grothues8.de> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://richard-deutsch.com>
{'id': '180', 'url': 'http://richard-deutsch.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://badischtauchen.de/robots.txt> from <GET http://badischtauchen.de/robots.txt>
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://felgenfreddy.de> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://santakruz.de>
{'id': '240', 'url': 'http://santakruz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 30 Oct 2009 20:47:40 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yogamaster.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://birtekaufmann-photography.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongstrategy.com/robots.txt> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://fun4-u.info/>
{'id': '226', 'url': 'https://fun4-u.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 03 Aug 2019 10:52:54 GMT', 'tableLayout': True, 'ssl_name': 'fun4-u.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://pharmabox.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://grothues8.de>
{'id': '222', 'url': 'http://grothues8.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongstrategy.com> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://losmenombak-mentawai.com> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://felgenfreddy.de>
{'id': '237', 'url': 'http://felgenfreddy.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://badischtauchen.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pharmabox.de> (referer: None)
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.mattner.orgrobots.txt> from <GET http://mattner.org/robots.txt>
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yogamaster.de>
{'id': '221', 'url': 'http://yogamaster.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://badischtauchen.de/> from <GET http://badischtauchen.de>
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://birtekaufmann-photography.de>
{'id': '215', 'url': 'http://birtekaufmann-photography.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essenonpaper.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://badischtauchen.de/> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://future-fashion.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dejongstrategy.com>
{'id': '243', 'url': 'http://dejongstrategy.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 28 Dec 2014 13:42:08 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://x-schreck.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://losmenombak-mentawai.com>
{'id': '191', 'url': 'http://losmenombak-mentawai.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://future-fashion.de> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pharmabox.de>
{'id': '244', 'url': 'http://pharmabox.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 03 Mar 2020 18:24:35 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://x-schreck.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://meditationkisse.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://essenonpaper.de>
{'id': '241', 'url': 'http://essenonpaper.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://badischtauchen.de/>
{'id': '242', 'url': 'https://badischtauchen.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 18 Sep 2021 11:27:27 GMT', 'tableLayout': False, 'ssl_name': 'badischtauchen.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ostsea.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gasthof-rolfes.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://future-fashion.de>
{'id': '247', 'url': 'http://future-fashion.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 05 Jan 2016 16:33:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.okv-weiden.de/robots.txt> from <GET http://okv-weiden.de/robots.txt>
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ostsea.de> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://x-schreck.de>
{'id': '248', 'url': 'http://x-schreck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 06 Oct 2007 18:12:18 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-lexikon.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bioticon.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/robots.txt> from <GET http://www.okv-weiden.de/robots.txt>
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://meditationkisse.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bioticon.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://klangschalen-schop.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ostsea.de>
{'id': '251', 'url': 'http://ostsea.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 15 Jan 2017 21:15:54 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-lexikon.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://efggp.de/> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sound-producer.com/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.dknv.de/> from <GET http://dknv.de/robots.txt>
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://meditationkisse.de>
{'id': '249', 'url': 'http://meditationkisse.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bioticon.de>
{'id': '253', 'url': 'http://bioticon.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.okv-weiden.de/> from <GET http://okv-weiden.de>
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikonblog.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-lexikon.de>
{'id': '250', 'url': 'http://arbeitsbuehnen-lexikon.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/robots.txt> from <GET http://www.okv-weiden.de/robots.txt>
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://efggp.de/>
{'id': '166', 'url': 'https://efggp.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'efggp.de', 'ssl_start': '20230611231725Z', 'ssl_expire': '20230611231725Z'}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikonblog.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://michaelbuschke.com/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://has-wbe.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://klangschalen-schop.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://michaelbuschke.com> (referer: None)
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/> from <GET http://www.okv-weiden.de/>
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/> (referer: None)
2023-05-02 12:04:51 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikonblog.de>
{'id': '257', 'url': 'http://biotikonblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arztfragenonline.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ws-gmbh.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.dknv.de/> from <GET http://dknv.de>
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://klangschalen-schop.de>
{'id': '254', 'url': 'http://klangschalen-schop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arztfragenonline.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila.de> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://michaelbuschke.com>
{'id': '258', 'url': 'http://michaelbuschke.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 07 Sep 2019 22:37:35 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.org/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://barbaraundjan.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:51 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:04:51 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:04:51 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:04:51 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:04:51 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:04:51 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:04:51 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://flugzeuge-weltweit.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://has-wbe.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://peking-laufenburg.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arztfragenonline.de>
{'id': '261', 'url': 'http://arztfragenonline.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila.de>
{'id': '260', 'url': 'http://casamobila.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.gasthof-rolfes.de/> from <GET http://gasthof-rolfes.de>
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://peking-laufenburg.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://barbaraundjan.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westsidemusic.com/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-fragen-online.de/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://has-wbe.de>
{'id': '255', 'url': 'http://has-wbe.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.at/robots.txt> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-fragen-online.de> (referer: None)
2023-05-02 12:04:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://was-ist-opc.de>
{'id': '262', 'url': 'http://was-ist-opc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://flugzeuge-weltweit.de> (referer: None)
2023-05-02 12:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.at> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://peking-laufenburg.de>
{'id': '264', 'url': 'http://peking-laufenburg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 24 Aug 2020 11:50:05 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://volki.pb.photographyrobots.txt> from <GET http://galerie-bauer.de/robots.txt>
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://barbaraundjan.de>
{'id': '259', 'url': 'http://barbaraundjan.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westsidemusic.com> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ws-gmbh.de> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.okv-weiden.de/>
{'id': '252', 'url': 'https://www.okv-weiden.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 02 May 2023 12:04:51 GMT', 'tableLayout': False, 'ssl_name': 'www.okv-weiden.de', 'ssl_start': '20230522235959Z', 'ssl_expire': '20230522235959Z'}
2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arzt-fragen-online.de>
{'id': '266', 'url': 'http://arzt-fragen-online.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://flugzeuge-weltweit.de>
{'id': '263', 'url': 'http://flugzeuge-weltweit.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.org> (referer: None)
2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.at>
{'id': '267', 'url': 'http://biotikon.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nomadpublishing.store/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobila24.eu/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://grinberg.de/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nomadpublishing.store> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www6.dknv.de/>
{'id': '256', 'url': 'http://www6.dknv.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobila24.eu> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://westsidemusic.com>
{'id': '265', 'url': 'http://westsidemusic.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ws-gmbh.de>
{'id': '206', 'url': 'http://ws-gmbh.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://curcuma-nebenwirkungen.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.org>
{'id': '238', 'url': 'http://govido.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://curcuma-nebenwirkungen.de> (referer: None)
2023-05-02 12:04:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://grinberg.de/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://grinberg.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://aus-keil-wird-schmidt.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://indiaschop.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tropicanalife.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://aus-keil-wird-schmidt.de> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grinberg.de> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nomadpublishing.store>
{'id': '270', 'url': 'http://nomadpublishing.store', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tropicanalife.de> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobila24.eu>
{'id': '271', 'url': 'http://casademobila24.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.eu/robots.txt> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://curcuma-nebenwirkungen.de>
{'id': '273', 'url': 'http://curcuma-nebenwirkungen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.eu> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://holzmoebel-experte.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.gasthof-rolfes.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.kg-muellekolk.de/robots.txt> from <GET http://kg-muellekolk.de/robots.txt>
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holzmoebel-experte.de> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://aus-keil-wird-schmidt.de>
{'id': '274', 'url': 'http://aus-keil-wird-schmidt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 22 Apr 2008 07:31:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://indiaschop.de> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://grinberg.de>
{'id': '272', 'url': 'http://grinberg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 28 Jan 2023 20:32:03 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzteonlinefragen-unb.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tropicanalife.de>
{'id': '276', 'url': 'http://tropicanalife.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 28 Mar 2009 22:52:40 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2Frobots.txt&bp-auth=1&action=bpnoaccess> (referer: None)
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 9 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 131 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 145 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 149 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 157 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 159 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 166 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 169 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 177 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzteonlinefragen-unb.de> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativopc.eu>
{'id': '277', 'url': 'http://3nativopc.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cradle-software.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/> from <GET http://sememo.de>
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://strumpfaffen.org/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bergium.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://strumpfaffen.org> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cradle-software.de> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://holzmoebel-experte.de>
{'id': '278', 'url': 'http://holzmoebel-experte.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://indiaschop.de>
{'id': '275', 'url': 'http://indiaschop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bergium.de> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://studio-balance.at/robots.txt> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzteonlinefragen-unb.de>
{'id': '280', 'url': 'http://xn--rzteonlinefragen-unb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thebodyasarchive.com/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://studio-balance.at> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thebodyasarchive.com> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.ch/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taxi-meile.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 104 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 108 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 112 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 118 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 130 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 132 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 138 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 140 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 146 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 162 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 166 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 168 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 170 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 174 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 176 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 178 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 182 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 184 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 188 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 190 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 192 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 196 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 200 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 204 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 206 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 208 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 210 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 212 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 214 without any user agent to enforce it on.
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sound-producer.com> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-babyzimmer.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://strumpfaffen.org>
{'id': '282', 'url': 'http://strumpfaffen.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 23 Apr 2012 11:57:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cradle-software.de>
{'id': '281', 'url': 'http://cradle-software.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bergium.de>
{'id': '283', 'url': 'http://bergium.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 07 Nov 2017 13:41:54 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.ch> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-babyzimmer.de> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://studio-balance.at>
{'id': '285', 'url': 'http://studio-balance.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 30 Dec 2020 13:19:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-shop-paderborn.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondrien-funktion.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thebodyasarchive.com>
{'id': '286', 'url': 'http://thebodyasarchive.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 27 Dec 2020 18:13:23 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://sound-producer.com>
{'id': '203', 'url': 'http://sound-producer.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://format-mehrweg.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondrien-funktion.de> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.ch>
{'id': '287', 'url': 'http://biotikon.ch', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.com/robots.txt> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-babyzimmer.de>
{'id': '288', 'url': 'http://massivholz-babyzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cosmecon.eu/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://format-mehrweg.de> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.com> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cosmecon.eu> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-dielenmoebel.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mitochondrien-funktion.de>
{'id': '290', 'url': 'http://mitochondrien-funktion.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-dielenmoebel.de> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--kleinegrashpfer-9vb.at/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dark-hawks.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://format-mehrweg.de>
{'id': '292', 'url': 'http://format-mehrweg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 10 Dec 2019 16:45:18 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--kleinegrashpfer-9vb.at> (referer: None)
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-alpha-ketoglutarat.com>
{'id': '291', 'url': 'http://arginin-alpha-ketoglutarat.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cosmecon.eu>
{'id': '293', 'url': 'http://cosmecon.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 25 Nov 2009 14:28:26 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-lothar-wester.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-shop-paderborn.de> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://skiverband-mv.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dinnerticket.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:04:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-dielenmoebel.de>
{'id': '294', 'url': 'http://massivholz-dielenmoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://eventation.de/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://reitanlage-riedmuehle.de/robots.txt> (referer: None)
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:04:52 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ticila.com/robots.txt> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--kleinegrashpfer-9vb.at>
{'id': '295', 'url': 'http://xn--kleinegrashpfer-9vb.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 13 Apr 2023 07:16:11 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://seat-factory.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ticila.com> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-shop-paderborn.de>
{'id': '289', 'url': 'http://yoga-shop-paderborn.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://seat-factory.de> (referer: None)
2023-05-02 12:04:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://eventation.de/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://eventation.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://technolog-e.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://skiverband-mv.de> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-datenbank.net/robots.txt> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dinnerticket.de> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://technolog-e.de> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mcpicchu.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://eventation.de> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ticila.com>
{'id': '300', 'url': 'http://ticila.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 05 Jan 2016 16:33:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://seat-factory.de>
{'id': '301', 'url': 'http://seat-factory.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 15 Jul 2015 14:10:37 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mcpicchu.de> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-lothar-wester.de> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://trengo.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lederoutfits.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://skiverband-mv.de>
{'id': '298', 'url': 'http://skiverband-mv.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dirkklages.com/robots.txt> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dinnerticket.de>
{'id': '296', 'url': 'http://dinnerticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://technolog-e.de>
{'id': '302', 'url': 'http://technolog-e.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 07 Nov 2014 10:40:41 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://reitanlage-riedmuehle.de> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dirkklages.com> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lederoutfits.de> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://eventation.de>
{'id': '299', 'url': 'http://eventation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mcpicchu.de>
{'id': '304', 'url': 'http://mcpicchu.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 23 Jul 2022 22:42:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-lothar-wester.de>
{'id': '297', 'url': 'http://yoga-lothar-wester.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-datenbank.net> (referer: None)
2023-05-02 12:04:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.nachhilfe-oase.de/robots.txt> from <GET http://123discounter.de/robots.txt>
2023-05-02 12:04:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.kg-muellekolk.de/> from <GET http://kg-muellekolk.de>
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trengo.de> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cymeg.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://reitanlage-riedmuehle.de>
{'id': '284', 'url': 'http://reitanlage-riedmuehle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kulturbeutelhamburg.de/> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dirkklages.com>
{'id': '307', 'url': 'http://dirkklages.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 17 Apr 2021 09:45:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cymeg.de> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lederoutfits.de>
{'id': '308', 'url': 'http://lederoutfits.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Nov 2017 23:18:52 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-plomari.com/robots.txt> (referer: None)
2023-05-02 12:04:53 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-datenbank.net>
{'id': '303', 'url': 'http://auto-datenbank.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://teamtrzweb.de> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://die-goldene-schallplatte.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 780: invalid start byte
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dark-hawks.de> (referer: None)
2023-05-02 12:04:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.nachhilfe-oase.de/> from <GET http://123discounter.de>
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://trengo.de>
{'id': '306', 'url': 'http://trengo.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.eu/robots.txt> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://kulturbeutelhamburg.de/>
{'id': '147', 'url': 'https://kulturbeutelhamburg.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'kulturbeutelhamburg.de', 'ssl_start': '20230605235959Z', 'ssl_expire': '20230605235959Z'}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://schiefer-lautsprecher.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.eu> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cymeg.de>
{'id': '313', 'url': 'http://cymeg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 17 Apr 2021 09:45:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-plomari.com> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://teamtrzweb.de>
{'id': '305', 'url': 'http://teamtrzweb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://die-goldene-schallplatte.de> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dark-hawks.de>
{'id': '279', 'url': 'http://dark-hawks.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.gasthof-rolfes.de/> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativ-opc.eu>
{'id': '314', 'url': 'http://3nativ-opc.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-plomari.com>
{'id': '309', 'url': 'http://ouzo-plomari.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://die-goldene-schallplatte.de>
{'id': '312', 'url': 'http://die-goldene-schallplatte.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.nachhilfe-oase.de/>
{'id': '311', 'url': 'http://www.nachhilfe-oase.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.gasthof-rolfes.de/>
{'id': '239', 'url': 'http://www.gasthof-rolfes.de/', 'status': 200, 'title': 'Gasthof Rolfes ', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongsblog.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:04:53 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:04:53 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schiefer-lautsprecher.de> (referer: None)
2023-05-02 12:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/robots.txt> (referer: None)
2023-05-02 12:04:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schiefer-lautsprecher.de>
{'id': '315', 'url': 'http://schiefer-lautsprecher.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongsblog.de> (referer: None)
2023-05-02 12:04:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taxi-meile.de> (referer: None)
2023-05-02 12:04:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dejongsblog.de>
{'id': '310', 'url': 'http://dejongsblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taxi-meile.de>
{'id': '246', 'url': 'http://taxi-meile.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/> (referer: None)
2023-05-02 12:04:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess> from <GET https://sememo.de/>
2023-05-02 12:04:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.kg-muellekolk.de/>
{'id': '268', 'url': 'http://www.kg-muellekolk.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:04:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess> (referer: None)
2023-05-02 12:04:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:04:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess>
{'id': '174', 'url': 'https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess', 'status': 200, 'title': 'Anmelden ‹ sememo — WordPress', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'sememo.de', 'ssl_start': '20230710235959Z', 'ssl_expire': '20230710235959Z'}
2023-05-02 12:04:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://photos.hr-photo.derobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:05:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.pfotenabdruck.inforobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:05:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.exelant.derobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:05:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.formativmedia.atrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:05:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.medizinrechtler.inforobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:05:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.mattner.orgrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:05:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://volki.pb.photographyrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:05:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://photos.hr-photo.derobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:05:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://hr-photo.de/robots.txt>: DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:05:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://photos.hr-photo.de> from <GET http://hr-photo.de>
2023-05-02 12:05:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://photos.hr-photo.de/robots.txt> (referer: None)
2023-05-02 12:05:14 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 771: invalid start byte
2023-05-02 12:05:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://photos.hr-photo.de> (referer: None)
2023-05-02 12:05:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:05:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://photos.hr-photo.de>
{'id': '24', 'url': 'http://photos.hr-photo.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:05:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.pfotenabdruck.inforobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:05:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://pfoten-abdruck.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:05:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.pfotenabdruck.info> from <GET http://pfoten-abdruck.de>
2023-05-02 12:05:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.pfotenabdruck.info/robots.txt> (referer: None)
2023-05-02 12:05:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.pfotenabdruck.info> (referer: None)
2023-05-02 12:05:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:05:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.pfotenabdruck.info>
{'id': '167', 'url': 'https://www.pfotenabdruck.info', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 21 Apr 2020 09:28:05 GMT', 'tableLayout': True, 'ssl_name': 'pfotenabdruck.info', 'ssl_start': '20230609235959Z', 'ssl_expire': '20230609235959Z'}
2023-05-02 12:05:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.exelant.derobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:05:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://exelant.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:05:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.exelant.de> from <GET http://exelant.de>
2023-05-02 12:05:19 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.exelant.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.exelant.de'))])
2023-05-02 12:05:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.exelant.de/robots.txt> (referer: None)
2023-05-02 12:05:19 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:05:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.exelant.de> (referer: None)
2023-05-02 12:05:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:05:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.exelant.de>
{'id': '201', 'url': 'https://www.exelant.de', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:05:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://www.formativmedia.atrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:05:20 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://formativ-print.at/robots.txt>: DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:05:21 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.formativmedia.at> from <GET http://formativ-print.at>
2023-05-02 12:05:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.medizinrechtler.inforobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:05:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://grad-der-behinderung.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:05:21 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.medizinrechtler.info> from <GET http://grad-der-behinderung.de>
2023-05-02 12:05:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://www.mattner.orgrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:05:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://mattner.org/robots.txt>: DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:05:21 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.medizinrechtler.info"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.medizinrechtler.info'))])
2023-05-02 12:05:21 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.mattner.org> from <GET http://mattner.org>
2023-05-02 12:05:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.mattner.org/robots.txt> (referer: None)
2023-05-02 12:05:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.medizinrechtler.info/robots.txt> (referer: None)
2023-05-02 12:05:21 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:05:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.medizinrechtler.info> (referer: None)
2023-05-02 12:05:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.mattner.org> (referer: None)
2023-05-02 12:05:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:05:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.medizinrechtler.info>
{'id': '234', 'url': 'https://www.medizinrechtler.info', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:05:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:05:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.mattner.org>
{'id': '245', 'url': 'http://www.mattner.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:05:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.formativmedia.at/robots.txt> (referer: None)
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 81 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:05:22 [protego] DEBUG: Rule at line 109 without any user agent to enforce it on.
2023-05-02 12:05:22 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.formativmedia.at/> from <GET http://www.formativmedia.at>
2023-05-02 12:05:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://volki.pb.photographyrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:05:23 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://galerie-bauer.de/robots.txt>: DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:05:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://volki.pb.photography> from <GET http://galerie-bauer.de>
2023-05-02 12:05:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://volki.pb.photography/robots.txt> (referer: None)
2023-05-02 12:05:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://volki.pb.photography> (referer: None)
2023-05-02 12:05:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.formativmedia.at/> (referer: None)
2023-05-02 12:05:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:05:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://volki.pb.photography>
{'id': '269', 'url': 'https://volki.pb.photography', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': '*.pb.photography', 'ssl_start': '20240222235959Z', 'ssl_expire': '20240222235959Z'}
2023-05-02 12:05:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:05:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.formativmedia.at/>
{'id': '196', 'url': 'https://www.formativmedia.at/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'einfachwanda.at', 'ssl_start': '20230515034747Z', 'ssl_expire': '20230515034747Z'}
2023-05-02 12:05:23 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:05:23 [scrapy.extensions.feedexport] INFO: Stored jl feed (313 items) in: baddata_results_20230502_120443.jl
2023-05-02 12:05:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 14,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 14,
 'downloader/request_bytes': 171795,
 'downloader/request_count': 757,
 'downloader/request_method_count/GET': 757,
 'downloader/response_bytes': 2675509,
 'downloader/response_count': 743,
 'downloader/response_status_count/200': 466,
 'downloader/response_status_count/301': 24,
 'downloader/response_status_count/302': 67,
 'downloader/response_status_count/404': 174,
 'downloader/response_status_count/500': 6,
 'downloader/response_status_count/503': 6,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 39.324295,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 5, 23, 756389),
 'httpcompression/response_bytes': 4729890,
 'httpcompression/response_count': 352,
 'item_scraped_count': 313,
 'log_count/DEBUG': 1794,
 'log_count/ERROR': 21,
 'log_count/INFO': 11,
 'log_count/WARNING': 334,
 'memusage/max': 67072000,
 'memusage/startup': 67072000,
 'response_received_count': 643,
 'retry/count': 13,
 'retry/max_reached': 13,
 'retry/reason_count/500 Internal Server Error': 3,
 'retry/reason_count/503 Service Unavailable': 3,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 7,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 7,
 'robotstxt/request_count': 336,
 'robotstxt/response_count': 329,
 'robotstxt/response_status_count/200': 150,
 'robotstxt/response_status_count/404': 174,
 'robotstxt/response_status_count/500': 3,
 'robotstxt/response_status_count/503': 2,
 'scheduler/dequeued': 365,
 'scheduler/dequeued/memory': 365,
 'scheduler/enqueued': 365,
 'scheduler/enqueued/memory': 365,
 'start_time': datetime.datetime(2023, 5, 2, 12, 4, 44, 432094)}
2023-05-02 12:05:23 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:08:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:08:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:08:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:08:22 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:08:22 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:08:22 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:08:22 [scrapy.extensions.telnet] INFO: Telnet Password: 4f82226d51471785
2023-05-02 12:08:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:08:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:08:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:08:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:08:22 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:08:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:08:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://lisaglauer.com/robots.txt> from <GET http://lisaglauer.de/robots.txt>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.be/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfs-europe.de/robots.txt> from <GET http://pfs-europe.de/robots.txt>
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sprachlos-ev-beratung.de/robots.txt> from <GET http://sprachlos-ev-beratung.de/robots.txt>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-wohnzimmer.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hausegger.net/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mgvliederkranz-asbach.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lahres.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://kaffeeundservice.at/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.be> (referer: None)
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://docs.moodle.org:443/robots.txt> from <GET http://docs.moodle.org/robots.txt>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://efwe-art.at/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-wohnzimmer.de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lahres.com> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://bayern-immobilie.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hausegger.net> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lifttaxi.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mgvliederkranz-asbach.de> (referer: None)
2023-05-02 12:08:23 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET http://lifttaxi.com> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mind-holiday.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://steigertaxi.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://karo-stimme.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://kaffeeundservice.at/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://kaffeeundservice.at/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mixable.media/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "pfs-europe.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'pfs-europe.de'))])
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://kaliner-yoga.de/robots.txt> from <GET http://kaliner-yoga.de/robots.txt>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfs-europe.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://photos.hr-photo.derobots.txt> from <GET http://hr-photo.de/robots.txt>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mixable.media> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-teamblog.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.be>
{'id': '7', 'url': 'http://biotikon.be', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://efwe-art.at> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-wohnzimmer.de>
{'id': '13', 'url': 'http://massivholz-wohnzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lahres.com>
{'id': '15', 'url': 'http://lahres.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 25 Dec 2019 08:39:49 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hausegger.net>
{'id': '12', 'url': 'http://hausegger.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 12 Sep 2012 11:18:42 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfs-europe.de/> from <GET http://pfs-europe.de>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-teamblog.de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bayern-immobilie.de> (referer: None)
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://lisaglauer.com/> from <GET http://lisaglauer.de>
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mgvliederkranz-asbach.de>
{'id': '11', 'url': 'http://mgvliederkranz-asbach.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 28 Jan 2009 10:53:37 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://steigertaxi.de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://coaching-im-alltag.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gbv-grosskarolinenfeld.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://docs.moodle.org:443/2x/pl/Flash> from <GET http://docs.moodle.org/2x/pl/Flash>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mind-holiday.com> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://karo-stimme.de> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mixable.media>
{'id': '17', 'url': 'http://mixable.media', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfs-europe.de/> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lifttaxi.com/> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://efwe-art.at>
{'id': '5', 'url': 'http://efwe-art.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.dienstleistung-solar.de/robots.txt> from <GET http://dienstleistung-solar.de/robots.txt>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://billardcafe-suedpark.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaukus.org/robots.txt> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '25', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon-teamblog.de>
{'id': '23', 'url': 'http://biotikon-teamblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaffeeundservice.at> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bayern-immobilie.de>
{'id': '6', 'url': 'http://bayern-immobilie.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 463 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 469 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 472 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 507 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 513 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET http://kaukus.org/index2.html> from <GET http://kaukus.org>
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET http://www.purpur.eatbu.com/?lang=de> from <GET http://billardcafe-suedpark.de>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gbv-grosskarolinenfeld.de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coaching-im-alltag.de> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://steigertaxi.de>
{'id': '8', 'url': 'http://steigertaxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mind-holiday.com>
{'id': '4', 'url': 'http://mind-holiday.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.com> (referer: None)
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://karo-stimme.de>
{'id': '9', 'url': 'http://karo-stimme.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://pfs-europe.de/>
{'id': '3', 'url': 'https://pfs-europe.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 03 Aug 2019 10:52:54 GMT', 'tableLayout': True, 'ssl_name': 'fun4-u.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lifttaxi.com/>
{'id': '1', 'url': 'http://lifttaxi.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.wassersportcenter-heiligenhafen.de/> from <GET http://wassersportcenter-heiligenhafen.de>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaukus.org/index2.html> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kaffeeundservice.at>
{'id': '14', 'url': 'http://kaffeeundservice.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/2x/pl/Flash> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kilthau.tech/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://seelenfeder.at/robots.txt> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.info>
{'id': '31', 'url': 'http://wasistopc.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://drey.info/robots.txt> from <GET http://drey.info/robots.txt>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://illger.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.purpur.eatbu.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://seelenfeder.at> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gbv-grosskarolinenfeld.de>
{'id': '19', 'url': 'http://gbv-grosskarolinenfeld.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://byzickl.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.purpur.eatbu.com/?lang=de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://illger.de> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coaching-im-alltag.de>
{'id': '18', 'url': 'http://coaching-im-alltag.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 463 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 469 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 472 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 507 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 513 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dobbrunz.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-star.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativ-opc.com>
{'id': '33', 'url': 'http://3nativ-opc.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gartengestaltung-brandner.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 781: invalid start byte
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 1 times): 503 Service Unavailable
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://byzickl.de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kilthau.tech> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kaukus.org/index2.html>
{'id': '32', 'url': 'http://kaukus.org/index2.html', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2009 20:14:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/> from <GET http://www.wassersportcenter-heiligenhafen.de/>
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://docs.moodle.org:443/2x/pl/Flash>
{'id': '0', 'url': 'https://docs.moodle.org:443/2x/pl/Flash', 'status': 200, 'title': 'Flash – MoodleDocs', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 24 Apr 2023 19:09:27 GMT', 'tableLayout': False, 'ssl_name': 'sni.cloudflaressl.com', 'ssl_start': '20240501235959Z', 'ssl_expire': '20240501235959Z'}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dobbrunz.com> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cellhomoeostasis.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://seelenfeder.at>
{'id': '35', 'url': 'http://seelenfeder.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 10 Mar 2018 15:51:03 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.purpur.eatbu.com/?lang=de>
{'id': '30', 'url': 'http://www.purpur.eatbu.com/?lang=de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://illger.de>
{'id': '37', 'url': 'http://illger.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 07 Nov 2011 11:18:19 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://magnonics.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cellhomoeostasis.com> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/> (referer: None)
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://diekicktipper.de/robots.txt> from <GET http://diekicktipper.de/robots.txt>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://drey.info/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://magnonics.de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ts-it-service.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schoolscout24.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://drey.info/> from <GET http://drey.info>
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://byzickl.de>
{'id': '36', 'url': 'http://byzickl.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gesundheitsforum-norderstedt.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dj-sb.com/robots.txt> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lisaglauer.com/>
{'id': '2', 'url': 'http://lisaglauer.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kilthau.tech>
{'id': '34', 'url': 'http://kilthau.tech', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schoolscout24.de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ts-it-service.de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://malttec.net/robots.txt> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dobbrunz.com>
{'id': '40', 'url': 'http://dobbrunz.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 03 Jul 2016 18:53:33 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 2 times): 503 Service Unavailable
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://drey.info/> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-flurmoebel.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gesundheitsforum-norderstedt.com> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kanzlei-sauerwein.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dj-sb.com> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cellhomoeostasis.com>
{'id': '41', 'url': 'http://cellhomoeostasis.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET https://teamtrzweb.de> from <GET http://malttec.net>
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.dienstleistung-solar.de/> from <GET http://dienstleistung-solar.de>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hgwimmobilien.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.wassersportcenter-heiligenhafen.de/>
{'id': '16', 'url': 'https://www.wassersportcenter-heiligenhafen.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'wassersportcenter-heiligenhafen.de', 'ssl_start': '20230719235959Z', 'ssl_expire': '20230719235959Z'}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yogashop-paderborn.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kanzlei-sauerwein.de> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://diekicktipper.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essen-auf-raedern-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-flurmoebel.de> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://magnonics.de>
{'id': '42', 'url': 'http://magnonics.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 26 Aug 2009 08:33:56 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://diekicktipper.de/> from <GET http://diekicktipper.de>
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.net/robots.txt> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wetzel.work/robots.txt> (referer: None)
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 213 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 214 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-05-02 12:08:23 [protego] DEBUG: Rule at line 234 without any user agent to enforce it on.
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schoolscout24.de>
{'id': '46', 'url': 'http://schoolscout24.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 03 May 2011 19:33:17 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ts-it-service.de>
{'id': '45', 'url': 'http://ts-it-service.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://drey.info/>
{'id': '38', 'url': 'https://drey.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 17 Mar 2014 08:12:11 GMT', 'tableLayout': True, 'ssl_name': 'drey.info', 'ssl_start': '20230705235959Z', 'ssl_expire': '20230705235959Z'}
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gesundheitsforum-norderstedt.com>
{'id': '48', 'url': 'http://gesundheitsforum-norderstedt.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 26 Mar 2012 15:32:11 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.net> (referer: None)
2023-05-02 12:08:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:23 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dj-sb.com>
{'id': '47', 'url': 'http://dj-sb.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 28 Dec 2014 13:42:08 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wetzel.work> (referer: None)
2023-05-02 12:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gartengestaltung-brandner.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-bueromoebel.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://buehnen-taxi.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kanzlei-sauerwein.de>
{'id': '51', 'url': 'http://kanzlei-sauerwein.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 03 Jan 2010 02:56:47 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://diekicktipper.de/> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-flurmoebel.de>
{'id': '50', 'url': 'http://massivholz-flurmoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://teamtrzweb.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hgwimmobilien.de> (referer: None)
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 1 times): 503 Service Unavailable
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-bueromoebel.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://goting-kliff53.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-online-fragen-ktb.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-pilavas.com/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.net>
{'id': '55', 'url': 'http://biotikon.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://goting-kliff53.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wetzel.work>
{'id': '56', 'url': 'http://wetzel.work', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 14 Aug 2017 17:23:07 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bce-clan.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-online-fragen-ktb.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gartengestaltung-brandner.de>
{'id': '28', 'url': 'http://gartengestaltung-brandner.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.info/robots.txt> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://diekicktipper.de/>
{'id': '44', 'url': 'https://diekicktipper.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'diekicktipper.de', 'ssl_start': '20230527235959Z', 'ssl_expire': '20230527235959Z'}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://bavarian-starlights.com/robots.txt> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hgwimmobilien.de>
{'id': '43', 'url': 'http://hgwimmobilien.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.info> (referer: None)
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 2 times): 503 Service Unavailable
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://xn--edelstahlscheckkartenhlle-0wc.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-bueromoebel.de>
{'id': '59', 'url': 'http://massivholz-bueromoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://buehnen-taxi.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://goting-kliff53.de>
{'id': '60', 'url': 'http://goting-kliff53.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 27 Jul 2022 11:59:55 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bce-clan.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzte-online-fragen-ktb.de>
{'id': '61', 'url': 'http://xn--rzte-online-fragen-ktb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bauelemente-riede.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-star.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-makler.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-pilavas.com> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ciber-man.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bauelemente-riede.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ciber-man.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://was-ist-opc.info>
{'id': '65', 'url': 'http://was-ist-opc.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/robots.txt> from <GET http://cyberfiber.me/robots.txt>
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://buehnen-taxi.de>
{'id': '53', 'url': 'http://buehnen-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kaliner-yoga.de/robots.txt> from <GET https://kaliner-yoga.de/robots.txt>
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://openlate.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--edelstahlscheckkartenhlle-0wc.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://buehnentaxi.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://xn--holzmbel-experte-qwb.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bce-clan.de>
{'id': '64', 'url': 'http://bce-clan.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-star.de>
{'id': '39', 'url': 'http://yoga-star.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-pilavas.com>
{'id': '57', 'url': 'http://ouzo-pilavas.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--holzmbel-experte-qwb.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bauelemente-riede.de>
{'id': '69', 'url': 'http://bauelemente-riede.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 16 May 2018 20:42:31 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essen-auf-raedern-willebadessen.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-makler.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teamtrzweb.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ciber-man.de>
{'id': '70', 'url': 'http://ciber-man.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 20 Jan 2013 06:17:41 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://schluesselvereinzler.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/> (failed 1 times): 503 Service Unavailable
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://da365.de/robots.txt> from <GET http://da365.de/robots.txt>
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mobile-mietstation.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 774: invalid start byte
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yogashop-paderborn.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-markt.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 776: invalid start byte
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--edelstahlscheckkartenhlle-0wc.de>
{'id': '67', 'url': 'http://xn--edelstahlscheckkartenhlle-0wc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://style-and-smile.com/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kindz.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-qigong-master.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://huckbros.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--holzmbel-experte-qwb.de>
{'id': '74', 'url': 'http://xn--holzmbel-experte-qwb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://essen-auf-raedern-willebadessen.de>
{'id': '27', 'url': 'http://essen-auf-raedern-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://buehnentaxi.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-jugendzimmer.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-qigong-master.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-makler.de>
{'id': '63', 'url': 'http://arbeitsbuehnen-makler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://huckbros.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://teamtrzweb.de>
{'id': '49', 'url': 'https://teamtrzweb.de', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'biergartenteam.de', 'ssl_start': '20230609051948Z', 'ssl_expire': '20230609051948Z'}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ignaz-schwarzbach.eu/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://marco-k.com/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schluesselvereinzler.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yogashop-paderborn.de>
{'id': '52', 'url': 'http://yogashop-paderborn.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-jugendzimmer.de> (referer: None)
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.bavarian-starlights.com/> from <GET http://bavarian-starlights.com>
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ignaz-schwarzbach.eu> (referer: None)
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/> (failed 2 times): 503 Service Unavailable
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/> (referer: None)
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.sprachlos-ev-beratung.de/robots.txt> from <GET https://sprachlos-ev-beratung.de/robots.txt>
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mobile-mietstation.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://marco-k.com> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila.info/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gzlw.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://no-flush-niacin.com/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-markt.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://buehnentaxi.de>
{'id': '68', 'url': 'http://buehnentaxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://www.bavarian-starlights.com/robots.txt> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taichichuan-qigong-master.de>
{'id': '79', 'url': 'http://taichichuan-qigong-master.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://huckbros.de>
{'id': '78', 'url': 'http://huckbros.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Feb 2023 08:29:22 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila.info> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://no-flush-niacin.com> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gzlw.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-kinderzimmer.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.net/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schluesselvereinzler.de>
{'id': '76', 'url': 'http://schluesselvereinzler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-jugendzimmer.de>
{'id': '80', 'url': 'http://massivholz-jugendzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobila24.com/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ganzheitlichesheilen.eu/robots.txt> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ignaz-schwarzbach.eu>
{'id': '81', 'url': 'http://ignaz-schwarzbach.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 04 Dec 2022 16:28:01 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-kinderzimmer.de> (referer: None)
2023-05-02 12:08:24 [dubdev] ERROR: HttpError on https://www.dienstleistung-solar.de/
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mobile-mietstation.de>
{'id': '71', 'url': 'http://mobile-mietstation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://marco-k.com>
{'id': '82', 'url': 'http://marco-k.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 23 Jul 2022 22:42:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmedmichalzik.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ganzheitlichesheilen.eu> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobila24.com> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmichalzik.eu/robots.txt> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-markt.de>
{'id': '72', 'url': 'http://arbeitsbuehnen-markt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmedmichalzik.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://openlate.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nows.tk/robots.txt> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila.info>
{'id': '83', 'url': 'http://casamobila.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmichalzik.eu> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://no-flush-niacin.com>
{'id': '86', 'url': 'http://no-flush-niacin.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gzlw.de>
{'id': '85', 'url': 'http://gzlw.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 28 Jan 2009 09:42:45 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://boncomputa.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rztefragenonline-unb.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fmz-frankershausen.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nows.tk> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-kinderzimmer.de>
{'id': '87', 'url': 'http://massivholz-kinderzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rztefragenonline-unb.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hgwimmobilien.org/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ganzheitlichesheilen.eu>
{'id': '90', 'url': 'http://ganzheitlichesheilen.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.org/robots.txt> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobila24.com>
{'id': '89', 'url': 'http://casademobila24.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://westside-linedance.com/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lift-taxi.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://drmedmichalzik.de>
{'id': '91', 'url': 'http://drmedmichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://openlate.de>
{'id': '54', 'url': 'http://openlate.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.org> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westside-linedance.com> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://edelstahlscheckkartenhuelle.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://da365.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://cyberfiber.me/robots.txt> (referer: None)
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://drmichalzik.eu>
{'id': '93', 'url': 'http://drmichalzik.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nows.tk>
{'id': '94', 'url': 'http://nows.tk', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 06 Jun 2018 13:57:15 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wwb1.net/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.net> (referer: None)
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/> from <GET http://cyberfiber.me>
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rztefragenonline-unb.de>
{'id': '95', 'url': 'http://xn--rztefragenonline-unb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-friedrichwipfel.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-nektar.com/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://boncomputa.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://doc-blog.org/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-alexander-michalzik.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-friedrichwipfel.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wwb1.net> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hgwimmobilien.org> (referer: None)
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://kaliner-yoga.de/> from <GET http://kaliner-yoga.de>
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://doc-blog.org> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://strukturplatten.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-alexander-michalzik.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.org>
{'id': '97', 'url': 'http://biotikon.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://westside-linedance.com>
{'id': '99', 'url': 'http://westside-linedance.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 29 Jan 2020 19:02:17 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://wvm-branchenportal.com/robots.txt> from <GET http://wvm-branchenportal.com/robots.txt>
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tibetanische-klangschalen.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobilashop.eu/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://impax.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://edelstahlscheckkartenhuelle.de> (referer: None)
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sprachlos-ev-beratung.de/> from <GET http://sprachlos-ev-beratung.de>
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.net>
{'id': '75', 'url': 'http://govido.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://sulden-apresski.com/robots.txt> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobilashop.eu> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lift-taxi.de> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://strukturplatten.de> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://boncomputa.de>
{'id': '88', 'url': 'http://boncomputa.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taichichuan-friedrichwipfel.de>
{'id': '103', 'url': 'http://taichichuan-friedrichwipfel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sulden-apresski.com> (referer: None)
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wwb1.net>
{'id': '102', 'url': 'http://wwb1.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 30 Sep 2019 13:35:47 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ateliereichner.de/robots.txt> (referer: None)
2023-05-02 12:08:24 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hgwimmobilien.org>
{'id': '92', 'url': 'http://hgwimmobilien.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://doc-blog.org>
{'id': '104', 'url': 'http://doc-blog.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://marcodesigner.com/robots.txt> from <GET http://marcodesigner.com/robots.txt>
2023-05-02 12:08:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dr-alexander-michalzik.de>
{'id': '105', 'url': 'http://dr-alexander-michalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-nektar.com> (referer: None)
2023-05-02 12:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wvm-branchenportal.com/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://impax.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ateliereichner.de> (referer: None)
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://wvm-branchenportal.com/> from <GET http://wvm-branchenportal.com>
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://edelstahlscheckkartenhuelle.de>
{'id': '101', 'url': 'http://edelstahlscheckkartenhuelle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-esszimmer.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobilashop.eu>
{'id': '110', 'url': 'http://casademobilashop.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mastershaft.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tibetanische-klangschalen.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "marcodesigner.com"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'marcodesigner.com'))])
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lift-taxi.de>
{'id': '84', 'url': 'http://lift-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://strukturplatten.de>
{'id': '106', 'url': 'http://strukturplatten.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 04 Aug 2021 09:44:09 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wvm-branchenportal.com/> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://sulden-apresski.com>
{'id': '111', 'url': 'http://sulden-apresski.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 13 Mar 2018 16:25:36 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-esszimmer.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenheim-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-nektar.com>
{'id': '96', 'url': 'http://ouzo-nektar.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://da365.de/> from <GET http://da365.de>
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://impax.de>
{'id': '100', 'url': 'http://impax.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-schlafzimmer.com/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.info/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ateliereichner.de>
{'id': '114', 'url': 'http://ateliereichner.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tibetanischeklagschalen.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dr-michalzik.at/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tibetanische-klangschalen.de>
{'id': '107', 'url': 'http://tibetanische-klangschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-schlafzimmer.com> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://wvm-branchenportal.com/>
{'id': '108', 'url': 'https://wvm-branchenportal.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 14 Oct 2020 22:24:24 GMT', 'tableLayout': True, 'ssl_name': 'wvm-branchenportal.com', 'ssl_start': '20240307235959Z', 'ssl_expire': '20240307235959Z'}
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-esszimmer.de>
{'id': '118', 'url': 'http://massivholz-esszimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://carcal.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.bavarian-starlights.com/> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mastershaft.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nowikow.org/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://heueu.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://wahnfriet.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.org/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://heueu.de> (referer: None)
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.sprachlos-ev-beratung.de/> from <GET https://sprachlos-ev-beratung.de/>
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://djmirco.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.org> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenheim-willebadessen.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://marcodesigner.com/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://djmirco.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tibetanischeklagschalen.de> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-schlafzimmer.com>
{'id': '122', 'url': 'http://massivholz-schlafzimmer.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hilcura.com/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.bavarian-starlights.com/>
{'id': '66', 'url': 'http://www.bavarian-starlights.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mastershaft.de>
{'id': '113', 'url': 'http://mastershaft.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://carcal.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-michalzik.at> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nowikow.org> (referer: None)
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://marcodesigner.com/> from <GET http://marcodesigner.com>
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://el-toque-latino.com/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://heueu.de>
{'id': '125', 'url': 'http://heueu.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 12 Sep 2009 08:57:13 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://el-toque-latino.com> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.info/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexander-michalzik.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wahnfriet.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-online-fragen.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kaliner-yoga.de/> from <GET https://kaliner-yoga.de/>
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://biotikon.es/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.info> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-alpha-ketoglutarat.org>
{'id': '126', 'url': 'http://arginin-alpha-ketoglutarat.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://altenheim-willebadessen.de>
{'id': '58', 'url': 'http://altenheim-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://djmirco.de>
{'id': '127', 'url': 'http://djmirco.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tibetanischeklagschalen.de>
{'id': '121', 'url': 'http://tibetanischeklagschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-online-fragen.de> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://carcal.de>
{'id': '123', 'url': 'http://carcal.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dr-michalzik.at>
{'id': '117', 'url': 'http://dr-michalzik.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nowikow.org>
{'id': '119', 'url': 'http://nowikow.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://aufklebershop-geislingen.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexander-michalzik.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://die-goldene-cd.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://aufklebershop-geislingen.de> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://el-toque-latino.com>
{'id': '129', 'url': 'http://el-toque-latino.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 07 Sep 2007 23:41:14 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> from <GET http://xn--wohlfhlwerkstatt-nzb.de/robots.txt>
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronamitarbeiterschutz.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wahnfriet.de>
{'id': '120', 'url': 'http://wahnfriet.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://da365.de/> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.info>
{'id': '130', 'url': 'http://biotikon.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://springblade.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.es> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronamitarbeiterschutz.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinoline-quinone.org/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arzt-online-fragen.de>
{'id': '134', 'url': 'http://arzt-online-fragen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://alexander-michalzik.de>
{'id': '133', 'url': 'http://alexander-michalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinoline-quinone.org> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://edv-frenzel.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tribulus-saponine.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://marcodesigner.com/> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://aufklebershop-geislingen.de>
{'id': '135', 'url': 'http://aufklebershop-geislingen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Jul 2010 08:21:56 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://die-goldene-cd.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kindz.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tribulus-saponine.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://edv-frenzel.de> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://da365.de/>
{'id': '62', 'url': 'https://da365.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'da365.de', 'ssl_start': '20230708235959Z', 'ssl_expire': '20230708235959Z'}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://delta-mb.de/robots.txt> from <GET http://delta-mb.de/robots.txt>
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.es>
{'id': '128', 'url': 'http://biotikon.es', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coronamitarbeiterschutz.de>
{'id': '137', 'url': 'http://coronamitarbeiterschutz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-dosierung.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://springblade.de> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pyrroloquinoline-quinone.org>
{'id': '140', 'url': 'http://pyrroloquinoline-quinone.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-dosierung.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> (referer: None)
2023-05-02 12:08:25 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://marcodesigner.com/>
{'id': '115', 'url': 'https://marcodesigner.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'die-rohrreinigung.com', 'ssl_start': '20210714235959Z', 'ssl_expire': '20210714235959Z'}
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> from <GET http://xn--wohlfhlwerkstatt-nzb.de>
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lifttaxi.eu/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://streamify.tv/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://die-goldene-cd.de>
{'id': '131', 'url': 'http://die-goldene-cd.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://patcy.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kindz.de>
{'id': '26', 'url': 'http://kindz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tribulus-saponine.de>
{'id': '142', 'url': 'http://tribulus-saponine.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://edv-frenzel.de>
{'id': '141', 'url': 'http://edv-frenzel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.info> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-blog.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://formergy.biz/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.health/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-blog.de> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://springblade.de>
{'id': '139', 'url': 'http://springblade.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hilcura.de/> from <GET http://hilcura.com>
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://huckbros.com/robots.txt> from <GET http://huckbros.com/robots.txt>
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://resveratrol-dosierung.de>
{'id': '145', 'url': 'http://resveratrol-dosierung.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.health> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://patcy.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.info>
{'id': '112', 'url': 'http://govido.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon-blog.de>
{'id': '149', 'url': 'http://biotikon-blog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://crazybulls.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://huckbros.com/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.health>
{'id': '150', 'url': 'http://biotikon.health', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://huckbros.com/> from <GET http://huckbros.com>
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://crazybulls.de> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://formergy.biz> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fusionstreet.com/robots.txt> (referer: None)
2023-05-02 12:08:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://patcy.de>
{'id': '148', 'url': 'http://patcy.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://abandonedzone.com/robots.txt> (referer: None)
2023-05-02 12:08:25 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:25 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://huckbros.com/> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://abandonedzone.com> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfotenabdruck.info/robots.txt> from <GET http://pfotenabdruck.info/robots.txt>
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yuriol.com/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nevaton.eu/robots.txt> (referer: None)
2023-05-02 12:08:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://pixxpress.com/robots.txt> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://crazybulls.de>
{'id': '153', 'url': 'http://crazybulls.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Dec 2022 21:55:50 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://formergy.biz>
{'id': '143', 'url': 'http://formergy.biz', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pixxpress.com> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www6.xn--wohlfhlwerkstatt-nzb.de/>
{'id': '138', 'url': 'http://www6.xn--wohlfhlwerkstatt-nzb.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://delta-mb.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://huckbros.com/>
{'id': '151', 'url': 'https://huckbros.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Feb 2023 08:29:22 GMT', 'tableLayout': False, 'ssl_name': 'huckbros.com', 'ssl_start': '20230715235959Z', 'ssl_expire': '20230715235959Z'}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.com/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://delta-mb.de/> from <GET http://delta-mb.de>
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://abandonedzone.com>
{'id': '146', 'url': 'http://abandonedzone.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.sprachlos-ev-beratung.de/>
{'id': '10', 'url': 'https://www.sprachlos-ev-beratung.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'sprachlos-ev-beratung.de', 'ssl_start': '20230711235959Z', 'ssl_expire': '20230711235959Z'}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.com> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fmz-frankershausen.de> (referer: None)
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://rustikal-lecker.com/robots.txt> from <GET http://rustikal-lecker.com/robots.txt>
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://pfotenabdruck.info/robots.txt> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pixxpress.com>
{'id': '155', 'url': 'http://pixxpress.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 26 Aug 2011 20:30:29 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fusionstreet.com> (referer: None)
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfotenabdruck.info/> from <GET http://pfotenabdruck.info>
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dirkmeineke.de/robots.txt> from <GET http://dirkmeineke.de/robots.txt>
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://klappstuhl51.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pflegeplatz-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "rustikal-lecker.com"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'rustikal-lecker.com'))])
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfotenabdruck.info/> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativopc.com>
{'id': '160', 'url': 'http://3nativopc.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fmz-frankershausen.de>
{'id': '20', 'url': 'http://fmz-frankershausen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinolinequinone.org/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://manwe.net/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinolinequinone.org> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://cyberfiber.me/> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fusionstreet.com>
{'id': '154', 'url': 'http://fusionstreet.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://efggp.de/robots.txt> from <GET http://efggp.de/robots.txt>
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://dirkmeineke.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://pfotenabdruck.info/>
{'id': '156', 'url': 'https://pfotenabdruck.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 21 Apr 2020 09:28:05 GMT', 'tableLayout': True, 'ssl_name': 'pfotenabdruck.info', 'ssl_start': '20230609235959Z', 'ssl_expire': '20230609235959Z'}
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dirkmeineke.de/> from <GET http://dirkmeineke.de>
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.pfotenabdruck.inforobots.txt> from <GET http://pfoten-abdruck.de/robots.txt>
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://klappstuhl51.de> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnendiscount.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hilcura.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://jagdgruppe-bad-aibling.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thomasludwig.net/robots.txt> (referer: None)
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pyrroloquinolinequinone.org>
{'id': '164', 'url': 'http://pyrroloquinolinequinone.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dirkmeineke.de/> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nevaton.eu> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://jagdgruppe-bad-aibling.de> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://cyberfiber.me/>
{'id': '73', 'url': 'https://cyberfiber.me/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'cyberfiber.me', 'ssl_start': '20220206235959Z', 'ssl_expire': '20220206235959Z'}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pflegeplatz-willebadessen.de> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://rustikal-lecker.com/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lifttaxi.eu> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobilashop.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://rustikal-lecker.com/> from <GET http://rustikal-lecker.com>
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holistic-medicine.ch/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://go-paps.com/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://delta-mb.de/> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobilashop.de> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://klappstuhl51.de>
{'id': '159', 'url': 'http://klappstuhl51.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnendiscount.de> (referer: None)
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/robots.txt> from <GET http://sememo.de/robots.txt>
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holistic-medicine.ch> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://go-paps.com> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.eu/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 762: invalid start byte
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dirkmeineke.de/>
{'id': '163', 'url': 'https://dirkmeineke.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 13 Mar 2023 20:03:35 GMT', 'tableLayout': True, 'ssl_name': 'dirkmeineke.de', 'ssl_start': '20230611205738Z', 'ssl_expire': '20230611205738Z'}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nevaton.eu>
{'id': '98', 'url': 'http://nevaton.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://jagdgruppe-bad-aibling.de>
{'id': '168', 'url': 'http://jagdgruppe-bad-aibling.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 27 Apr 2023 12:47:45 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gukrause.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://manwe.net> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wikistudien.org/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://yuriol.com/> from <GET http://yuriol.com>
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pflegeplatz-willebadessen.de>
{'id': '158', 'url': 'http://pflegeplatz-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gukrause.de> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://discounterticket.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://victoria-einrichtungen.eu/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lifttaxi.eu>
{'id': '132', 'url': 'http://lifttaxi.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://delta-mb.de/>
{'id': '144', 'url': 'https://delta-mb.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'ascon-systems.de', 'ssl_start': '20230706235959Z', 'ssl_expire': '20230706235959Z'}
2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobilashop.de>
{'id': '169', 'url': 'http://casamobilashop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnendiscount.de>
{'id': '165', 'url': 'http://arbeitsbuehnendiscount.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wikistudien.org> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://holistic-medicine.ch>
{'id': '171', 'url': 'http://holistic-medicine.ch', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenpflege-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://go-paps.com>
{'id': '172', 'url': 'http://go-paps.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 14 Apr 2022 14:49:32 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.kaliner-yoga.de/>
{'id': '21', 'url': 'https://www.kaliner-yoga.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'kaliner-yoga.de', 'ssl_start': '20230615235959Z', 'ssl_expire': '20230615235959Z'}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://victoria-einrichtungen.eu> (referer: None)
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hotcoconut.eu/robots.txt> from <GET http://hotcoconut.eu/robots.txt>
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thomasludwig.net> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-nebenwirkungen.info/robots.txt> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://manwe.net>
{'id': '157', 'url': 'http://manwe.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-nebenwirkungen.info> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotcoconut.eu/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kley-net.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rustikal-lecker.com/> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yuriol.com/> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.de> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gukrause.de>
{'id': '175', 'url': 'http://gukrause.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://discounterticket.de> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wikistudien.org>
{'id': '177', 'url': 'http://wikistudien.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hotcoconut.eu/> from <GET http://hotcoconut.eu>
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://victoria-einrichtungen.eu>
{'id': '176', 'url': 'http://victoria-einrichtungen.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fabian-klemt.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kley-net.de> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thomasludwig.net>
{'id': '162', 'url': 'http://thomasludwig.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fabian-klemt.de> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://resveratrol-nebenwirkungen.info>
{'id': '179', 'url': 'http://resveratrol-nebenwirkungen.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://richard-deutsch.com/robots.txt/> from <GET http://richard-deutsch.com/robots.txt>
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotcoconut.eu/> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://christian-kilthau.com/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://stein-und-soehne.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://efggp.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenpflege-willebadessen.de> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://rustikal-lecker.com/>
{'id': '161', 'url': 'https://rustikal-lecker.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://yuriol.com/>
{'id': '116', 'url': 'https://yuriol.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 27 Apr 2023 09:59:24 GMT', 'tableLayout': False, 'ssl_name': 'yuriol.com', 'ssl_start': '20230531103008Z', 'ssl_expire': '20230531103008Z'}
2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.de>
{'id': '182', 'url': 'http://wasistopc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://erp-ratschlag.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://efggp.de/> from <GET http://efggp.de>
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://stein-und-soehne.de> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://discounterticket.de>
{'id': '170', 'url': 'http://discounterticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kley-net.de>
{'id': '183', 'url': 'http://kley-net.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 01 Mar 2022 16:42:42 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lilier.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-fragen-online-ktb.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://erp-ratschlag.de> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fabian-klemt.de>
{'id': '185', 'url': 'http://fabian-klemt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 11 Dec 2008 11:42:32 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hotcoconut.eu/>
{'id': '181', 'url': 'https://hotcoconut.eu/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'hotcoconut.eu', 'ssl_start': '20230518232429Z', 'ssl_expire': '20230518232429Z'}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lilier.de> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gracic.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-fragen-online-ktb.de> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://christian-kilthau.com> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.eu> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://altenpflege-willebadessen.de>
{'id': '173', 'url': 'http://altenpflege-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://chronisch-untervoegelt.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://richard-deutsch.com/robots.txt/> (referer: None)
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 53 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 61 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 93 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 97 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 104 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 115 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 123 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 131 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 149 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 151 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 153 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 205 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 235 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 242 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 254 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 273 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 280 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 311 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 318 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 348 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 355 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 374 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 382 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 383 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 391 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 399 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 400 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 407 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 408 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 409 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 416 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 417 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 418 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 425 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 426 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 427 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 434 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 435 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 436 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 447 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 448 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 449 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 451 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 452 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 453 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 454 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 482 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 495 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 496 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 502 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 532 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 552 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 553 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 554 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 555 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 556 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 558 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 559 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 560 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 561 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 562 without any user agent to enforce it on.
2023-05-02 12:08:26 [protego] DEBUG: Rule at line 563 without any user agent to enforce it on.
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://stein-und-soehne.de>
{'id': '187', 'url': 'http://stein-und-soehne.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 16 Nov 2022 23:48:49 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mitochondriale-medizin.org/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 779: invalid start byte
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://style-and-smile.com> (referer: None)
2023-05-02 12:08:26 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://erp-ratschlag.de>
{'id': '189', 'url': 'http://erp-ratschlag.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Jul 2010 08:53:51 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.formativmedia.atrobots.txt> from <GET http://formativ-print.at/robots.txt>
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-plus.org/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kopfstandstuhl.de/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://losmenombak-mentawai.com/robots.txt> (referer: None)
2023-05-02 12:08:26 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kinderpartyfun.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lilier.de>
{'id': '190', 'url': 'http://lilier.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 30 Dec 2013 12:30:38 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzte-fragen-online-ktb.de>
{'id': '192', 'url': 'http://xn--rzte-fragen-online-ktb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://christian-kilthau.com>
{'id': '186', 'url': 'http://christian-kilthau.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.eu>
{'id': '136', 'url': 'http://govido.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hilcura.de/> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gracic.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-plus.org> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://style-and-smile.com>
{'id': '77', 'url': 'http://style-and-smile.com', 'status': 200, 'title': 'WordPress 5.3.0 – Eine weitere WordPress-Website', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dralexandermichalzik.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://richard-deutsch.com> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://himalayaklangschalen.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dralexandermichalzik.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexandermichalzik.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.exelant.derobots.txt> from <GET http://exelant.de/robots.txt>
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://chronisch-untervoegelt.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexandermichalzik.de> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hilcura.de/>
{'id': '109', 'url': 'https://hilcura.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'hilcura.de', 'ssl_start': '20240329235959Z', 'ssl_expire': '20240329235959Z'}
2023-05-02 12:08:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://webmarktplatz24.de/robots.txt> from <GET http://webmarktplatz24.de/robots.txt>
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lokomotor.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gracic.de>
{'id': '193', 'url': 'http://gracic.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-plus.org>
{'id': '195', 'url': 'http://arginin-plus.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://richard-deutsch.com>
{'id': '180', 'url': 'http://richard-deutsch.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://fuerdiesache.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://wengkbuehle.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dralexandermichalzik.de>
{'id': '199', 'url': 'http://dralexandermichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://streamify.tv> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kopfstandstuhl.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondriale-medizin.org> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lokomotor.de> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://chronisch-untervoegelt.de>
{'id': '188', 'url': 'http://chronisch-untervoegelt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://himalayaklangschalen.de> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://alexandermichalzik.de>
{'id': '202', 'url': 'http://alexandermichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://losmenombak-mentawai.com> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://secstate.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://efggp.de/> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://osd2000.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trendsandtechnik.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://webmarktplatz24.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cnc-fanpage.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://secstate.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://osd2000.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dukartstein.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lingk-shops.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://streamify.tv>
{'id': '124', 'url': 'http://streamify.tv', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://webmarktplatz24.de/> from <GET http://webmarktplatz24.de>
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cnc-fanpage.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lingk-shops.de> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kopfstandstuhl.de>
{'id': '197', 'url': 'http://kopfstandstuhl.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mitochondriale-medizin.org>
{'id': '184', 'url': 'http://mitochondriale-medizin.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lokomotor.de>
{'id': '204', 'url': 'http://lokomotor.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trendsandtechnik.de> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://himalayaklangschalen.de>
{'id': '200', 'url': 'http://himalayaklangschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://maik-wiege.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://losmenombak-mentawai.com>
{'id': '191', 'url': 'http://losmenombak-mentawai.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wengkbuehle.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://webmarktplatz24.de/> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fuerdiesache.de> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 https://efggp.de/>
{'id': '166', 'url': 'https://efggp.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'efggp.de', 'ssl_start': '20230611231725Z', 'ssl_expire': '20230611231725Z'}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://maik-wiege.de> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://secstate.de>
{'id': '207', 'url': 'http://secstate.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 15 Mar 2010 17:15:22 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://osd2000.de>
{'id': '209', 'url': 'http://osd2000.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 20 Feb 2011 15:04:02 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cnc-fanpage.de>
{'id': '210', 'url': 'http://cnc-fanpage.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobilashop.com/robots.txt> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lingk-shops.de>
{'id': '211', 'url': 'http://lingk-shops.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 28 Sep 2021 13:22:44 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://blass-net.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dinner-ticket.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nrw-zwerge.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 766: invalid start byte
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://trendsandtechnik.de>
{'id': '208', 'url': 'http://trendsandtechnik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ws-gmbh.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobilashop.com> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://krop.at/robots.txt> (referer: None)
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wengkbuehle.de>
{'id': '198', 'url': 'http://wengkbuehle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 https://webmarktplatz24.de/>
{'id': '205', 'url': 'https://webmarktplatz24.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'webmarktplatz24.de', 'ssl_start': '20230523235959Z', 'ssl_expire': '20230523235959Z'}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://blass-net.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://peter-flaspoehler.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fuerdiesache.de>
{'id': '194', 'url': 'http://fuerdiesache.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://johannes-doerfler.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://maik-wiege.de>
{'id': '213', 'url': 'http://maik-wiege.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://peter-flaspoehler.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yogamaster.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kinderpartyfun.de/> from <GET http://kinderpartyfun.de>
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronaschutzhilfe.com/robots.txt> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobilashop.com>
{'id': '217', 'url': 'http://casademobilashop.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dinner-ticket.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronaschutzhilfe.com> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://krop.at> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://blass-net.de>
{'id': '218', 'url': 'http://blass-net.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 23 May 2018 16:59:30 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nrw-zwerge.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.kinderpartyfun.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.kinderpartyfun.de'))])
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://peter-flaspoehler.de>
{'id': '220', 'url': 'http://peter-flaspoehler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 04 Mar 2023 14:19:35 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://johannes-doerfler.de> (referer: None)
2023-05-02 12:08:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://fun4-u.info/robots.txt> from <GET http://fun4-u.info/robots.txt>
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila-shop.info/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://steiger-taxi.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mobilemietstation.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila-shop.info> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dinner-ticket.de>
{'id': '214', 'url': 'http://dinner-ticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coronaschutzhilfe.com>
{'id': '223', 'url': 'http://coronaschutzhilfe.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://krop.at>
{'id': '219', 'url': 'http://krop.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://serenafate.com/robots.txt> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nrw-zwerge.de>
{'id': '212', 'url': 'http://nrw-zwerge.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hotland.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "fun4-u.info"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'fun4-u.info'))])
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://johannes-doerfler.de>
{'id': '216', 'url': 'http://johannes-doerfler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://urbaczek.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://fun4-u.info/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hotland.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.kinderpartyfun.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://urbaczek.de> (referer: None)
2023-05-02 12:08:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://fun4-u.info/> from <GET http://fun4-u.info>
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila-shop.info>
{'id': '227', 'url': 'http://casamobila-shop.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://atlantis-magazin.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://steiger-taxi.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mobilemietstation.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://fam-rauch.com/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://fun4-u.info/> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fam-rauch.com> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hyaluronsaeure-kapseln.org/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.medizinrechtler.inforobots.txt> from <GET http://grad-der-behinderung.de/robots.txt>
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://serenafate.com> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://weihrauchforum.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yogamaster.de> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://birtekaufmann-photography.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 59 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 61 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 79 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 105 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 113 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 117 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 133 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 138 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 167 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 175 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 183 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 187 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 188 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 191 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 203 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 213 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 227 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 229 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 233 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 235 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 237 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 240 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 242 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 246 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 248 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 253 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 255 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 259 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 263 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 264 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 265 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 268 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 269 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 270 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 272 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 274 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 275 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 278 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 281 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 284 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 289 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 297 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 298 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 302 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 304 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 306 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 310 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 313 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 317 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 320 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 322 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 327 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 328 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 332 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 333 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 338 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 342 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 346 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 350 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 358 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 368 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 376 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 380 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 383 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 384 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 386 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 387 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 391 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 396 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 412 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 429 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 477 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 481 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 483 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 484 without any user agent to enforce it on.
2023-05-02 12:08:27 [protego] DEBUG: Rule at line 486 without any user agent to enforce it on.
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hyaluronsaeure-kapseln.org> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://herzinfarkt-vorbeugen-herzgesundheit.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://weihrauchforum.de> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hotland.de>
{'id': '229', 'url': 'http://hotland.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://urbaczek.de>
{'id': '230', 'url': 'http://urbaczek.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 05 Feb 2023 15:42:17 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2Frobots.txt&bp-auth=1&action=bpnoaccess> from <GET https://sememo.de/robots.txt>
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://steiger-taxi.de>
{'id': '225', 'url': 'http://steiger-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mobilemietstation.de>
{'id': '224', 'url': 'http://mobilemietstation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kinderpartyfun.de/> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://herzinfarkt-vorbeugen-herzgesundheit.de> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 https://fun4-u.info/>
{'id': '226', 'url': 'https://fun4-u.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 03 Aug 2019 10:52:54 GMT', 'tableLayout': True, 'ssl_name': 'fun4-u.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fam-rauch.com>
{'id': '232', 'url': 'http://fam-rauch.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 19 Jun 2011 18:13:24 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://santakruz.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://serenafate.com>
{'id': '228', 'url': 'http://serenafate.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yogamaster.de>
{'id': '221', 'url': 'http://yogamaster.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://felgenfreddy.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://essenonpaper.de/robots.txt> (referer: None)
2023-05-02 12:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://santakruz.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hyaluronsaeure-kapseln.org>
{'id': '233', 'url': 'http://hyaluronsaeure-kapseln.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://badischtauchen.de/robots.txt> from <GET http://badischtauchen.de/robots.txt>
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://weihrauchforum.de>
{'id': '235', 'url': 'http://weihrauchforum.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essenonpaper.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.kinderpartyfun.de/>
{'id': '178', 'url': 'https://www.kinderpartyfun.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://herzinfarkt-vorbeugen-herzgesundheit.de>
{'id': '236', 'url': 'http://herzinfarkt-vorbeugen-herzgesundheit.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongstrategy.com/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://pharmabox.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://felgenfreddy.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongstrategy.com> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grothues8.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pharmabox.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://santakruz.de>
{'id': '240', 'url': 'http://santakruz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 30 Oct 2009 20:47:40 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://essenonpaper.de>
{'id': '241', 'url': 'http://essenonpaper.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.mattner.orgrobots.txt> from <GET http://mattner.org/robots.txt>
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://badischtauchen.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ws-gmbh.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://future-fashion.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://badischtauchen.de/> from <GET http://badischtauchen.de>
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://felgenfreddy.de>
{'id': '237', 'url': 'http://felgenfreddy.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dejongstrategy.com>
{'id': '243', 'url': 'http://dejongstrategy.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 28 Dec 2014 13:42:08 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://future-fashion.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://badischtauchen.de/> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://x-schreck.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pharmabox.de>
{'id': '244', 'url': 'http://pharmabox.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 03 Mar 2020 18:24:35 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://x-schreck.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://meditationkisse.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ws-gmbh.de>
{'id': '206', 'url': 'http://ws-gmbh.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://atlantis-magazin.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://future-fashion.de>
{'id': '247', 'url': 'http://future-fashion.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 05 Jan 2016 16:33:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ostsea.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dukartstein.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 https://badischtauchen.de/>
{'id': '242', 'url': 'https://badischtauchen.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 18 Sep 2021 11:27:27 GMT', 'tableLayout': False, 'ssl_name': 'badischtauchen.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ostsea.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://x-schreck.de>
{'id': '248', 'url': 'http://x-schreck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 06 Oct 2007 18:12:18 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.okv-weiden.de/robots.txt> from <GET http://okv-weiden.de/robots.txt>
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.org/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://birtekaufmann-photography.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-lexikon.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://atlantis-magazin.de>
{'id': '231', 'url': 'http://atlantis-magazin.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bioticon.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://meditationkisse.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dukartstein.de>
{'id': '152', 'url': 'http://dukartstein.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/robots.txt> from <GET http://www.okv-weiden.de/robots.txt>
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://klangschalen-schop.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bioticon.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ostsea.de>
{'id': '251', 'url': 'http://ostsea.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 15 Jan 2017 21:15:54 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.dknv.de/> from <GET http://dknv.de/robots.txt>
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://birtekaufmann-photography.de>
{'id': '215', 'url': 'http://birtekaufmann-photography.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://meditationkisse.de>
{'id': '249', 'url': 'http://meditationkisse.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-lexikon.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bioticon.de>
{'id': '253', 'url': 'http://bioticon.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikonblog.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.okv-weiden.de/> from <GET http://okv-weiden.de>
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://klangschalen-schop.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikonblog.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://has-wbe.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/robots.txt> from <GET http://www.okv-weiden.de/robots.txt>
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://michaelbuschke.com/robots.txt> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-lexikon.de>
{'id': '250', 'url': 'http://arbeitsbuehnen-lexikon.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.org> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2Frobots.txt&bp-auth=1&action=bpnoaccess> (referer: None)
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 9 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 131 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 145 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 149 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 157 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 159 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 166 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 169 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 177 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://michaelbuschke.com> (referer: None)
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/> from <GET http://sememo.de>
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/> (referer: None)
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.dknv.de/> from <GET http://dknv.de>
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/> from <GET http://www.okv-weiden.de/>
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taxi-meile.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 104 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 108 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 112 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 118 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 130 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 132 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 138 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 140 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 146 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 162 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 166 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 168 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 170 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 174 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 176 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 178 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 182 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 184 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 188 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 190 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 192 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 196 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 200 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 204 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 206 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 208 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 210 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 212 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 214 without any user agent to enforce it on.
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arztfragenonline.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://klangschalen-schop.de>
{'id': '254', 'url': 'http://klangschalen-schop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikonblog.de>
{'id': '257', 'url': 'http://biotikonblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sound-producer.com/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://barbaraundjan.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:08:28 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kulturbeutelhamburg.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.org>
{'id': '238', 'url': 'http://govido.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arztfragenonline.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://michaelbuschke.com>
{'id': '258', 'url': 'http://michaelbuschke.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 07 Sep 2019 22:37:35 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://flugzeuge-weltweit.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://peking-laufenburg.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westsidemusic.com/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://barbaraundjan.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-fragen-online.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://has-wbe.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://peking-laufenburg.de> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-fragen-online.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila.de>
{'id': '260', 'url': 'http://casamobila.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.at/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://volki.pb.photographyrobots.txt> from <GET http://galerie-bauer.de/robots.txt>
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nomadpublishing.store/robots.txt> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arztfragenonline.de>
{'id': '261', 'url': 'http://arztfragenonline.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.at> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://was-ist-opc.de>
{'id': '262', 'url': 'http://was-ist-opc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobila24.eu/robots.txt> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westsidemusic.com> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nomadpublishing.store> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://barbaraundjan.de>
{'id': '259', 'url': 'http://barbaraundjan.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://has-wbe.de>
{'id': '255', 'url': 'http://has-wbe.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://flugzeuge-weltweit.de> (referer: None)
2023-05-02 12:08:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://grinberg.de/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobila24.eu> (referer: None)
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://peking-laufenburg.de>
{'id': '264', 'url': 'http://peking-laufenburg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 24 Aug 2020 11:50:05 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arzt-fragen-online.de>
{'id': '266', 'url': 'http://arzt-fragen-online.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.at>
{'id': '267', 'url': 'http://biotikon.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taxi-meile.de> (referer: None)
2023-05-02 12:08:28 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://westsidemusic.com>
{'id': '265', 'url': 'http://westsidemusic.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nomadpublishing.store>
{'id': '270', 'url': 'http://nomadpublishing.store', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.okv-weiden.de/>
{'id': '252', 'url': 'https://www.okv-weiden.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 02 May 2023 12:08:28 GMT', 'tableLayout': False, 'ssl_name': 'www.okv-weiden.de', 'ssl_start': '20230522235959Z', 'ssl_expire': '20230522235959Z'}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://curcuma-nebenwirkungen.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://aus-keil-wird-schmidt.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gasthof-rolfes.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:08:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://grinberg.de/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://grinberg.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://curcuma-nebenwirkungen.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://aus-keil-wird-schmidt.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://indiaschop.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tropicanalife.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://holzmoebel-experte.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://flugzeuge-weltweit.de>
{'id': '263', 'url': 'http://flugzeuge-weltweit.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.eu/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobila24.eu>
{'id': '271', 'url': 'http://casademobila24.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www6.dknv.de/>
{'id': '256', 'url': 'http://www6.dknv.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taxi-meile.de>
{'id': '246', 'url': 'http://taxi-meile.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grinberg.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tropicanalife.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.eu> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holzmoebel-experte.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzteonlinefragen-unb.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cradle-software.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://curcuma-nebenwirkungen.de>
{'id': '273', 'url': 'http://curcuma-nebenwirkungen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://aus-keil-wird-schmidt.de>
{'id': '274', 'url': 'http://aus-keil-wird-schmidt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 22 Apr 2008 07:31:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://strumpfaffen.org/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzteonlinefragen-unb.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bergium.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://strumpfaffen.org> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://indiaschop.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bergium.de> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://grinberg.de>
{'id': '272', 'url': 'http://grinberg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 28 Jan 2023 20:32:03 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thebodyasarchive.com/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tropicanalife.de>
{'id': '276', 'url': 'http://tropicanalife.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 28 Mar 2009 22:52:40 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativopc.eu>
{'id': '277', 'url': 'http://3nativopc.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://holzmoebel-experte.de>
{'id': '278', 'url': 'http://holzmoebel-experte.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.kg-muellekolk.de/robots.txt> from <GET http://kg-muellekolk.de/robots.txt>
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thebodyasarchive.com> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.ch/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzteonlinefragen-unb.de>
{'id': '280', 'url': 'http://xn--rzteonlinefragen-unb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.ch> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-babyzimmer.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://strumpfaffen.org>
{'id': '282', 'url': 'http://strumpfaffen.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 23 Apr 2012 11:57:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://indiaschop.de>
{'id': '275', 'url': 'http://indiaschop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bergium.de>
{'id': '283', 'url': 'http://bergium.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 07 Nov 2017 13:41:54 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://studio-balance.at/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-babyzimmer.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondrien-funktion.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://studio-balance.at> (referer: None)
2023-05-02 12:08:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess> from <GET https://sememo.de/>
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thebodyasarchive.com>
{'id': '286', 'url': 'http://thebodyasarchive.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 27 Dec 2020 18:13:23 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondrien-funktion.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.com/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-shop-paderborn.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://reitanlage-riedmuehle.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.ch>
{'id': '287', 'url': 'http://biotikon.ch', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.com> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://format-mehrweg.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cosmecon.eu/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-babyzimmer.de>
{'id': '288', 'url': 'http://massivholz-babyzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://format-mehrweg.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-dielenmoebel.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--kleinegrashpfer-9vb.at/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cosmecon.eu> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://studio-balance.at>
{'id': '285', 'url': 'http://studio-balance.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 30 Dec 2020 13:19:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cradle-software.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-dielenmoebel.de> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mitochondrien-funktion.de>
{'id': '290', 'url': 'http://mitochondrien-funktion.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--kleinegrashpfer-9vb.at> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-alpha-ketoglutarat.com>
{'id': '291', 'url': 'http://arginin-alpha-ketoglutarat.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-shop-paderborn.de> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://format-mehrweg.de>
{'id': '292', 'url': 'http://format-mehrweg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 10 Dec 2019 16:45:18 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.gasthof-rolfes.de/> from <GET http://gasthof-rolfes.de>
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-lothar-wester.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://skiverband-mv.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cosmecon.eu>
{'id': '293', 'url': 'http://cosmecon.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 25 Nov 2009 14:28:26 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://reitanlage-riedmuehle.de> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cradle-software.de>
{'id': '281', 'url': 'http://cradle-software.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-dielenmoebel.de>
{'id': '294', 'url': 'http://massivholz-dielenmoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://eventation.de/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--kleinegrashpfer-9vb.at>
{'id': '295', 'url': 'http://xn--kleinegrashpfer-9vb.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 13 Apr 2023 07:16:11 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dinnerticket.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ticila.com/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://grothues8.de/Wordpress/> from <GET http://grothues8.de>
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-shop-paderborn.de>
{'id': '289', 'url': 'http://yoga-shop-paderborn.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ticila.com> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://seat-factory.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://skiverband-mv.de> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://reitanlage-riedmuehle.de>
{'id': '284', 'url': 'http://reitanlage-riedmuehle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://eventation.de/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://eventation.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://seat-factory.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://technolog-e.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-lothar-wester.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://technolog-e.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dinnerticket.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mcpicchu.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ticila.com>
{'id': '300', 'url': 'http://ticila.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 05 Jan 2016 16:33:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://eventation.de> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-datenbank.net/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://skiverband-mv.de>
{'id': '298', 'url': 'http://skiverband-mv.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://trengo.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sound-producer.com> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mcpicchu.de> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://seat-factory.de>
{'id': '301', 'url': 'http://seat-factory.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 15 Jul 2015 14:10:37 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-lothar-wester.de>
{'id': '297', 'url': 'http://yoga-lothar-wester.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dirkklages.com/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://technolog-e.de>
{'id': '302', 'url': 'http://technolog-e.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 07 Nov 2014 10:40:41 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dirkklages.com> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lederoutfits.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dinnerticket.de>
{'id': '296', 'url': 'http://dinnerticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://eventation.de>
{'id': '299', 'url': 'http://eventation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-datenbank.net> (referer: None)
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lederoutfits.de> (referer: None)
2023-05-02 12:08:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.nachhilfe-oase.de/robots.txt> from <GET http://123discounter.de/robots.txt>
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://sound-producer.com>
{'id': '203', 'url': 'http://sound-producer.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mcpicchu.de>
{'id': '304', 'url': 'http://mcpicchu.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 23 Jul 2022 22:42:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trengo.de> (referer: None)
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dirkklages.com>
{'id': '307', 'url': 'http://dirkklages.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 17 Apr 2021 09:45:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-plomari.com/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cymeg.de/robots.txt> (referer: None)
2023-05-02 12:08:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.nachhilfe-oase.de/> from <GET http://123discounter.de>
2023-05-02 12:08:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-datenbank.net>
{'id': '303', 'url': 'http://auto-datenbank.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lederoutfits.de>
{'id': '308', 'url': 'http://lederoutfits.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Nov 2017 23:18:52 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://teamtrzweb.de> (referer: None)
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://die-goldene-schallplatte.de/robots.txt> (referer: None)
2023-05-02 12:08:30 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 780: invalid start byte
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cymeg.de> (referer: None)
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.eu/robots.txt> (referer: None)
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/robots.txt> (referer: None)
2023-05-02 12:08:30 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://trengo.de>
{'id': '306', 'url': 'http://trengo.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess> (referer: None)
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.gasthof-rolfes.de/robots.txt> (referer: None)
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.eu> (referer: None)
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/> (referer: None)
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-plomari.com> (referer: None)
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://schiefer-lautsprecher.de/robots.txt> (referer: None)
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://teamtrzweb.de>
{'id': '305', 'url': 'http://teamtrzweb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cymeg.de>
{'id': '313', 'url': 'http://cymeg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 17 Apr 2021 09:45:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess>
{'id': '174', 'url': 'https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess', 'status': 200, 'title': 'Anmelden ‹ sememo — WordPress', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'sememo.de', 'ssl_start': '20230710235959Z', 'ssl_expire': '20230710235959Z'}
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dark-hawks.de/robots.txt> (referer: None)
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://die-goldene-schallplatte.de> (referer: None)
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativ-opc.eu>
{'id': '314', 'url': 'http://3nativ-opc.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.nachhilfe-oase.de/>
{'id': '311', 'url': 'http://www.nachhilfe-oase.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-plomari.com>
{'id': '309', 'url': 'http://ouzo-plomari.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongsblog.de/robots.txt> (referer: None)
2023-05-02 12:08:30 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:08:30 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:08:30 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:08:30 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.kg-muellekolk.de/> from <GET http://kg-muellekolk.de>
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://die-goldene-schallplatte.de>
{'id': '312', 'url': 'http://die-goldene-schallplatte.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schiefer-lautsprecher.de> (referer: None)
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schiefer-lautsprecher.de>
{'id': '315', 'url': 'http://schiefer-lautsprecher.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongsblog.de> (referer: None)
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/robots.txt> (referer: None)
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dejongsblog.de>
{'id': '310', 'url': 'http://dejongsblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dark-hawks.de> (referer: None)
2023-05-02 12:08:30 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://kulturbeutelhamburg.de/> from <GET http://kulturbeutelhamburg.de>
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dark-hawks.de>
{'id': '279', 'url': 'http://dark-hawks.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.gasthof-rolfes.de/> (referer: None)
2023-05-02 12:08:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.gasthof-rolfes.de/>
{'id': '239', 'url': 'http://www.gasthof-rolfes.de/', 'status': 200, 'title': 'Gasthof Rolfes ', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grothues8.de/Wordpress/> (referer: None)
2023-05-02 12:08:31 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:31 [scrapy.core.scraper] DEBUG: Scraped from <200 http://grothues8.de/Wordpress/>
{'id': '222', 'url': 'http://grothues8.de/Wordpress/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/> (referer: None)
2023-05-02 12:08:31 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:31 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.kg-muellekolk.de/>
{'id': '268', 'url': 'http://www.kg-muellekolk.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kulturbeutelhamburg.de/> (referer: None)
2023-05-02 12:08:32 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://kulturbeutelhamburg.de/>
{'id': '147', 'url': 'https://kulturbeutelhamburg.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'kulturbeutelhamburg.de', 'ssl_start': '20230605235959Z', 'ssl_expire': '20230605235959Z'}
2023-05-02 12:08:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://photos.hr-photo.derobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:08:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.pfotenabdruck.inforobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:08:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.formativmedia.atrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:08:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.exelant.derobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:08:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.medizinrechtler.inforobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:08:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.mattner.orgrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:08:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://volki.pb.photographyrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:08:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://photos.hr-photo.derobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:08:55 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://hr-photo.de/robots.txt>: DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:08:55 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://photos.hr-photo.de> from <GET http://hr-photo.de>
2023-05-02 12:08:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://photos.hr-photo.de/robots.txt> (referer: None)
2023-05-02 12:08:55 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 771: invalid start byte
2023-05-02 12:08:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://photos.hr-photo.de> (referer: None)
2023-05-02 12:08:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://photos.hr-photo.de>
{'id': '24', 'url': 'http://photos.hr-photo.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:08:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.pfotenabdruck.inforobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:08:56 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://pfoten-abdruck.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:08:56 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.pfotenabdruck.info> from <GET http://pfoten-abdruck.de>
2023-05-02 12:08:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.pfotenabdruck.info/robots.txt> (referer: None)
2023-05-02 12:08:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.pfotenabdruck.info> (referer: None)
2023-05-02 12:08:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.pfotenabdruck.info>
{'id': '167', 'url': 'https://www.pfotenabdruck.info', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 21 Apr 2020 09:28:05 GMT', 'tableLayout': True, 'ssl_name': 'pfotenabdruck.info', 'ssl_start': '20230609235959Z', 'ssl_expire': '20230609235959Z'}
2023-05-02 12:08:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://www.formativmedia.atrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:08:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://formativ-print.at/robots.txt>: DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:08:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.formativmedia.at> from <GET http://formativ-print.at>
2023-05-02 12:08:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.exelant.derobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:08:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://exelant.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:08:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.exelant.de> from <GET http://exelant.de>
2023-05-02 12:08:58 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.exelant.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.exelant.de'))])
2023-05-02 12:08:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.exelant.de/robots.txt> (referer: None)
2023-05-02 12:08:58 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:08:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.exelant.de> (referer: None)
2023-05-02 12:08:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:08:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.exelant.de>
{'id': '201', 'url': 'https://www.exelant.de', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:08:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.formativmedia.at/robots.txt> (referer: None)
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 81 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:08:59 [protego] DEBUG: Rule at line 109 without any user agent to enforce it on.
2023-05-02 12:08:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.formativmedia.at/> from <GET http://www.formativmedia.at>
2023-05-02 12:09:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://volki.pb.photographyrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:09:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://galerie-bauer.de/robots.txt>: DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:09:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://volki.pb.photography> from <GET http://galerie-bauer.de>
2023-05-02 12:09:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://www.mattner.orgrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:09:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://mattner.org/robots.txt>: DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:09:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.mattner.org> from <GET http://mattner.org>
2023-05-02 12:09:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.mattner.org/robots.txt> (referer: None)
2023-05-02 12:09:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.medizinrechtler.inforobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:09:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://grad-der-behinderung.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:09:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.formativmedia.at/> (referer: None)
2023-05-02 12:09:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://volki.pb.photography/robots.txt> (referer: None)
2023-05-02 12:09:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.mattner.org> (referer: None)
2023-05-02 12:09:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.medizinrechtler.info> from <GET http://grad-der-behinderung.de>
2023-05-02 12:09:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://volki.pb.photography> (referer: None)
2023-05-02 12:09:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.formativmedia.at/>
{'id': '196', 'url': 'https://www.formativmedia.at/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'einfachwanda.at', 'ssl_start': '20230515034747Z', 'ssl_expire': '20230515034747Z'}
2023-05-02 12:09:00 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.medizinrechtler.info"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.medizinrechtler.info'))])
2023-05-02 12:09:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.mattner.org>
{'id': '245', 'url': 'http://www.mattner.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://volki.pb.photography>
{'id': '269', 'url': 'https://volki.pb.photography', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': '*.pb.photography', 'ssl_start': '20240222235959Z', 'ssl_expire': '20240222235959Z'}
2023-05-02 12:09:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.medizinrechtler.info/robots.txt> (referer: None)
2023-05-02 12:09:00 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:09:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.medizinrechtler.info> (referer: None)
2023-05-02 12:09:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.medizinrechtler.info>
{'id': '234', 'url': 'https://www.medizinrechtler.info', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:09:01 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:09:01 [scrapy.extensions.feedexport] INFO: Stored jl feed (314 items) in: baddata_results_20230502_120822.jl
2023-05-02 12:09:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 14,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 14,
 'downloader/request_bytes': 172983,
 'downloader/request_count': 762,
 'downloader/request_method_count/GET': 762,
 'downloader/response_bytes': 2696797,
 'downloader/response_count': 748,
 'downloader/response_status_count/200': 469,
 'downloader/response_status_count/301': 27,
 'downloader/response_status_count/302': 66,
 'downloader/response_status_count/404': 174,
 'downloader/response_status_count/500': 6,
 'downloader/response_status_count/503': 6,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 38.192777,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 9, 1, 158755),
 'httpcompression/response_bytes': 4811764,
 'httpcompression/response_count': 353,
 'item_scraped_count': 314,
 'log_count/DEBUG': 1799,
 'log_count/ERROR': 21,
 'log_count/INFO': 11,
 'log_count/WARNING': 339,
 'memusage/max': 66617344,
 'memusage/startup': 66617344,
 'response_received_count': 646,
 'retry/count': 13,
 'retry/max_reached': 13,
 'retry/reason_count/500 Internal Server Error': 3,
 'retry/reason_count/503 Service Unavailable': 3,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 7,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 7,
 'robotstxt/request_count': 338,
 'robotstxt/response_count': 331,
 'robotstxt/response_status_count/200': 152,
 'robotstxt/response_status_count/404': 174,
 'robotstxt/response_status_count/500': 3,
 'robotstxt/response_status_count/503': 2,
 'scheduler/dequeued': 368,
 'scheduler/dequeued/memory': 368,
 'scheduler/enqueued': 368,
 'scheduler/enqueued/memory': 368,
 'start_time': datetime.datetime(2023, 5, 2, 12, 8, 22, 965978)}
2023-05-02 12:09:01 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:09:13 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:09:13 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:09:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:09:13 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:09:13 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:09:13 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:09:13 [scrapy.extensions.telnet] INFO: Telnet Password: cfa047d5ecb800c0
2023-05-02 12:09:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:09:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:09:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:09:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:09:13 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:09:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:09:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfs-europe.de/robots.txt> from <GET http://pfs-europe.de/robots.txt>
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://lisaglauer.com/robots.txt> from <GET http://lisaglauer.de/robots.txt>
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-wohnzimmer.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.be/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lahres.com/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mgvliederkranz-asbach.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://docs.moodle.org:443/robots.txt> from <GET http://docs.moodle.org/robots.txt>
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sprachlos-ev-beratung.de/robots.txt> from <GET http://sprachlos-ev-beratung.de/robots.txt>
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://kaffeeundservice.at/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hausegger.net/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.be> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lahres.com> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-wohnzimmer.de> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mgvliederkranz-asbach.de> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hausegger.net> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mixable.media/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:09:13 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "pfs-europe.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'pfs-europe.de'))])
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://kaffeeundservice.at/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://kaffeeundservice.at/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET http://lifttaxi.com> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mind-holiday.com/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://steigertaxi.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfs-europe.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://docs.moodle.org:443/2x/pl/Flash> from <GET http://docs.moodle.org/2x/pl/Flash>
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mixable.media> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://bayern-immobilie.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://kaliner-yoga.de/robots.txt> from <GET http://kaliner-yoga.de/robots.txt>
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfs-europe.de/> from <GET http://pfs-europe.de>
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-teamblog.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://efwe-art.at/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:09:13 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:13 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.be>
{'id': '7', 'url': 'http://biotikon.be', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://karo-stimme.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lifttaxi.com/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:09:13 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:13 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lahres.com>
{'id': '15', 'url': 'http://lahres.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 25 Dec 2019 08:39:49 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-teamblog.de> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfs-europe.de/> (referer: None)
2023-05-02 12:09:13 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-wohnzimmer.de>
{'id': '13', 'url': 'http://massivholz-wohnzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:13 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mgvliederkranz-asbach.de>
{'id': '11', 'url': 'http://mgvliederkranz-asbach.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 28 Jan 2009 10:53:37 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://photos.hr-photo.derobots.txt> from <GET http://hr-photo.de/robots.txt>
2023-05-02 12:09:13 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:13 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hausegger.net>
{'id': '12', 'url': 'http://hausegger.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 12 Sep 2012 11:18:42 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://coaching-im-alltag.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://lisaglauer.com/> from <GET http://lisaglauer.de>
2023-05-02 12:09:13 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:13 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mixable.media>
{'id': '17', 'url': 'http://mixable.media', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mind-holiday.com> (referer: None)
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.dienstleistung-solar.de/robots.txt> from <GET http://dienstleistung-solar.de/robots.txt>
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gbv-grosskarolinenfeld.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://steigertaxi.de> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://billardcafe-suedpark.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bayern-immobilie.de> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://efwe-art.at> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> (referer: None)
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 463 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 469 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 472 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 507 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 513 without any user agent to enforce it on.
2023-05-02 12:09:13 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:09:13 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:13 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon-teamblog.de>
{'id': '23', 'url': 'http://biotikon-teamblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://pfs-europe.de/>
{'id': '3', 'url': 'https://pfs-europe.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 03 Aug 2019 10:52:54 GMT', 'tableLayout': True, 'ssl_name': 'fun4-u.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:09:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET http://www.purpur.eatbu.com/?lang=de> from <GET http://billardcafe-suedpark.de>
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://karo-stimme.de> (referer: None)
2023-05-02 12:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lifttaxi.com/> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '25', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.wassersportcenter-heiligenhafen.de/> from <GET http://wassersportcenter-heiligenhafen.de>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaukus.org/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mind-holiday.com>
{'id': '4', 'url': 'http://mind-holiday.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://steigertaxi.de>
{'id': '8', 'url': 'http://steigertaxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.purpur.eatbu.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET http://kaukus.org/index2.html> from <GET http://kaukus.org>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essen-auf-raedern-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kilthau.tech/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gbv-grosskarolinenfeld.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coaching-im-alltag.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bayern-immobilie.de>
{'id': '6', 'url': 'http://bayern-immobilie.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://efwe-art.at>
{'id': '5', 'url': 'http://efwe-art.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.purpur.eatbu.com/?lang=de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.com> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://seelenfeder.at/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaffeeundservice.at> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaukus.org/index2.html> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://karo-stimme.de>
{'id': '9', 'url': 'http://karo-stimme.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lifttaxi.com/>
{'id': '1', 'url': 'http://lifttaxi.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gartengestaltung-brandner.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 781: invalid start byte
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.info>
{'id': '31', 'url': 'http://wasistopc.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://seelenfeder.at> (referer: None)
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://drey.info/robots.txt> from <GET http://drey.info/robots.txt>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://illger.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 1 times): 503 Service Unavailable
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dobbrunz.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cellhomoeostasis.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-star.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 463 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 469 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 472 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 507 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 513 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.sprachlos-ev-beratung.de/robots.txt> from <GET https://sprachlos-ev-beratung.de/robots.txt>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://byzickl.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://illger.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gbv-grosskarolinenfeld.de>
{'id': '19', 'url': 'http://gbv-grosskarolinenfeld.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coaching-im-alltag.de>
{'id': '18', 'url': 'http://coaching-im-alltag.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/2x/pl/Flash> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cellhomoeostasis.com> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kilthau.tech> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dobbrunz.com> (referer: None)
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/> from <GET http://www.wassersportcenter-heiligenhafen.de/>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://magnonics.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fmz-frankershausen.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://byzickl.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.purpur.eatbu.com/?lang=de>
{'id': '30', 'url': 'http://www.purpur.eatbu.com/?lang=de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativ-opc.com>
{'id': '33', 'url': 'http://3nativ-opc.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 2 times): 503 Service Unavailable
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kaffeeundservice.at>
{'id': '14', 'url': 'http://kaffeeundservice.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kaukus.org/index2.html>
{'id': '32', 'url': 'http://kaukus.org/index2.html', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2009 20:14:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://seelenfeder.at>
{'id': '35', 'url': 'http://seelenfeder.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 10 Mar 2018 15:51:03 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lisaglauer.com/>
{'id': '2', 'url': 'http://lisaglauer.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://magnonics.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://illger.de>
{'id': '37', 'url': 'http://illger.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 07 Nov 2011 11:18:19 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://docs.moodle.org:443/2x/pl/Flash>
{'id': '0', 'url': 'https://docs.moodle.org:443/2x/pl/Flash', 'status': 200, 'title': 'Flash – MoodleDocs', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 24 Apr 2023 19:09:27 GMT', 'tableLayout': False, 'ssl_name': 'sni.cloudflaressl.com', 'ssl_start': '20240501235959Z', 'ssl_expire': '20240501235959Z'}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cellhomoeostasis.com>
{'id': '41', 'url': 'http://cellhomoeostasis.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kilthau.tech>
{'id': '34', 'url': 'http://kilthau.tech', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dobbrunz.com>
{'id': '40', 'url': 'http://dobbrunz.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 03 Jul 2016 18:53:33 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://diekicktipper.de/robots.txt> from <GET http://diekicktipper.de/robots.txt>
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.dienstleistung-solar.de/> from <GET http://dienstleistung-solar.de>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ts-it-service.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schoolscout24.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://drey.info/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-star.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dj-sb.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://byzickl.de>
{'id': '36', 'url': 'http://byzickl.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gesundheitsforum-norderstedt.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://drey.info/> from <GET http://drey.info>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-flurmoebel.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://malttec.net/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schoolscout24.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dj-sb.com> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ts-it-service.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kanzlei-sauerwein.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hgwimmobilien.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gesundheitsforum-norderstedt.com> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yogashop-paderborn.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://drey.info/> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://magnonics.de>
{'id': '42', 'url': 'http://magnonics.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 26 Aug 2009 08:33:56 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET https://teamtrzweb.de> from <GET http://malttec.net>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-flurmoebel.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gartengestaltung-brandner.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.wassersportcenter-heiligenhafen.de/>
{'id': '16', 'url': 'https://www.wassersportcenter-heiligenhafen.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'wassersportcenter-heiligenhafen.de', 'ssl_start': '20230719235959Z', 'ssl_expire': '20230719235959Z'}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kanzlei-sauerwein.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.net/robots.txt> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-star.de>
{'id': '39', 'url': 'http://yoga-star.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kaliner-yoga.de/robots.txt> from <GET https://kaliner-yoga.de/robots.txt>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essen-auf-raedern-willebadessen.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wetzel.work/robots.txt> (referer: None)
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 213 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 214 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 234 without any user agent to enforce it on.
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.net> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://diekicktipper.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schoolscout24.de>
{'id': '46', 'url': 'http://schoolscout24.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 03 May 2011 19:33:17 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 1 times): 503 Service Unavailable
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dj-sb.com>
{'id': '47', 'url': 'http://dj-sb.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 28 Dec 2014 13:42:08 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wetzel.work> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ts-it-service.de>
{'id': '45', 'url': 'http://ts-it-service.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-bueromoebel.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://goting-kliff53.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://diekicktipper.de/> from <GET http://diekicktipper.de>
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gesundheitsforum-norderstedt.com>
{'id': '48', 'url': 'http://gesundheitsforum-norderstedt.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 26 Mar 2012 15:32:11 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://drey.info/>
{'id': '38', 'url': 'https://drey.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 17 Mar 2014 08:12:11 GMT', 'tableLayout': True, 'ssl_name': 'drey.info', 'ssl_start': '20230705235959Z', 'ssl_expire': '20230705235959Z'}
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-flurmoebel.de>
{'id': '50', 'url': 'http://massivholz-flurmoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gartengestaltung-brandner.de>
{'id': '28', 'url': 'http://gartengestaltung-brandner.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-online-fragen-ktb.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://goting-kliff53.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-bueromoebel.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hgwimmobilien.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://buehnen-taxi.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kanzlei-sauerwein.de>
{'id': '51', 'url': 'http://kanzlei-sauerwein.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 03 Jan 2010 02:56:47 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-online-fragen-ktb.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://essen-auf-raedern-willebadessen.de>
{'id': '27', 'url': 'http://essen-auf-raedern-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://diekicktipper.de/> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://teamtrzweb.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.net>
{'id': '55', 'url': 'http://biotikon.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yogashop-paderborn.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wetzel.work>
{'id': '56', 'url': 'http://wetzel.work', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 14 Aug 2017 17:23:07 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bce-clan.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://bavarian-starlights.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.info/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-pilavas.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://goting-kliff53.de>
{'id': '60', 'url': 'http://goting-kliff53.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 27 Jul 2022 11:59:55 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenheim-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://xn--edelstahlscheckkartenhlle-0wc.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.info> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-bueromoebel.de>
{'id': '59', 'url': 'http://massivholz-bueromoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hgwimmobilien.de>
{'id': '43', 'url': 'http://hgwimmobilien.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bauelemente-riede.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzte-online-fragen-ktb.de>
{'id': '61', 'url': 'http://xn--rzte-online-fragen-ktb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-makler.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 2 times): 503 Service Unavailable
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://buehnen-taxi.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ciber-man.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://diekicktipper.de/>
{'id': '44', 'url': 'https://diekicktipper.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'diekicktipper.de', 'ssl_start': '20230527235959Z', 'ssl_expire': '20230527235959Z'}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bauelemente-riede.de> (referer: None)
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sprachlos-ev-beratung.de/> from <GET http://sprachlos-ev-beratung.de>
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yogashop-paderborn.de>
{'id': '52', 'url': 'http://yogashop-paderborn.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ciber-man.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bce-clan.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://xn--holzmbel-experte-qwb.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/robots.txt> from <GET http://cyberfiber.me/robots.txt>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-pilavas.com> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://was-ist-opc.info>
{'id': '65', 'url': 'http://was-ist-opc.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--holzmbel-experte-qwb.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://schluesselvereinzler.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--edelstahlscheckkartenhlle-0wc.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://buehnentaxi.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://buehnen-taxi.de>
{'id': '53', 'url': 'http://buehnen-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bauelemente-riede.de>
{'id': '69', 'url': 'http://bauelemente-riede.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 16 May 2018 20:42:31 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://style-and-smile.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fmz-frankershausen.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ciber-man.de>
{'id': '70', 'url': 'http://ciber-man.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 20 Jan 2013 06:17:41 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://openlate.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:09:14 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-makler.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bce-clan.de>
{'id': '64', 'url': 'http://bce-clan.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://huckbros.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://da365.de/robots.txt> from <GET http://da365.de/robots.txt>
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-qigong-master.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenheim-willebadessen.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mobile-mietstation.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 774: invalid start byte
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-markt.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 776: invalid start byte
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-pilavas.com>
{'id': '57', 'url': 'http://ouzo-pilavas.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://huckbros.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-jugendzimmer.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-qigong-master.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--holzmbel-experte-qwb.de>
{'id': '74', 'url': 'http://xn--holzmbel-experte-qwb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--edelstahlscheckkartenhlle-0wc.de>
{'id': '67', 'url': 'http://xn--edelstahlscheckkartenhlle-0wc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ignaz-schwarzbach.eu/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/> (failed 1 times): 503 Service Unavailable
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schluesselvereinzler.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://marco-k.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-jugendzimmer.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teamtrzweb.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ignaz-schwarzbach.eu> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila.info/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://buehnentaxi.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gzlw.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fmz-frankershausen.de>
{'id': '20', 'url': 'http://fmz-frankershausen.de', 'status': 200, 'title': 'Fanfaren- und Musikzug Frankershausen – Freiwillige Feuerwehr Frankershausen 1956 e.V.', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-makler.de>
{'id': '63', 'url': 'http://arbeitsbuehnen-makler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://marco-k.com> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.net/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila.info> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://altenheim-willebadessen.de>
{'id': '58', 'url': 'http://altenheim-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gzlw.de> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://no-flush-niacin.com/robots.txt> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://huckbros.de>
{'id': '78', 'url': 'http://huckbros.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Feb 2023 08:29:22 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taichichuan-qigong-master.de>
{'id': '79', 'url': 'http://taichichuan-qigong-master.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-kinderzimmer.de/robots.txt> (referer: None)
2023-05-02 12:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mobile-mietstation.de> (referer: None)
2023-05-02 12:09:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schluesselvereinzler.de>
{'id': '76', 'url': 'http://schluesselvereinzler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://no-flush-niacin.com> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobila24.com/robots.txt> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-jugendzimmer.de>
{'id': '80', 'url': 'http://massivholz-jugendzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.sprachlos-ev-beratung.de/> from <GET https://sprachlos-ev-beratung.de/>
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-markt.de> (referer: None)
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.bavarian-starlights.com/> from <GET http://bavarian-starlights.com>
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kindz.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ganzheitlichesheilen.eu/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://cyberfiber.me/robots.txt> (referer: None)
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-kinderzimmer.de> (referer: None)
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/> (failed 2 times): 503 Service Unavailable
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobila24.com> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://teamtrzweb.de>
{'id': '49', 'url': 'https://teamtrzweb.de', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'biergartenteam.de', 'ssl_start': '20230609051948Z', 'ssl_expire': '20230609051948Z'}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ignaz-schwarzbach.eu>
{'id': '81', 'url': 'http://ignaz-schwarzbach.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 04 Dec 2022 16:28:01 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://buehnentaxi.de>
{'id': '68', 'url': 'http://buehnentaxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmedmichalzik.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmichalzik.eu/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ganzheitlichesheilen.eu> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nows.tk/robots.txt> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://marco-k.com>
{'id': '82', 'url': 'http://marco-k.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 23 Jul 2022 22:42:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila.info>
{'id': '83', 'url': 'http://casamobila.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gzlw.de>
{'id': '85', 'url': 'http://gzlw.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 28 Jan 2009 09:42:45 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/> from <GET http://cyberfiber.me>
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mobile-mietstation.de>
{'id': '71', 'url': 'http://mobile-mietstation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmedmichalzik.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmichalzik.eu> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://no-flush-niacin.com>
{'id': '86', 'url': 'http://no-flush-niacin.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nows.tk> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://boncomputa.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lift-taxi.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://kaliner-yoga.de/> from <GET http://kaliner-yoga.de>
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://www.bavarian-starlights.com/robots.txt> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-markt.de>
{'id': '72', 'url': 'http://arbeitsbuehnen-markt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rztefragenonline-unb.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-kinderzimmer.de>
{'id': '87', 'url': 'http://massivholz-kinderzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.org/robots.txt> (referer: None)
2023-05-02 12:09:15 [dubdev] ERROR: HttpError on https://www.dienstleistung-solar.de/
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://westside-linedance.com/robots.txt> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobila24.com>
{'id': '89', 'url': 'http://casademobila24.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rztefragenonline-unb.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.org> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hgwimmobilien.org/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://edelstahlscheckkartenhuelle.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westside-linedance.com> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ganzheitlichesheilen.eu>
{'id': '90', 'url': 'http://ganzheitlichesheilen.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wwb1.net/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-friedrichwipfel.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.net> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://doc-blog.org/robots.txt> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://drmedmichalzik.de>
{'id': '91', 'url': 'http://drmedmichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wwb1.net> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://drmichalzik.eu>
{'id': '93', 'url': 'http://drmichalzik.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-alexander-michalzik.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nows.tk>
{'id': '94', 'url': 'http://nows.tk', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 06 Jun 2018 13:57:15 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-friedrichwipfel.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://strukturplatten.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://doc-blog.org> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-nektar.com/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-alexander-michalzik.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://boncomputa.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://da365.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tibetanische-klangschalen.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rztefragenonline-unb.de>
{'id': '95', 'url': 'http://xn--rztefragenonline-unb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://wvm-branchenportal.com/robots.txt> from <GET http://wvm-branchenportal.com/robots.txt>
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://cyberfiber.me/> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.org>
{'id': '97', 'url': 'http://biotikon.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://openlate.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://strukturplatten.de> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://westside-linedance.com>
{'id': '99', 'url': 'http://westside-linedance.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 29 Jan 2020 19:02:17 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://edelstahlscheckkartenhuelle.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobilashop.eu/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hgwimmobilien.org> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lift-taxi.de> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.net>
{'id': '75', 'url': 'http://govido.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wwb1.net>
{'id': '102', 'url': 'http://wwb1.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 30 Sep 2019 13:35:47 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://sulden-apresski.com/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://impax.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobilashop.eu> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ateliereichner.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taichichuan-friedrichwipfel.de>
{'id': '103', 'url': 'http://taichichuan-friedrichwipfel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://doc-blog.org>
{'id': '104', 'url': 'http://doc-blog.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dr-alexander-michalzik.de>
{'id': '105', 'url': 'http://dr-alexander-michalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://boncomputa.de>
{'id': '88', 'url': 'http://boncomputa.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sulden-apresski.com> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tibetanische-klangschalen.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-nektar.com> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mastershaft.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://impax.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.info/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ateliereichner.de> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://cyberfiber.me/>
{'id': '73', 'url': 'https://cyberfiber.me/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'cyberfiber.me', 'ssl_start': '20220206235959Z', 'ssl_expire': '20220206235959Z'}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://openlate.de>
{'id': '54', 'url': 'http://openlate.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://strukturplatten.de>
{'id': '106', 'url': 'http://strukturplatten.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 04 Aug 2021 09:44:09 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://edelstahlscheckkartenhuelle.de>
{'id': '101', 'url': 'http://edelstahlscheckkartenhuelle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hgwimmobilien.org>
{'id': '92', 'url': 'http://hgwimmobilien.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lift-taxi.de>
{'id': '84', 'url': 'http://lift-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wvm-branchenportal.com/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://marcodesigner.com/robots.txt> from <GET http://marcodesigner.com/robots.txt>
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobilashop.eu>
{'id': '110', 'url': 'http://casademobilashop.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://sulden-apresski.com>
{'id': '111', 'url': 'http://sulden-apresski.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 13 Mar 2018 16:25:36 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://da365.de/> from <GET http://da365.de>
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://wvm-branchenportal.com/> from <GET http://wvm-branchenportal.com>
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tibetanische-klangschalen.de>
{'id': '107', 'url': 'http://tibetanische-klangschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-nektar.com>
{'id': '96', 'url': 'http://ouzo-nektar.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-esszimmer.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://impax.de>
{'id': '100', 'url': 'http://impax.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ateliereichner.de>
{'id': '114', 'url': 'http://ateliereichner.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mastershaft.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wvm-branchenportal.com/> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-schlafzimmer.com/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tibetanischeklagschalen.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://carcal.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-esszimmer.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.bavarian-starlights.com/> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hilcura.com/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "marcodesigner.com"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'marcodesigner.com'))])
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kaliner-yoga.de/> from <GET https://kaliner-yoga.de/>
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-schlafzimmer.com> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.org/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://heueu.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://djmirco.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dr-michalzik.at/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nevaton.eu/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.org> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://heueu.de> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mastershaft.de>
{'id': '113', 'url': 'http://mastershaft.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://wvm-branchenportal.com/>
{'id': '108', 'url': 'https://wvm-branchenportal.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 14 Oct 2020 22:24:24 GMT', 'tableLayout': True, 'ssl_name': 'wvm-branchenportal.com', 'ssl_start': '20240307235959Z', 'ssl_expire': '20240307235959Z'}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nowikow.org/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://djmirco.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.info> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-esszimmer.de>
{'id': '118', 'url': 'http://massivholz-esszimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.bavarian-starlights.com/>
{'id': '66', 'url': 'http://www.bavarian-starlights.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://wahnfriet.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.info/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://el-toque-latino.com/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://carcal.de> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-schlafzimmer.com>
{'id': '122', 'url': 'http://massivholz-schlafzimmer.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.sprachlos-ev-beratung.de/>
{'id': '10', 'url': 'https://www.sprachlos-ev-beratung.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'sprachlos-ev-beratung.de', 'ssl_start': '20230711235959Z', 'ssl_expire': '20230711235959Z'}
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-alpha-ketoglutarat.org>
{'id': '126', 'url': 'http://arginin-alpha-ketoglutarat.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://el-toque-latino.com> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.info> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tibetanischeklagschalen.de> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://heueu.de>
{'id': '125', 'url': 'http://heueu.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 12 Sep 2009 08:57:13 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://biotikon.es/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://djmirco.de>
{'id': '127', 'url': 'http://djmirco.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://marcodesigner.com/robots.txt> (referer: None)
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.info>
{'id': '112', 'url': 'http://govido.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-michalzik.at> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://aufklebershop-geislingen.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexander-michalzik.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-online-fragen.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://marcodesigner.com/> from <GET http://marcodesigner.com>
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://carcal.de>
{'id': '123', 'url': 'http://carcal.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexander-michalzik.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://aufklebershop-geislingen.de> (referer: None)
2023-05-02 12:09:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> from <GET http://xn--wohlfhlwerkstatt-nzb.de/robots.txt>
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronamitarbeiterschutz.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-online-fragen.de> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nowikow.org> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://die-goldene-cd.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://el-toque-latino.com>
{'id': '129', 'url': 'http://el-toque-latino.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 07 Sep 2007 23:41:14 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.info>
{'id': '130', 'url': 'http://biotikon.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tibetanischeklagschalen.de>
{'id': '121', 'url': 'http://tibetanischeklagschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://springblade.de/robots.txt> (referer: None)
2023-05-02 12:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wahnfriet.de> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronamitarbeiterschutz.de> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.es> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinoline-quinone.org/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tribulus-saponine.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://edv-frenzel.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://da365.de/> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dr-michalzik.at>
{'id': '117', 'url': 'http://dr-michalzik.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tribulus-saponine.de> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinoline-quinone.org> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://alexander-michalzik.de>
{'id': '133', 'url': 'http://alexander-michalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://delta-mb.de/robots.txt> from <GET http://delta-mb.de/robots.txt>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://edv-frenzel.de> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://aufklebershop-geislingen.de>
{'id': '135', 'url': 'http://aufklebershop-geislingen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Jul 2010 08:21:56 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arzt-online-fragen.de>
{'id': '134', 'url': 'http://arzt-online-fragen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nowikow.org>
{'id': '119', 'url': 'http://nowikow.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-dosierung.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wahnfriet.de>
{'id': '120', 'url': 'http://wahnfriet.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://die-goldene-cd.de> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://abandonedzone.com/robots.txt> (referer: None)
2023-05-02 12:09:16 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:16 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nevaton.eu> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coronamitarbeiterschutz.de>
{'id': '137', 'url': 'http://coronamitarbeiterschutz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-dosierung.de> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.es>
{'id': '128', 'url': 'http://biotikon.es', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://da365.de/>
{'id': '62', 'url': 'https://da365.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'da365.de', 'ssl_start': '20230708235959Z', 'ssl_expire': '20230708235959Z'}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://springblade.de> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://patcy.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://abandonedzone.com> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://marcodesigner.com/> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://formergy.biz/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kindz.de> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tribulus-saponine.de>
{'id': '142', 'url': 'http://tribulus-saponine.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> (referer: None)
2023-05-02 12:09:16 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-blog.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.health/robots.txt> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pyrroloquinoline-quinone.org>
{'id': '140', 'url': 'http://pyrroloquinoline-quinone.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://edv-frenzel.de>
{'id': '141', 'url': 'http://edv-frenzel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> from <GET http://xn--wohlfhlwerkstatt-nzb.de>
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hilcura.de/> from <GET http://hilcura.com>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://style-and-smile.com> (referer: None)
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://huckbros.com/robots.txt> from <GET http://huckbros.com/robots.txt>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-blog.de> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.health> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://die-goldene-cd.de>
{'id': '131', 'url': 'http://die-goldene-cd.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://crazybulls.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nevaton.eu>
{'id': '98', 'url': 'http://nevaton.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://pixxpress.com/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fusionstreet.com/robots.txt> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://resveratrol-dosierung.de>
{'id': '145', 'url': 'http://resveratrol-dosierung.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://springblade.de>
{'id': '139', 'url': 'http://springblade.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://abandonedzone.com>
{'id': '146', 'url': 'http://abandonedzone.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://patcy.de> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://marcodesigner.com/>
{'id': '115', 'url': 'https://marcodesigner.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'die-rohrreinigung.com', 'ssl_start': '20210714235959Z', 'ssl_expire': '20210714235959Z'}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://streamify.tv/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pixxpress.com> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kindz.de>
{'id': '26', 'url': 'http://kindz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://style-and-smile.com>
{'id': '77', 'url': 'http://style-and-smile.com', 'status': 200, 'title': 'WordPress 5.3.0 – Eine weitere WordPress-Website', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://crazybulls.de> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon-blog.de>
{'id': '149', 'url': 'http://biotikon-blog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.health>
{'id': '150', 'url': 'http://biotikon.health', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://huckbros.com/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://formergy.biz> (referer: None)
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://huckbros.com/> from <GET http://huckbros.com>
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://patcy.de>
{'id': '148', 'url': 'http://patcy.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfotenabdruck.info/robots.txt> from <GET http://pfotenabdruck.info/robots.txt>
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pixxpress.com>
{'id': '155', 'url': 'http://pixxpress.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 26 Aug 2011 20:30:29 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.com/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://rustikal-lecker.com/robots.txt> from <GET http://rustikal-lecker.com/robots.txt>
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://crazybulls.de>
{'id': '153', 'url': 'http://crazybulls.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Dec 2022 21:55:50 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.com> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://huckbros.com/> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fusionstreet.com> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://formergy.biz>
{'id': '143', 'url': 'http://formergy.biz', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pflegeplatz-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://pfotenabdruck.info/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dirkmeineke.de/robots.txt> from <GET http://dirkmeineke.de/robots.txt>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://klappstuhl51.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:09:16 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "rustikal-lecker.com"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'rustikal-lecker.com'))])
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://delta-mb.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:16 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfotenabdruck.info/> from <GET http://pfotenabdruck.info>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinolinequinone.org/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://manwe.net/robots.txt> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativopc.com>
{'id': '160', 'url': 'http://3nativopc.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://delta-mb.de/> from <GET http://delta-mb.de>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yuriol.com/robots.txt> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://huckbros.com/>
{'id': '151', 'url': 'https://huckbros.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Feb 2023 08:29:22 GMT', 'tableLayout': False, 'ssl_name': 'huckbros.com', 'ssl_start': '20230715235959Z', 'ssl_expire': '20230715235959Z'}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfotenabdruck.info/> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinolinequinone.org> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fusionstreet.com>
{'id': '154', 'url': 'http://fusionstreet.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www6.xn--wohlfhlwerkstatt-nzb.de/>
{'id': '138', 'url': 'http://www6.xn--wohlfhlwerkstatt-nzb.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lifttaxi.eu/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://efggp.de/robots.txt> from <GET http://efggp.de/robots.txt>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://dirkmeineke.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://jagdgruppe-bad-aibling.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.pfotenabdruck.inforobots.txt> from <GET http://pfoten-abdruck.de/robots.txt>
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dirkmeineke.de/> from <GET http://dirkmeineke.de>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://jagdgruppe-bad-aibling.de> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnendiscount.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://klappstuhl51.de> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobilashop.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://pfotenabdruck.info/>
{'id': '156', 'url': 'https://pfotenabdruck.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 21 Apr 2020 09:28:05 GMT', 'tableLayout': True, 'ssl_name': 'pfotenabdruck.info', 'ssl_start': '20230609235959Z', 'ssl_expire': '20230609235959Z'}
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pyrroloquinolinequinone.org>
{'id': '164', 'url': 'http://pyrroloquinolinequinone.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dirkmeineke.de/> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pflegeplatz-willebadessen.de> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thomasludwig.net/robots.txt> (referer: None)
2023-05-02 12:09:16 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:16 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:09:16 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:09:16 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobilashop.de> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holistic-medicine.ch/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://go-paps.com/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hilcura.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://rustikal-lecker.com/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holistic-medicine.ch> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://jagdgruppe-bad-aibling.de>
{'id': '168', 'url': 'http://jagdgruppe-bad-aibling.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 27 Apr 2023 12:47:45 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://go-paps.com> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://klappstuhl51.de>
{'id': '159', 'url': 'http://klappstuhl51.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/robots.txt> from <GET http://sememo.de/robots.txt>
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://rustikal-lecker.com/> from <GET http://rustikal-lecker.com>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnendiscount.de> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dirkmeineke.de/>
{'id': '163', 'url': 'https://dirkmeineke.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 13 Mar 2023 20:03:35 GMT', 'tableLayout': True, 'ssl_name': 'dirkmeineke.de', 'ssl_start': '20230611205738Z', 'ssl_expire': '20230611205738Z'}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gukrause.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://manwe.net> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pflegeplatz-willebadessen.de>
{'id': '158', 'url': 'http://pflegeplatz-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://discounterticket.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://victoria-einrichtungen.eu/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gukrause.de> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobilashop.de>
{'id': '169', 'url': 'http://casamobilashop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://holistic-medicine.ch>
{'id': '171', 'url': 'http://holistic-medicine.ch', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://victoria-einrichtungen.eu> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wikistudien.org/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-nebenwirkungen.info/robots.txt> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://go-paps.com>
{'id': '172', 'url': 'http://go-paps.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 14 Apr 2022 14:49:32 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-nebenwirkungen.info> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenpflege-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnendiscount.de>
{'id': '165', 'url': 'http://arbeitsbuehnendiscount.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wikistudien.org> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://delta-mb.de/> (referer: None)
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hotcoconut.eu/robots.txt> from <GET http://hotcoconut.eu/robots.txt>
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://manwe.net>
{'id': '157', 'url': 'http://manwe.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gukrause.de>
{'id': '175', 'url': 'http://gukrause.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotcoconut.eu/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thomasludwig.net> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kley-net.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://victoria-einrichtungen.eu>
{'id': '176', 'url': 'http://victoria-einrichtungen.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.de/robots.txt> (referer: None)
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hotcoconut.eu/> from <GET http://hotcoconut.eu>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://discounterticket.de> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://resveratrol-nebenwirkungen.info>
{'id': '179', 'url': 'http://resveratrol-nebenwirkungen.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kley-net.de> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wikistudien.org>
{'id': '177', 'url': 'http://wikistudien.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rustikal-lecker.com/> (referer: None)
2023-05-02 12:09:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://delta-mb.de/>
{'id': '144', 'url': 'https://delta-mb.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'ascon-systems.de', 'ssl_start': '20230706235959Z', 'ssl_expire': '20230706235959Z'}
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.de> (referer: None)
2023-05-02 12:09:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://richard-deutsch.com/robots.txt/> from <GET http://richard-deutsch.com/robots.txt>
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotcoconut.eu/> (referer: None)
2023-05-02 12:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fabian-klemt.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://christian-kilthau.com/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fabian-klemt.de> (referer: None)
2023-05-02 12:09:17 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://yuriol.com/> from <GET http://yuriol.com>
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.kaliner-yoga.de/>
{'id': '21', 'url': 'https://www.kaliner-yoga.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'kaliner-yoga.de', 'ssl_start': '20230615235959Z', 'ssl_expire': '20230615235959Z'}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://stein-und-soehne.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenpflege-willebadessen.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://efggp.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thomasludwig.net>
{'id': '162', 'url': 'http://thomasludwig.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://discounterticket.de>
{'id': '170', 'url': 'http://discounterticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kley-net.de>
{'id': '183', 'url': 'http://kley-net.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 01 Mar 2022 16:42:42 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://rustikal-lecker.com/>
{'id': '161', 'url': 'https://rustikal-lecker.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.de>
{'id': '182', 'url': 'http://wasistopc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hotcoconut.eu/>
{'id': '181', 'url': 'https://hotcoconut.eu/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'hotcoconut.eu', 'ssl_start': '20230518232429Z', 'ssl_expire': '20230518232429Z'}
2023-05-02 12:09:17 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://efggp.de/> from <GET http://efggp.de>
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://erp-ratschlag.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://stein-und-soehne.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dukartstein.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://christian-kilthau.com> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://richard-deutsch.com/robots.txt/> (referer: None)
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 53 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 61 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 93 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 97 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 104 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 115 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 123 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 131 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 149 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 151 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 153 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 205 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 235 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 242 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 254 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 273 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 280 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 311 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 318 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 348 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 355 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 374 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 382 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 383 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 391 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 399 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 400 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 407 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 408 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 409 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 416 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 417 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 418 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 425 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 426 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 427 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 434 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 435 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 436 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 447 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 448 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 449 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 451 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 452 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 453 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 454 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 482 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 495 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 496 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 502 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 532 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 552 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 553 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 554 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 555 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 556 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 558 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 559 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 560 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 561 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 562 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 563 without any user agent to enforce it on.
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fabian-klemt.de>
{'id': '185', 'url': 'http://fabian-klemt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 11 Dec 2008 11:42:32 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://altenpflege-willebadessen.de>
{'id': '173', 'url': 'http://altenpflege-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://erp-ratschlag.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yuriol.com/> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lifttaxi.eu> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lilier.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gracic.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-fragen-online-ktb.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://stein-und-soehne.de>
{'id': '187', 'url': 'http://stein-und-soehne.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 16 Nov 2022 23:48:49 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lilier.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mitochondriale-medizin.org/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 779: invalid start byte
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-fragen-online-ktb.de> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://christian-kilthau.com>
{'id': '186', 'url': 'http://christian-kilthau.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-plus.org/robots.txt> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://erp-ratschlag.de>
{'id': '189', 'url': 'http://erp-ratschlag.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Jul 2010 08:53:51 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-plus.org> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://yuriol.com/>
{'id': '116', 'url': 'https://yuriol.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 27 Apr 2023 09:59:24 GMT', 'tableLayout': False, 'ssl_name': 'yuriol.com', 'ssl_start': '20230531103008Z', 'ssl_expire': '20230531103008Z'}
2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lifttaxi.eu>
{'id': '132', 'url': 'http://lifttaxi.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kinderpartyfun.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kopfstandstuhl.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://richard-deutsch.com> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lilier.de>
{'id': '190', 'url': 'http://lilier.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 30 Dec 2013 12:30:38 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzte-fragen-online-ktb.de>
{'id': '192', 'url': 'http://xn--rzte-fragen-online-ktb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hilcura.de/> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gracic.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dralexandermichalzik.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.formativmedia.atrobots.txt> from <GET http://formativ-print.at/robots.txt>
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://himalayaklangschalen.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://fuerdiesache.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-plus.org>
{'id': '195', 'url': 'http://arginin-plus.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.eu/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 762: invalid start byte
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dralexandermichalzik.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://wengkbuehle.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://richard-deutsch.com>
{'id': '180', 'url': 'http://richard-deutsch.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.exelant.derobots.txt> from <GET http://exelant.de/robots.txt>
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexandermichalzik.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lokomotor.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hilcura.de/>
{'id': '109', 'url': 'https://hilcura.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'hilcura.de', 'ssl_start': '20240329235959Z', 'ssl_expire': '20240329235959Z'}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexandermichalzik.de> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gracic.de>
{'id': '193', 'url': 'http://gracic.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://webmarktplatz24.de/robots.txt> from <GET http://webmarktplatz24.de/robots.txt>
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lokomotor.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondriale-medizin.org> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dralexandermichalzik.de>
{'id': '199', 'url': 'http://dralexandermichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://himalayaklangschalen.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wengkbuehle.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://secstate.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fuerdiesache.de> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://alexandermichalzik.de>
{'id': '202', 'url': 'http://alexandermichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://secstate.de> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lokomotor.de>
{'id': '204', 'url': 'http://lokomotor.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mitochondriale-medizin.org>
{'id': '184', 'url': 'http://mitochondriale-medizin.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://osd2000.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trendsandtechnik.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://himalayaklangschalen.de>
{'id': '200', 'url': 'http://himalayaklangschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://osd2000.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://efggp.de/> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://webmarktplatz24.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wengkbuehle.de>
{'id': '198', 'url': 'http://wengkbuehle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cnc-fanpage.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fuerdiesache.de>
{'id': '194', 'url': 'http://fuerdiesache.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://webmarktplatz24.de/> from <GET http://webmarktplatz24.de>
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trendsandtechnik.de> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://secstate.de>
{'id': '207', 'url': 'http://secstate.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 15 Mar 2010 17:15:22 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lingk-shops.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://chronisch-untervoegelt.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cnc-fanpage.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://webmarktplatz24.de/> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lingk-shops.de> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://maik-wiege.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://osd2000.de>
{'id': '209', 'url': 'http://osd2000.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 20 Feb 2011 15:04:02 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://efggp.de/>
{'id': '166', 'url': 'https://efggp.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'efggp.de', 'ssl_start': '20230611231725Z', 'ssl_expire': '20230611231725Z'}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://maik-wiege.de> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://trendsandtechnik.de>
{'id': '208', 'url': 'http://trendsandtechnik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cnc-fanpage.de>
{'id': '210', 'url': 'http://cnc-fanpage.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://streamify.tv> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://webmarktplatz24.de/>
{'id': '205', 'url': 'https://webmarktplatz24.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': 'webmarktplatz24.de', 'ssl_start': '20230523235959Z', 'ssl_expire': '20230523235959Z'}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nrw-zwerge.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 766: invalid start byte
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lingk-shops.de>
{'id': '211', 'url': 'http://lingk-shops.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 28 Sep 2021 13:22:44 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dinner-ticket.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:09:17 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kinderpartyfun.de/> from <GET http://kinderpartyfun.de>
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobilashop.com/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://johannes-doerfler.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://maik-wiege.de>
{'id': '213', 'url': 'http://maik-wiege.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://blass-net.de/robots.txt> (referer: None)
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobilashop.com> (referer: None)
2023-05-02 12:09:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://streamify.tv>
{'id': '124', 'url': 'http://streamify.tv', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://blass-net.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.kinderpartyfun.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.kinderpartyfun.de'))])
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://peter-flaspoehler.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dinner-ticket.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://krop.at/robots.txt> (referer: None)
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nrw-zwerge.de> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobilashop.com>
{'id': '217', 'url': 'http://casademobilashop.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://peter-flaspoehler.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://johannes-doerfler.de> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://blass-net.de>
{'id': '218', 'url': 'http://blass-net.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 23 May 2018 16:59:30 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronaschutzhilfe.com/robots.txt> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dinner-ticket.de>
{'id': '214', 'url': 'http://dinner-ticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nrw-zwerge.de>
{'id': '212', 'url': 'http://nrw-zwerge.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.kinderpartyfun.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://krop.at> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronaschutzhilfe.com> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://peter-flaspoehler.de>
{'id': '220', 'url': 'http://peter-flaspoehler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 04 Mar 2023 14:19:35 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://johannes-doerfler.de>
{'id': '216', 'url': 'http://johannes-doerfler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dukartstein.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kopfstandstuhl.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yogamaster.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://fun4-u.info/robots.txt> from <GET http://fun4-u.info/robots.txt>
2023-05-02 12:09:18 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2Frobots.txt&bp-auth=1&action=bpnoaccess> from <GET https://sememo.de/robots.txt>
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://krop.at>
{'id': '219', 'url': 'http://krop.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coronaschutzhilfe.com>
{'id': '223', 'url': 'http://coronaschutzhilfe.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dukartstein.de>
{'id': '152', 'url': 'http://dukartstein.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://birtekaufmann-photography.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 59 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 61 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 79 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 105 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 113 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 117 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 133 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 138 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 167 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 175 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 183 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 187 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 188 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 191 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 203 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 213 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 227 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 229 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 233 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 235 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 237 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 240 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 242 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 246 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 248 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 253 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 255 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 259 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 263 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 264 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 265 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 268 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 269 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 270 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 272 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 274 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 275 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 278 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 281 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 284 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 289 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 297 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 298 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 302 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 304 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 306 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 310 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 313 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 317 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 320 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 322 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 327 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 328 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 332 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 333 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 338 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 342 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 346 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 350 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 358 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 368 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 376 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 380 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 383 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 384 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 386 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 387 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 391 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 396 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 412 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 429 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 477 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 481 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 483 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 484 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 486 without any user agent to enforce it on.
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kinderpartyfun.de/> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kopfstandstuhl.de>
{'id': '197', 'url': 'http://kopfstandstuhl.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://steiger-taxi.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mobilemietstation.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:09:18 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "fun4-u.info"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'fun4-u.info'))])
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://fun4-u.info/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila-shop.info/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://serenafate.com/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://fun4-u.info/> from <GET http://fun4-u.info>
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila-shop.info> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hotland.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.kinderpartyfun.de/>
{'id': '178', 'url': 'https://www.kinderpartyfun.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://urbaczek.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://fun4-u.info/> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hotland.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://urbaczek.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://atlantis-magazin.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mobilemietstation.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://steiger-taxi.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://fam-rauch.com/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hyaluronsaeure-kapseln.org/robots.txt> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila-shop.info>
{'id': '227', 'url': 'http://casamobila-shop.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://serenafate.com> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fam-rauch.com> (referer: None)
2023-05-02 12:09:18 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.medizinrechtler.inforobots.txt> from <GET http://grad-der-behinderung.de/robots.txt>
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hyaluronsaeure-kapseln.org> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://weihrauchforum.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://fun4-u.info/>
{'id': '226', 'url': 'https://fun4-u.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 03 Aug 2019 10:52:54 GMT', 'tableLayout': True, 'ssl_name': 'fun4-u.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hotland.de>
{'id': '229', 'url': 'http://hotland.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://urbaczek.de>
{'id': '230', 'url': 'http://urbaczek.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 05 Feb 2023 15:42:17 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://weihrauchforum.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://herzinfarkt-vorbeugen-herzgesundheit.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mobilemietstation.de>
{'id': '224', 'url': 'http://mobilemietstation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://steiger-taxi.de>
{'id': '225', 'url': 'http://steiger-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://herzinfarkt-vorbeugen-herzgesundheit.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://atlantis-magazin.de> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://serenafate.com>
{'id': '228', 'url': 'http://serenafate.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fam-rauch.com>
{'id': '232', 'url': 'http://fam-rauch.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 19 Jun 2011 18:13:24 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hyaluronsaeure-kapseln.org>
{'id': '233', 'url': 'http://hyaluronsaeure-kapseln.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://grothues8.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 9 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 93 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 99 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 123 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 130 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 133 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 140 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 153 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 161 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 162 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 168 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 170 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 171 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 174 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 178 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 184 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 187 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 190 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 206 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 209 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 212 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 239 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 240 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 241 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 244 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 245 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 246 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 247 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 248 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 249 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 251 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 252 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 253 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 254 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 255 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 256 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 258 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 259 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 265 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 268 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 271 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 274 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 280 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 283 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 289 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 298 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 304 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 310 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 311 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 312 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 313 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 314 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 316 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 317 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 318 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 319 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 320 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 321 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 322 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 323 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 325 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 327 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 331 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 332 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 333 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 334 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 335 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 336 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 338 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 340 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 341 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 342 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 343 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 344 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 348 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 349 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 352 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 353 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 357 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 358 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 361 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 362 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 365 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 366 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 369 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 370 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 373 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 374 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 378 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 382 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 386 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 389 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 397 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 398 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 402 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 405 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 406 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 412 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 413 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 414 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 420 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 425 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 428 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 431 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 437 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 438 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 439 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 440 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 441 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 442 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 443 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 444 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 445 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 446 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 447 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 457 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 462 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 466 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 470 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 473 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 474 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 476 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 477 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 478 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 481 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 482 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 483 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 484 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 485 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 488 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 489 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 490 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 491 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 493 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 499 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 501 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 502 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 503 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 504 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 521 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 522 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 530 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 531 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 532 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 533 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 534 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 536 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 538 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 539 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 540 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 542 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 543 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 544 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 545 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 546 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 547 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 549 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 550 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 556 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 558 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 561 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 562 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 563 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 564 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 565 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 566 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 571 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 579 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 580 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 581 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 586 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 589 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 590 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 591 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 592 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 593 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 595 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 596 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 597 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 599 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 600 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 601 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 602 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 606 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 608 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 612 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 614 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 625 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 626 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 627 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 628 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 629 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 630 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 631 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 632 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 633 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 634 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 635 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 639 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 641 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 647 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 649 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 658 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 661 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 667 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 672 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 678 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 679 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 680 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 682 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 689 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 690 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 691 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 692 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 694 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 699 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 700 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 703 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 707 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 709 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 710 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 711 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 714 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 717 without any user agent to enforce it on.
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://felgenfreddy.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://weihrauchforum.de>
{'id': '235', 'url': 'http://weihrauchforum.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://herzinfarkt-vorbeugen-herzgesundheit.de>
{'id': '236', 'url': 'http://herzinfarkt-vorbeugen-herzgesundheit.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://atlantis-magazin.de>
{'id': '231', 'url': 'http://atlantis-magazin.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://birtekaufmann-photography.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://santakruz.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://essenonpaper.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://santakruz.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essenonpaper.de> (referer: None)
2023-05-02 12:09:18 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://badischtauchen.de/robots.txt> from <GET http://badischtauchen.de/robots.txt>
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://felgenfreddy.de> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kulturbeutelhamburg.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://birtekaufmann-photography.de>
{'id': '215', 'url': 'http://birtekaufmann-photography.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://santakruz.de>
{'id': '240', 'url': 'http://santakruz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 30 Oct 2009 20:47:40 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://pharmabox.de/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongstrategy.com/robots.txt> (referer: None)
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2Frobots.txt&bp-auth=1&action=bpnoaccess> (referer: None)
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 9 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 131 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 145 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 149 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 157 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 159 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 166 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 169 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 177 without any user agent to enforce it on.
2023-05-02 12:09:18 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:09:18 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://essenonpaper.de>
{'id': '241', 'url': 'http://essenonpaper.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.core.scraper] DEBUG: Scraped from <200 http://felgenfreddy.de>
{'id': '237', 'url': 'http://felgenfreddy.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:18 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/> from <GET http://sememo.de>
2023-05-02 12:09:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pharmabox.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://badischtauchen.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongstrategy.com> (referer: None)
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.mattner.orgrobots.txt> from <GET http://mattner.org/robots.txt>
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://badischtauchen.de/> from <GET http://badischtauchen.de>
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sound-producer.com/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://future-fashion.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://badischtauchen.de/> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pharmabox.de>
{'id': '244', 'url': 'http://pharmabox.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 03 Mar 2020 18:24:35 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://future-fashion.de> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dejongstrategy.com>
{'id': '243', 'url': 'http://dejongstrategy.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 28 Dec 2014 13:42:08 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://x-schreck.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://chronisch-untervoegelt.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://x-schreck.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://meditationkisse.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://badischtauchen.de/>
{'id': '242', 'url': 'https://badischtauchen.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 18 Sep 2021 11:27:27 GMT', 'tableLayout': False, 'ssl_name': 'badischtauchen.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://future-fashion.de>
{'id': '247', 'url': 'http://future-fashion.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 05 Jan 2016 16:33:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://grothues8.de/Wordpress/> from <GET http://grothues8.de>
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ostsea.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://chronisch-untervoegelt.de>
{'id': '188', 'url': 'http://chronisch-untervoegelt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://x-schreck.de>
{'id': '248', 'url': 'http://x-schreck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 06 Oct 2007 18:12:18 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ostsea.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-lexikon.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://meditationkisse.de> (referer: None)
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.okv-weiden.de/robots.txt> from <GET http://okv-weiden.de/robots.txt>
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taxi-meile.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 104 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 108 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 112 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 118 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 130 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 132 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 138 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 140 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 146 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 162 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 166 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 168 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 170 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 174 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 176 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 178 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 182 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 184 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 188 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 190 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 192 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 196 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 200 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bioticon.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/robots.txt> from <GET http://www.okv-weiden.de/robots.txt>
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ostsea.de>
{'id': '251', 'url': 'http://ostsea.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 15 Jan 2017 21:15:54 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bioticon.de> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://meditationkisse.de>
{'id': '249', 'url': 'http://meditationkisse.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://klangschalen-schop.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-lexikon.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bioticon.de>
{'id': '253', 'url': 'http://bioticon.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.okv-weiden.de/> from <GET http://okv-weiden.de>
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.dknv.de/> from <GET http://dknv.de/robots.txt>
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gasthof-rolfes.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.eu> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://klangschalen-schop.de> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-lexikon.de>
{'id': '250', 'url': 'http://arbeitsbuehnen-lexikon.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/robots.txt> from <GET http://www.okv-weiden.de/robots.txt>
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikonblog.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://has-wbe.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taxi-meile.de> (referer: None)
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/> from <GET http://www.okv-weiden.de/>
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikonblog.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://michaelbuschke.com/robots.txt> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.eu>
{'id': '136', 'url': 'http://govido.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://klangschalen-schop.de>
{'id': '254', 'url': 'http://klangschalen-schop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/> (referer: None)
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://barbaraundjan.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:09:19 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taxi-meile.de>
{'id': '246', 'url': 'http://taxi-meile.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess> from <GET https://sememo.de/>
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.dknv.de/> from <GET http://dknv.de>
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikonblog.de>
{'id': '257', 'url': 'http://biotikonblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://barbaraundjan.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arztfragenonline.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://michaelbuschke.com> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grothues8.de/Wordpress/> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://has-wbe.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arztfragenonline.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://peking-laufenburg.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://flugzeuge-weltweit.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.de> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westsidemusic.com/robots.txt> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://barbaraundjan.de>
{'id': '259', 'url': 'http://barbaraundjan.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://peking-laufenburg.de> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://michaelbuschke.com>
{'id': '258', 'url': 'http://michaelbuschke.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 07 Sep 2019 22:37:35 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.okv-weiden.de/>
{'id': '252', 'url': 'https://www.okv-weiden.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 02 May 2023 12:09:19 GMT', 'tableLayout': False, 'ssl_name': 'www.okv-weiden.de', 'ssl_start': '20230522235959Z', 'ssl_expire': '20230522235959Z'}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.at/robots.txt> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-fragen-online.de/robots.txt> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://grothues8.de/Wordpress/>
{'id': '222', 'url': 'http://grothues8.de/Wordpress/', 'status': 200, 'title': 'Linus Tjaard Rabea Hener Grothues – Das ist die Private Homepage von Linus Tjaard Rabea Henner Grothues', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://has-wbe.de>
{'id': '255', 'url': 'http://has-wbe.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila.de>
{'id': '260', 'url': 'http://casamobila.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arztfragenonline.de>
{'id': '261', 'url': 'http://arztfragenonline.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.at> (referer: None)
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-fragen-online.de> (referer: None)
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://was-ist-opc.de>
{'id': '262', 'url': 'http://was-ist-opc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westsidemusic.com> (referer: None)
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.gasthof-rolfes.de/> from <GET http://gasthof-rolfes.de>
2023-05-02 12:09:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://peking-laufenburg.de>
{'id': '264', 'url': 'http://peking-laufenburg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 24 Aug 2020 11:50:05 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://volki.pb.photographyrobots.txt> from <GET http://galerie-bauer.de/robots.txt>
2023-05-02 12:09:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://flugzeuge-weltweit.de> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sound-producer.com> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nomadpublishing.store/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobila24.eu/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nomadpublishing.store> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobila24.eu> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.at>
{'id': '267', 'url': 'http://biotikon.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arzt-fragen-online.de>
{'id': '266', 'url': 'http://arzt-fragen-online.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://grinberg.de/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://curcuma-nebenwirkungen.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://westsidemusic.com>
{'id': '265', 'url': 'http://westsidemusic.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://curcuma-nebenwirkungen.de> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://aus-keil-wird-schmidt.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://flugzeuge-weltweit.de>
{'id': '263', 'url': 'http://flugzeuge-weltweit.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://sound-producer.com>
{'id': '203', 'url': 'http://sound-producer.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://grinberg.de/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://grinberg.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www6.dknv.de/>
{'id': '256', 'url': 'http://www6.dknv.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://aus-keil-wird-schmidt.de> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nomadpublishing.store>
{'id': '270', 'url': 'http://nomadpublishing.store', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yogamaster.de> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://indiaschop.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobila24.eu>
{'id': '271', 'url': 'http://casademobila24.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tropicanalife.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grinberg.de> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://curcuma-nebenwirkungen.de>
{'id': '273', 'url': 'http://curcuma-nebenwirkungen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.eu/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tropicanalife.de> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://holzmoebel-experte.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.eu> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://aus-keil-wird-schmidt.de>
{'id': '274', 'url': 'http://aus-keil-wird-schmidt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 22 Apr 2008 07:31:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yogamaster.de>
{'id': '221', 'url': 'http://yogamaster.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holzmoebel-experte.de> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://indiaschop.de> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://grinberg.de>
{'id': '272', 'url': 'http://grinberg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 28 Jan 2023 20:32:03 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzteonlinefragen-unb.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cradle-software.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzteonlinefragen-unb.de> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tropicanalife.de>
{'id': '276', 'url': 'http://tropicanalife.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 28 Mar 2009 22:52:40 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativopc.eu>
{'id': '277', 'url': 'http://3nativopc.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.kg-muellekolk.de/robots.txt> from <GET http://kg-muellekolk.de/robots.txt>
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://strumpfaffen.org/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cradle-software.de> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://holzmoebel-experte.de>
{'id': '278', 'url': 'http://holzmoebel-experte.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bergium.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://strumpfaffen.org> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://indiaschop.de>
{'id': '275', 'url': 'http://indiaschop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bergium.de> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzteonlinefragen-unb.de>
{'id': '280', 'url': 'http://xn--rzteonlinefragen-unb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://studio-balance.at/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://losmenombak-mentawai.com/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cradle-software.de>
{'id': '281', 'url': 'http://cradle-software.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://studio-balance.at> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thebodyasarchive.com/robots.txt> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://strumpfaffen.org>
{'id': '282', 'url': 'http://strumpfaffen.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 23 Apr 2012 11:57:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thebodyasarchive.com> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bergium.de>
{'id': '283', 'url': 'http://bergium.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 07 Nov 2017 13:41:54 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.ch/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.gasthof-rolfes.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-babyzimmer.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.ch> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess>
{'id': '174', 'url': 'https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess', 'status': 200, 'title': 'Anmelden ‹ sememo — WordPress', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'sememo.de', 'ssl_start': '20230710235959Z', 'ssl_expire': '20230710235959Z'}
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://studio-balance.at>
{'id': '285', 'url': 'http://studio-balance.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 30 Dec 2020 13:19:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-babyzimmer.de> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-shop-paderborn.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondrien-funktion.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://reitanlage-riedmuehle.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondrien-funktion.de> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thebodyasarchive.com>
{'id': '286', 'url': 'http://thebodyasarchive.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 27 Dec 2020 18:13:23 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.com/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://format-mehrweg.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.ch>
{'id': '287', 'url': 'http://biotikon.ch', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.com> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cosmecon.eu/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://format-mehrweg.de> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-babyzimmer.de>
{'id': '288', 'url': 'http://massivholz-babyzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-dielenmoebel.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cosmecon.eu> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mitochondrien-funktion.de>
{'id': '290', 'url': 'http://mitochondrien-funktion.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-dielenmoebel.de> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--kleinegrashpfer-9vb.at/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--kleinegrashpfer-9vb.at> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-alpha-ketoglutarat.com>
{'id': '291', 'url': 'http://arginin-alpha-ketoglutarat.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-lothar-wester.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://format-mehrweg.de>
{'id': '292', 'url': 'http://format-mehrweg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 10 Dec 2019 16:45:18 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://reitanlage-riedmuehle.de> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cosmecon.eu>
{'id': '293', 'url': 'http://cosmecon.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 25 Nov 2009 14:28:26 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://skiverband-mv.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-dielenmoebel.de>
{'id': '294', 'url': 'http://massivholz-dielenmoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://kulturbeutelhamburg.de/> from <GET http://kulturbeutelhamburg.de>
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dinnerticket.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--kleinegrashpfer-9vb.at>
{'id': '295', 'url': 'http://xn--kleinegrashpfer-9vb.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 13 Apr 2023 07:16:11 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://eventation.de/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ticila.com/robots.txt> (referer: None)
2023-05-02 12:09:20 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://reitanlage-riedmuehle.de>
{'id': '284', 'url': 'http://reitanlage-riedmuehle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ticila.com> (referer: None)
2023-05-02 12:09:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://eventation.de/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://eventation.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://seat-factory.de/robots.txt> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://seat-factory.de> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://skiverband-mv.de> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dinnerticket.de> (referer: None)
2023-05-02 12:09:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://technolog-e.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://eventation.de> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://technolog-e.de> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ticila.com>
{'id': '300', 'url': 'http://ticila.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 05 Jan 2016 16:33:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-datenbank.net/robots.txt> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mcpicchu.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mcpicchu.de> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-datenbank.net> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://seat-factory.de>
{'id': '301', 'url': 'http://seat-factory.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 15 Jul 2015 14:10:37 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://trengo.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dirkklages.com/robots.txt> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://skiverband-mv.de>
{'id': '298', 'url': 'http://skiverband-mv.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dinnerticket.de>
{'id': '296', 'url': 'http://dinnerticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dirkklages.com> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://eventation.de>
{'id': '299', 'url': 'http://eventation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://technolog-e.de>
{'id': '302', 'url': 'http://technolog-e.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 07 Nov 2014 10:40:41 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lederoutfits.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ws-gmbh.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.org/robots.txt> (referer: None)
2023-05-02 12:09:21 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lederoutfits.de> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mcpicchu.de>
{'id': '304', 'url': 'http://mcpicchu.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 23 Jul 2022 22:42:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-datenbank.net>
{'id': '303', 'url': 'http://auto-datenbank.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-shop-paderborn.de> (referer: None)
2023-05-02 12:09:21 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.nachhilfe-oase.de/robots.txt> from <GET http://123discounter.de/robots.txt>
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dirkklages.com>
{'id': '307', 'url': 'http://dirkklages.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 17 Apr 2021 09:45:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-lothar-wester.de> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dark-hawks.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trengo.de> (referer: None)
2023-05-02 12:09:21 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.kg-muellekolk.de/> from <GET http://kg-muellekolk.de>
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-plomari.com/robots.txt> (referer: None)
2023-05-02 12:09:21 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lederoutfits.de>
{'id': '308', 'url': 'http://lederoutfits.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Nov 2017 23:18:52 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.gasthof-rolfes.de/> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cymeg.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-shop-paderborn.de>
{'id': '289', 'url': 'http://yoga-shop-paderborn.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.nachhilfe-oase.de/> from <GET http://123discounter.de>
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cymeg.de> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://die-goldene-schallplatte.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 780: invalid start byte
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.eu/robots.txt> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-lothar-wester.de>
{'id': '297', 'url': 'http://yoga-lothar-wester.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://schiefer-lautsprecher.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://trengo.de>
{'id': '306', 'url': 'http://trengo.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.eu> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.gasthof-rolfes.de/>
{'id': '239', 'url': 'http://www.gasthof-rolfes.de/', 'status': 200, 'title': 'Gasthof Rolfes ', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-plomari.com> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://teamtrzweb.de> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cymeg.de>
{'id': '313', 'url': 'http://cymeg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 17 Apr 2021 09:45:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://die-goldene-schallplatte.de> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativ-opc.eu>
{'id': '314', 'url': 'http://3nativ-opc.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-plomari.com>
{'id': '309', 'url': 'http://ouzo-plomari.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.nachhilfe-oase.de/>
{'id': '311', 'url': 'http://www.nachhilfe-oase.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongsblog.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:09:21 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:09:21 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://losmenombak-mentawai.com> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://teamtrzweb.de>
{'id': '305', 'url': 'http://teamtrzweb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ws-gmbh.de> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://die-goldene-schallplatte.de>
{'id': '312', 'url': 'http://die-goldene-schallplatte.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://losmenombak-mentawai.com>
{'id': '191', 'url': 'http://losmenombak-mentawai.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schiefer-lautsprecher.de> (referer: None)
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/robots.txt> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ws-gmbh.de>
{'id': '206', 'url': 'http://ws-gmbh.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.org> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schiefer-lautsprecher.de>
{'id': '315', 'url': 'http://schiefer-lautsprecher.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dark-hawks.de> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.org>
{'id': '238', 'url': 'http://govido.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongsblog.de> (referer: None)
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dark-hawks.de>
{'id': '279', 'url': 'http://dark-hawks.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:21 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:21 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dejongsblog.de>
{'id': '310', 'url': 'http://dejongsblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/> (referer: None)
2023-05-02 12:09:22 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:22 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.kg-muellekolk.de/>
{'id': '268', 'url': 'http://www.kg-muellekolk.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kulturbeutelhamburg.de/> (referer: None)
2023-05-02 12:09:23 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://kulturbeutelhamburg.de/>
{'id': '147', 'url': 'https://kulturbeutelhamburg.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'kulturbeutelhamburg.de', 'ssl_start': '20230605235959Z', 'ssl_expire': '20230605235959Z'}
2023-05-02 12:09:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://photos.hr-photo.derobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:09:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.pfotenabdruck.inforobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:09:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.formativmedia.atrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:09:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.exelant.derobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:09:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.medizinrechtler.inforobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:09:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.mattner.orgrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:09:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://volki.pb.photographyrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:09:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://photos.hr-photo.derobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:09:44 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://hr-photo.de/robots.txt>: DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:09:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://photos.hr-photo.de> from <GET http://hr-photo.de>
2023-05-02 12:09:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://photos.hr-photo.de/robots.txt> (referer: None)
2023-05-02 12:09:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 771: invalid start byte
2023-05-02 12:09:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://photos.hr-photo.de> (referer: None)
2023-05-02 12:09:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://photos.hr-photo.de>
{'id': '24', 'url': 'http://photos.hr-photo.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.pfotenabdruck.inforobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:09:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://pfoten-abdruck.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:09:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.pfotenabdruck.info> from <GET http://pfoten-abdruck.de>
2023-05-02 12:09:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.pfotenabdruck.info/robots.txt> (referer: None)
2023-05-02 12:09:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.pfotenabdruck.info> (referer: None)
2023-05-02 12:09:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.exelant.derobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:09:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://exelant.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:09:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.exelant.de> from <GET http://exelant.de>
2023-05-02 12:09:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://www.formativmedia.atrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:09:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://formativ-print.at/robots.txt>: DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:09:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.formativmedia.at> from <GET http://formativ-print.at>
2023-05-02 12:09:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.pfotenabdruck.info>
{'id': '167', 'url': 'https://www.pfotenabdruck.info', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 21 Apr 2020 09:28:05 GMT', 'tableLayout': True, 'ssl_name': 'pfotenabdruck.info', 'ssl_start': '20230609235959Z', 'ssl_expire': '20230609235959Z'}
2023-05-02 12:09:49 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.exelant.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.exelant.de'))])
2023-05-02 12:09:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.exelant.de/robots.txt> (referer: None)
2023-05-02 12:09:49 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:09:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.exelant.de> (referer: None)
2023-05-02 12:09:49 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.exelant.de>
{'id': '201', 'url': 'https://www.exelant.de', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:09:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.formativmedia.at/robots.txt> (referer: None)
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 81 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:09:50 [protego] DEBUG: Rule at line 109 without any user agent to enforce it on.
2023-05-02 12:09:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.medizinrechtler.inforobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:09:50 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://grad-der-behinderung.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:09:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.medizinrechtler.info> from <GET http://grad-der-behinderung.de>
2023-05-02 12:09:50 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.medizinrechtler.info"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.medizinrechtler.info'))])
2023-05-02 12:09:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.formativmedia.at/> from <GET http://www.formativmedia.at>
2023-05-02 12:09:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.medizinrechtler.info/robots.txt> (referer: None)
2023-05-02 12:09:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:09:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.medizinrechtler.info> (referer: None)
2023-05-02 12:09:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.medizinrechtler.info>
{'id': '234', 'url': 'https://www.medizinrechtler.info', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:09:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.formativmedia.at/> (referer: None)
2023-05-02 12:09:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://www.mattner.orgrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:09:51 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://mattner.org/robots.txt>: DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:09:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.mattner.org> from <GET http://mattner.org>
2023-05-02 12:09:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.formativmedia.at/>
{'id': '196', 'url': 'https://www.formativmedia.at/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'einfachwanda.at', 'ssl_start': '20230515034747Z', 'ssl_expire': '20230515034747Z'}
2023-05-02 12:09:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.mattner.org/robots.txt> (referer: None)
2023-05-02 12:09:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.mattner.org> (referer: None)
2023-05-02 12:09:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.mattner.org>
{'id': '245', 'url': 'http://www.mattner.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:09:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://volki.pb.photographyrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:09:52 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://galerie-bauer.de/robots.txt>: DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:09:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://volki.pb.photography> from <GET http://galerie-bauer.de>
2023-05-02 12:09:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://volki.pb.photography/robots.txt> (referer: None)
2023-05-02 12:09:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://volki.pb.photography> (referer: None)
2023-05-02 12:09:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:09:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://volki.pb.photography>
{'id': '269', 'url': 'https://volki.pb.photography', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': '*.pb.photography', 'ssl_start': '20240222235959Z', 'ssl_expire': '20240222235959Z'}
2023-05-02 12:09:52 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:09:52 [scrapy.extensions.feedexport] INFO: Stored jl feed (314 items) in: baddata_results_20230502_120912.jl
2023-05-02 12:09:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 14,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 14,
 'downloader/request_bytes': 172983,
 'downloader/request_count': 762,
 'downloader/request_method_count/GET': 762,
 'downloader/response_bytes': 2791709,
 'downloader/response_count': 748,
 'downloader/response_status_count/200': 468,
 'downloader/response_status_count/301': 27,
 'downloader/response_status_count/302': 66,
 'downloader/response_status_count/404': 175,
 'downloader/response_status_count/500': 6,
 'downloader/response_status_count/503': 6,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 38.984888,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 9, 52, 606071),
 'httpcompression/response_bytes': 4959796,
 'httpcompression/response_count': 352,
 'item_scraped_count': 314,
 'log_count/DEBUG': 2104,
 'log_count/ERROR': 21,
 'log_count/INFO': 11,
 'log_count/WARNING': 324,
 'memusage/max': 66785280,
 'memusage/startup': 66785280,
 'response_received_count': 646,
 'retry/count': 13,
 'retry/max_reached': 13,
 'retry/reason_count/500 Internal Server Error': 3,
 'retry/reason_count/503 Service Unavailable': 3,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 7,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 7,
 'robotstxt/request_count': 338,
 'robotstxt/response_count': 331,
 'robotstxt/response_status_count/200': 151,
 'robotstxt/response_status_count/404': 175,
 'robotstxt/response_status_count/500': 3,
 'robotstxt/response_status_count/503': 2,
 'scheduler/dequeued': 368,
 'scheduler/dequeued/memory': 368,
 'scheduler/enqueued': 368,
 'scheduler/enqueued/memory': 368,
 'start_time': datetime.datetime(2023, 5, 2, 12, 9, 13, 621183)}
2023-05-02 12:09:52 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:13:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:13:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:13:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:13:56 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:13:56 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:13:56 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:13:56 [scrapy.extensions.telnet] INFO: Telnet Password: 8ae7e9b491beee15
2023-05-02 12:13:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:13:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:13:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:13:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:13:56 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:13:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:13:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://lisaglauer.com/robots.txt> from <GET http://lisaglauer.de/robots.txt>
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfs-europe.de/robots.txt> from <GET http://pfs-europe.de/robots.txt>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.be/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mgvliederkranz-asbach.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hausegger.net/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sprachlos-ev-beratung.de/robots.txt> from <GET http://sprachlos-ev-beratung.de/robots.txt>
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://docs.moodle.org:443/robots.txt> from <GET http://docs.moodle.org/robots.txt>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-wohnzimmer.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://kaffeeundservice.at/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lahres.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.be> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mgvliederkranz-asbach.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hausegger.net> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-wohnzimmer.de> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mixable.media/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://kaffeeundservice.at/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://kaffeeundservice.at/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "pfs-europe.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'pfs-europe.de'))])
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://bayern-immobilie.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://karo-stimme.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:13:57 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET http://lifttaxi.com> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lifttaxi.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://docs.moodle.org:443/2x/pl/Flash> from <GET http://docs.moodle.org/2x/pl/Flash>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mixable.media> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mind-holiday.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfs-europe.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://steigertaxi.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://kaliner-yoga.de/robots.txt> from <GET http://kaliner-yoga.de/robots.txt>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lahres.com> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfs-europe.de/> from <GET http://pfs-europe.de>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-teamblog.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.be>
{'id': '7', 'url': 'http://biotikon.be', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mgvliederkranz-asbach.de>
{'id': '11', 'url': 'http://mgvliederkranz-asbach.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 28 Jan 2009 10:53:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hausegger.net>
{'id': '12', 'url': 'http://hausegger.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 12 Sep 2012 11:18:42 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://photos.hr-photo.derobots.txt> from <GET http://hr-photo.de/robots.txt>
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-wohnzimmer.de>
{'id': '13', 'url': 'http://massivholz-wohnzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfs-europe.de/> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-teamblog.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://efwe-art.at/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gbv-grosskarolinenfeld.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://coaching-im-alltag.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mixable.media>
{'id': '17', 'url': 'http://mixable.media', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.dienstleistung-solar.de/robots.txt> from <GET http://dienstleistung-solar.de/robots.txt>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://karo-stimme.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mind-holiday.com> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lahres.com>
{'id': '15', 'url': 'http://lahres.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 25 Dec 2019 08:39:49 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lifttaxi.com/> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bayern-immobilie.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://billardcafe-suedpark.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://steigertaxi.de> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://pfs-europe.de/>
{'id': '3', 'url': 'https://pfs-europe.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 03 Aug 2019 10:52:54 GMT', 'tableLayout': True, 'ssl_name': 'fun4-u.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon-teamblog.de>
{'id': '23', 'url': 'http://biotikon-teamblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET http://www.purpur.eatbu.com/?lang=de> from <GET http://billardcafe-suedpark.de>
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '25', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/2x/pl/Flash> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaukus.org/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://efwe-art.at> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 463 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 469 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 472 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 507 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 513 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaffeeundservice.at> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gbv-grosskarolinenfeld.de> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://karo-stimme.de>
{'id': '9', 'url': 'http://karo-stimme.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mind-holiday.com>
{'id': '4', 'url': 'http://mind-holiday.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coaching-im-alltag.de> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lifttaxi.com/>
{'id': '1', 'url': 'http://lifttaxi.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.com> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET http://kaukus.org/index2.html> from <GET http://kaukus.org>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kilthau.tech/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.wassersportcenter-heiligenhafen.de/> from <GET http://wassersportcenter-heiligenhafen.de>
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bayern-immobilie.de>
{'id': '6', 'url': 'http://bayern-immobilie.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://steigertaxi.de>
{'id': '8', 'url': 'http://steigertaxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.purpur.eatbu.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://seelenfeder.at/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaukus.org/index2.html> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://drey.info/robots.txt> from <GET http://drey.info/robots.txt>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://illger.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.purpur.eatbu.com/?lang=de> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://docs.moodle.org:443/2x/pl/Flash>
{'id': '0', 'url': 'https://docs.moodle.org:443/2x/pl/Flash', 'status': 200, 'title': 'Flash – MoodleDocs', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 24 Apr 2023 19:09:27 GMT', 'tableLayout': False, 'ssl_name': 'sni.cloudflaressl.com', 'ssl_start': '20240501235959Z', 'ssl_expire': '20240501235959Z'}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://seelenfeder.at> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://illger.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 463 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 469 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 472 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 507 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 513 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://efwe-art.at>
{'id': '5', 'url': 'http://efwe-art.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-star.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://byzickl.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 1 times): 503 Service Unavailable
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.sprachlos-ev-beratung.de/robots.txt> from <GET https://sprachlos-ev-beratung.de/robots.txt>
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.info>
{'id': '31', 'url': 'http://wasistopc.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kaffeeundservice.at>
{'id': '14', 'url': 'http://kaffeeundservice.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gbv-grosskarolinenfeld.de>
{'id': '19', 'url': 'http://gbv-grosskarolinenfeld.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://byzickl.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dobbrunz.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cellhomoeostasis.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/> from <GET http://www.wassersportcenter-heiligenhafen.de/>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gartengestaltung-brandner.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 781: invalid start byte
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kilthau.tech> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coaching-im-alltag.de>
{'id': '18', 'url': 'http://coaching-im-alltag.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativ-opc.com>
{'id': '33', 'url': 'http://3nativ-opc.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kaukus.org/index2.html>
{'id': '32', 'url': 'http://kaukus.org/index2.html', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2009 20:14:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.purpur.eatbu.com/?lang=de>
{'id': '30', 'url': 'http://www.purpur.eatbu.com/?lang=de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://seelenfeder.at>
{'id': '35', 'url': 'http://seelenfeder.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 10 Mar 2018 15:51:03 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://illger.de>
{'id': '37', 'url': 'http://illger.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 07 Nov 2011 11:18:19 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cellhomoeostasis.com> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://magnonics.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dobbrunz.com> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://drey.info/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 2 times): 503 Service Unavailable
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://diekicktipper.de/robots.txt> from <GET http://diekicktipper.de/robots.txt>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://magnonics.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ts-it-service.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://drey.info/> from <GET http://drey.info>
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://byzickl.de>
{'id': '36', 'url': 'http://byzickl.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.dienstleistung-solar.de/> from <GET http://dienstleistung-solar.de>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schoolscout24.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gesundheitsforum-norderstedt.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dj-sb.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ts-it-service.de> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kilthau.tech>
{'id': '34', 'url': 'http://kilthau.tech', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cellhomoeostasis.com>
{'id': '41', 'url': 'http://cellhomoeostasis.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-flurmoebel.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://malttec.net/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schoolscout24.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://drey.info/> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gesundheitsforum-norderstedt.com> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.wassersportcenter-heiligenhafen.de/>
{'id': '16', 'url': 'https://www.wassersportcenter-heiligenhafen.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'wassersportcenter-heiligenhafen.de', 'ssl_start': '20230719235959Z', 'ssl_expire': '20230719235959Z'}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dj-sb.com> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hgwimmobilien.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dobbrunz.com>
{'id': '40', 'url': 'http://dobbrunz.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 03 Jul 2016 18:53:33 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kanzlei-sauerwein.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-flurmoebel.de> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET https://teamtrzweb.de> from <GET http://malttec.net>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essen-auf-raedern-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://magnonics.de>
{'id': '42', 'url': 'http://magnonics.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 26 Aug 2009 08:33:56 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://diekicktipper.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kanzlei-sauerwein.de> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ts-it-service.de>
{'id': '45', 'url': 'http://ts-it-service.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://diekicktipper.de/> from <GET http://diekicktipper.de>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.net/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wetzel.work/robots.txt> (referer: None)
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 213 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 214 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 234 without any user agent to enforce it on.
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 1 times): 503 Service Unavailable
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schoolscout24.de>
{'id': '46', 'url': 'http://schoolscout24.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 03 May 2011 19:33:17 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://drey.info/>
{'id': '38', 'url': 'https://drey.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 17 Mar 2014 08:12:11 GMT', 'tableLayout': False, 'ssl_name': 'drey.info', 'ssl_start': '20230705235959Z', 'ssl_expire': '20230705235959Z'}
2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gesundheitsforum-norderstedt.com>
{'id': '48', 'url': 'http://gesundheitsforum-norderstedt.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 26 Mar 2012 15:32:11 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gartengestaltung-brandner.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.net> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wetzel.work> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dj-sb.com>
{'id': '47', 'url': 'http://dj-sb.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 28 Dec 2014 13:42:08 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-flurmoebel.de>
{'id': '50', 'url': 'http://massivholz-flurmoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://lisaglauer.com/> from <GET http://lisaglauer.de>
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hgwimmobilien.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://buehnen-taxi.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kanzlei-sauerwein.de>
{'id': '51', 'url': 'http://kanzlei-sauerwein.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 03 Jan 2010 02:56:47 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://teamtrzweb.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://diekicktipper.de/> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://goting-kliff53.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-bueromoebel.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yogashop-paderborn.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 2 times): 503 Service Unavailable
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:13:57 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-online-fragen-ktb.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gartengestaltung-brandner.de>
{'id': '28', 'url': 'http://gartengestaltung-brandner.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://goting-kliff53.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essen-auf-raedern-willebadessen.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-bueromoebel.de> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.net>
{'id': '55', 'url': 'http://biotikon.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wetzel.work>
{'id': '56', 'url': 'http://wetzel.work', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 14 Aug 2017 17:23:07 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-pilavas.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-online-fragen-ktb.de> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/robots.txt> (referer: None)
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hgwimmobilien.de>
{'id': '43', 'url': 'http://hgwimmobilien.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bce-clan.de/robots.txt> (referer: None)
2023-05-02 12:13:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://diekicktipper.de/>
{'id': '44', 'url': 'https://diekicktipper.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'diekicktipper.de', 'ssl_start': '20230527235959Z', 'ssl_expire': '20230527235959Z'}
2023-05-02 12:13:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sprachlos-ev-beratung.de/> from <GET http://sprachlos-ev-beratung.de>
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://bavarian-starlights.com/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://xn--edelstahlscheckkartenhlle-0wc.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.info/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/> (failed 1 times): 503 Service Unavailable
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://buehnen-taxi.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.info> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bce-clan.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-pilavas.com> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://goting-kliff53.de>
{'id': '60', 'url': 'http://goting-kliff53.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 27 Jul 2022 11:59:55 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://essen-auf-raedern-willebadessen.de>
{'id': '27', 'url': 'http://essen-auf-raedern-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-bueromoebel.de>
{'id': '59', 'url': 'http://massivholz-bueromoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzte-online-fragen-ktb.de>
{'id': '61', 'url': 'http://xn--rzte-online-fragen-ktb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-makler.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bauelemente-riede.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ciber-man.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://buehnen-taxi.de>
{'id': '53', 'url': 'http://buehnen-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--edelstahlscheckkartenhlle-0wc.de> (referer: None)
2023-05-02 12:13:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/> (failed 2 times): 503 Service Unavailable
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ciber-man.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bauelemente-riede.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://buehnentaxi.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:13:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/robots.txt> from <GET http://cyberfiber.me/robots.txt>
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-star.de> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://was-ist-opc.info>
{'id': '65', 'url': 'http://was-ist-opc.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://xn--holzmbel-experte-qwb.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bce-clan.de>
{'id': '64', 'url': 'http://bce-clan.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-pilavas.com>
{'id': '57', 'url': 'http://ouzo-pilavas.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://schluesselvereinzler.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lisaglauer.com/>
{'id': '2', 'url': 'http://lisaglauer.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--holzmbel-experte-qwb.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://style-and-smile.com/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--edelstahlscheckkartenhlle-0wc.de>
{'id': '67', 'url': 'http://xn--edelstahlscheckkartenhlle-0wc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [dubdev] ERROR: HttpError on https://www.dienstleistung-solar.de/
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-makler.de> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ciber-man.de>
{'id': '70', 'url': 'http://ciber-man.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 20 Jan 2013 06:17:41 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bauelemente-riede.de>
{'id': '69', 'url': 'http://bauelemente-riede.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 16 May 2018 20:42:31 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://huckbros.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teamtrzweb.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mobile-mietstation.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 774: invalid start byte
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://buehnentaxi.de> (referer: None)
2023-05-02 12:13:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://da365.de/robots.txt> from <GET http://da365.de/robots.txt>
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-star.de>
{'id': '39', 'url': 'http://yoga-star.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://huckbros.de> (referer: None)
2023-05-02 12:13:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.sprachlos-ev-beratung.de/> from <GET https://sprachlos-ev-beratung.de/>
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-markt.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 776: invalid start byte
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-qigong-master.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--holzmbel-experte-qwb.de>
{'id': '74', 'url': 'http://xn--holzmbel-experte-qwb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ignaz-schwarzbach.eu/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schluesselvereinzler.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-jugendzimmer.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kindz.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-qigong-master.de> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-makler.de>
{'id': '63', 'url': 'http://arbeitsbuehnen-makler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ignaz-schwarzbach.eu> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://marco-k.com/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-jugendzimmer.de> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://teamtrzweb.de>
{'id': '49', 'url': 'https://teamtrzweb.de', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'biergartenteam.de', 'ssl_start': '20230609051948Z', 'ssl_expire': '20230609051948Z'}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila.info/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://buehnentaxi.de>
{'id': '68', 'url': 'http://buehnentaxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://marco-k.com> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mobile-mietstation.de> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://huckbros.de>
{'id': '78', 'url': 'http://huckbros.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Feb 2023 08:29:22 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gzlw.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://no-flush-niacin.com/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.bavarian-starlights.com/> from <GET http://bavarian-starlights.com>
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila.info> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schluesselvereinzler.de>
{'id': '76', 'url': 'http://schluesselvereinzler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taichichuan-qigong-master.de>
{'id': '79', 'url': 'http://taichichuan-qigong-master.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://no-flush-niacin.com> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-markt.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-kinderzimmer.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gzlw.de> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ignaz-schwarzbach.eu>
{'id': '81', 'url': 'http://ignaz-schwarzbach.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 04 Dec 2022 16:28:01 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-jugendzimmer.de>
{'id': '80', 'url': 'http://massivholz-jugendzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://www.bavarian-starlights.com/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-kinderzimmer.de> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://marco-k.com>
{'id': '82', 'url': 'http://marco-k.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 23 Jul 2022 22:42:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobila24.com/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ganzheitlichesheilen.eu/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mobile-mietstation.de>
{'id': '71', 'url': 'http://mobile-mietstation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmedmichalzik.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila.info>
{'id': '83', 'url': 'http://casamobila.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ganzheitlichesheilen.eu> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobila24.com> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmedmichalzik.de> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://no-flush-niacin.com>
{'id': '86', 'url': 'http://no-flush-niacin.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-markt.de>
{'id': '72', 'url': 'http://arbeitsbuehnen-markt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gzlw.de>
{'id': '85', 'url': 'http://gzlw.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 28 Jan 2009 09:42:45 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fmz-frankershausen.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmichalzik.eu/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://boncomputa.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nows.tk/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-kinderzimmer.de>
{'id': '87', 'url': 'http://massivholz-kinderzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmichalzik.eu> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rztefragenonline-unb.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nows.tk> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rztefragenonline-unb.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ganzheitlichesheilen.eu>
{'id': '90', 'url': 'http://ganzheitlichesheilen.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobila24.com>
{'id': '89', 'url': 'http://casademobila24.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.org/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://drmedmichalzik.de>
{'id': '91', 'url': 'http://drmedmichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hgwimmobilien.org/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://da365.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://cyberfiber.me/robots.txt> (referer: None)
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:13:58 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.org> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://drmichalzik.eu>
{'id': '93', 'url': 'http://drmichalzik.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/> from <GET http://cyberfiber.me>
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://westside-linedance.com/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yogashop-paderborn.de> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nows.tk>
{'id': '94', 'url': 'http://nows.tk', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 06 Jun 2018 13:57:15 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westside-linedance.com> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://boncomputa.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://edelstahlscheckkartenhuelle.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-nektar.com/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rztefragenonline-unb.de>
{'id': '95', 'url': 'http://xn--rztefragenonline-unb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wwb1.net/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-friedrichwipfel.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://doc-blog.org/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.org>
{'id': '97', 'url': 'http://biotikon.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wwb1.net> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hgwimmobilien.org> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-friedrichwipfel.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://doc-blog.org> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-alexander-michalzik.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yogashop-paderborn.de>
{'id': '52', 'url': 'http://yogashop-paderborn.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://westside-linedance.com>
{'id': '99', 'url': 'http://westside-linedance.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 29 Jan 2020 19:02:17 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kaliner-yoga.de/robots.txt> from <GET https://kaliner-yoga.de/robots.txt>
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://boncomputa.de>
{'id': '88', 'url': 'http://boncomputa.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-alexander-michalzik.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://strukturplatten.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tibetanische-klangschalen.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://edelstahlscheckkartenhuelle.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://impax.de/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-nektar.com> (referer: None)
2023-05-02 12:13:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://wvm-branchenportal.com/robots.txt> from <GET http://wvm-branchenportal.com/robots.txt>
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wwb1.net>
{'id': '102', 'url': 'http://wwb1.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 30 Sep 2019 13:35:47 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hgwimmobilien.org>
{'id': '92', 'url': 'http://hgwimmobilien.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://strukturplatten.de> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://cyberfiber.me/> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taichichuan-friedrichwipfel.de>
{'id': '103', 'url': 'http://taichichuan-friedrichwipfel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://doc-blog.org>
{'id': '104', 'url': 'http://doc-blog.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobilashop.eu/robots.txt> (referer: None)
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://sulden-apresski.com/robots.txt> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dr-alexander-michalzik.de>
{'id': '105', 'url': 'http://dr-alexander-michalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://edelstahlscheckkartenhuelle.de>
{'id': '101', 'url': 'http://edelstahlscheckkartenhuelle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobilashop.eu> (referer: None)
2023-05-02 12:13:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-nektar.com>
{'id': '96', 'url': 'http://ouzo-nektar.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://impax.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sulden-apresski.com> (referer: None)
2023-05-02 12:13:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://da365.de/> from <GET http://da365.de>
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tibetanische-klangschalen.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenheim-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wvm-branchenportal.com/robots.txt> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://strukturplatten.de>
{'id': '106', 'url': 'http://strukturplatten.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 04 Aug 2021 09:44:09 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://cyberfiber.me/>
{'id': '73', 'url': 'https://cyberfiber.me/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'cyberfiber.me', 'ssl_start': '20220206235959Z', 'ssl_expire': '20220206235959Z'}
2023-05-02 12:13:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://wvm-branchenportal.com/> from <GET http://wvm-branchenportal.com>
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.bavarian-starlights.com/> (referer: None)
2023-05-02 12:13:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://marcodesigner.com/robots.txt> from <GET http://marcodesigner.com/robots.txt>
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ateliereichner.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wvm-branchenportal.com/> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobilashop.eu>
{'id': '110', 'url': 'http://casademobilashop.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://impax.de>
{'id': '100', 'url': 'http://impax.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://sulden-apresski.com>
{'id': '111', 'url': 'http://sulden-apresski.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 13 Mar 2018 16:25:36 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ateliereichner.de> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tibetanische-klangschalen.de>
{'id': '107', 'url': 'http://tibetanische-klangschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-esszimmer.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mastershaft.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:13:59 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "marcodesigner.com"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'marcodesigner.com'))])
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.bavarian-starlights.com/>
{'id': '66', 'url': 'http://www.bavarian-starlights.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-esszimmer.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenheim-willebadessen.de> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.sprachlos-ev-beratung.de/>
{'id': '10', 'url': 'https://www.sprachlos-ev-beratung.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'sprachlos-ev-beratung.de', 'ssl_start': '20230711235959Z', 'ssl_expire': '20230711235959Z'}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dr-michalzik.at/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://wvm-branchenportal.com/>
{'id': '108', 'url': 'https://wvm-branchenportal.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 14 Oct 2020 22:24:24 GMT', 'tableLayout': False, 'ssl_name': 'wvm-branchenportal.com', 'ssl_start': '20240307235959Z', 'ssl_expire': '20240307235959Z'}
2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ateliereichner.de>
{'id': '114', 'url': 'http://ateliereichner.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://openlate.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-esszimmer.de>
{'id': '118', 'url': 'http://massivholz-esszimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nowikow.org/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mastershaft.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tibetanischeklagschalen.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-schlafzimmer.com/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lift-taxi.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://carcal.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nevaton.eu/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://marcodesigner.com/robots.txt> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://altenheim-willebadessen.de>
{'id': '58', 'url': 'http://altenheim-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-schlafzimmer.com> (referer: None)
2023-05-02 12:13:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://marcodesigner.com/> from <GET http://marcodesigner.com>
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://da365.de/> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hilcura.com/robots.txt> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mastershaft.de>
{'id': '113', 'url': 'http://mastershaft.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.org/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://heueu.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://wahnfriet.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://djmirco.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-michalzik.at> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nowikow.org> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tibetanischeklagschalen.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.org> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://heueu.de> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-schlafzimmer.com>
{'id': '122', 'url': 'http://massivholz-schlafzimmer.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://djmirco.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://el-toque-latino.com/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://carcal.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.info/robots.txt> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://da365.de/>
{'id': '62', 'url': 'https://da365.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'da365.de', 'ssl_start': '20230708235959Z', 'ssl_expire': '20230708235959Z'}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://el-toque-latino.com> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://style-and-smile.com> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dr-michalzik.at>
{'id': '117', 'url': 'http://dr-michalzik.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.info> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kindz.de> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nowikow.org>
{'id': '119', 'url': 'http://nowikow.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tibetanischeklagschalen.de>
{'id': '121', 'url': 'http://tibetanischeklagschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-alpha-ketoglutarat.org>
{'id': '126', 'url': 'http://arginin-alpha-ketoglutarat.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://heueu.de>
{'id': '125', 'url': 'http://heueu.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 12 Sep 2009 08:57:13 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexander-michalzik.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wahnfriet.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-online-fragen.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://die-goldene-cd.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://biotikon.es/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://marcodesigner.com/> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://djmirco.de>
{'id': '127', 'url': 'http://djmirco.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://aufklebershop-geislingen.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://carcal.de>
{'id': '123', 'url': 'http://carcal.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-online-fragen.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexander-michalzik.de> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://el-toque-latino.com>
{'id': '129', 'url': 'http://el-toque-latino.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 07 Sep 2007 23:41:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://style-and-smile.com>
{'id': '77', 'url': 'http://style-and-smile.com', 'status': 200, 'title': 'WordPress 5.3.0 – Eine weitere WordPress-Website', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://aufklebershop-geislingen.de> (referer: None)
2023-05-02 12:13:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> from <GET http://xn--wohlfhlwerkstatt-nzb.de/robots.txt>
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.info>
{'id': '130', 'url': 'http://biotikon.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronamitarbeiterschutz.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kindz.de>
{'id': '26', 'url': 'http://kindz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://springblade.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinoline-quinone.org/robots.txt> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wahnfriet.de>
{'id': '120', 'url': 'http://wahnfriet.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://edv-frenzel.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://die-goldene-cd.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinoline-quinone.org> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronamitarbeiterschutz.de> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://marcodesigner.com/>
{'id': '115', 'url': 'https://marcodesigner.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'die-rohrreinigung.com', 'ssl_start': '20210714235959Z', 'ssl_expire': '20210714235959Z'}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://openlate.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tribulus-saponine.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arzt-online-fragen.de>
{'id': '134', 'url': 'http://arzt-online-fragen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nevaton.eu> (referer: None)
2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://alexander-michalzik.de>
{'id': '133', 'url': 'http://alexander-michalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://edv-frenzel.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.es> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://aufklebershop-geislingen.de>
{'id': '135', 'url': 'http://aufklebershop-geislingen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Jul 2010 08:21:56 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tribulus-saponine.de> (referer: None)
2023-05-02 12:13:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://delta-mb.de/robots.txt> from <GET http://delta-mb.de/robots.txt>
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-dosierung.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://die-goldene-cd.de>
{'id': '131', 'url': 'http://die-goldene-cd.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pyrroloquinoline-quinone.org>
{'id': '140', 'url': 'http://pyrroloquinoline-quinone.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coronamitarbeiterschutz.de>
{'id': '137', 'url': 'http://coronamitarbeiterschutz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://abandonedzone.com/robots.txt> (referer: None)
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://springblade.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-dosierung.de> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://openlate.de>
{'id': '54', 'url': 'http://openlate.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> (referer: None)
2023-05-02 12:13:59 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nevaton.eu>
{'id': '98', 'url': 'http://nevaton.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://patcy.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> from <GET http://xn--wohlfhlwerkstatt-nzb.de>
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://abandonedzone.com> (referer: None)
2023-05-02 12:13:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hilcura.de/> from <GET http://hilcura.com>
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://edv-frenzel.de>
{'id': '141', 'url': 'http://edv-frenzel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.es>
{'id': '128', 'url': 'http://biotikon.es', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tribulus-saponine.de>
{'id': '142', 'url': 'http://tribulus-saponine.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-blog.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.health/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://formergy.biz/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-blog.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.health> (referer: None)
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://springblade.de>
{'id': '139', 'url': 'http://springblade.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://resveratrol-dosierung.de>
{'id': '145', 'url': 'http://resveratrol-dosierung.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://streamify.tv/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://huckbros.com/robots.txt> from <GET http://huckbros.com/robots.txt>
2023-05-02 12:13:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:13:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://abandonedzone.com>
{'id': '146', 'url': 'http://abandonedzone.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://patcy.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/robots.txt> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fmz-frankershausen.de> (referer: None)
2023-05-02 12:13:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://crazybulls.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon-blog.de>
{'id': '149', 'url': 'http://biotikon-blog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.health>
{'id': '150', 'url': 'http://biotikon.health', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://crazybulls.de> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fusionstreet.com/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://huckbros.com/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://pixxpress.com/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://formergy.biz> (referer: None)
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://huckbros.com/> from <GET http://huckbros.com>
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yuriol.com/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pixxpress.com> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://patcy.de>
{'id': '148', 'url': 'http://patcy.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fmz-frankershausen.de>
{'id': '20', 'url': 'http://fmz-frankershausen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> (referer: None)
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfotenabdruck.info/robots.txt> from <GET http://pfotenabdruck.info/robots.txt>
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://huckbros.com/> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://crazybulls.de>
{'id': '153', 'url': 'http://crazybulls.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Dec 2022 21:55:50 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://formergy.biz>
{'id': '143', 'url': 'http://formergy.biz', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pixxpress.com>
{'id': '155', 'url': 'http://pixxpress.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 26 Aug 2011 20:30:29 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www6.xn--wohlfhlwerkstatt-nzb.de/>
{'id': '138', 'url': 'http://www6.xn--wohlfhlwerkstatt-nzb.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://huckbros.com/>
{'id': '151', 'url': 'https://huckbros.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Feb 2023 08:29:22 GMT', 'tableLayout': False, 'ssl_name': 'huckbros.com', 'ssl_start': '20230715235959Z', 'ssl_expire': '20230715235959Z'}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://pfotenabdruck.info/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://delta-mb.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:14:00 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://delta-mb.de/> from <GET http://delta-mb.de>
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfotenabdruck.info/> from <GET http://pfotenabdruck.info>
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pflegeplatz-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fusionstreet.com> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://klappstuhl51.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.com/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfotenabdruck.info/> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.com> (referer: None)
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://rustikal-lecker.com/robots.txt> from <GET http://rustikal-lecker.com/robots.txt>
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dirkmeineke.de/robots.txt> from <GET http://dirkmeineke.de/robots.txt>
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://manwe.net/robots.txt> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fusionstreet.com>
{'id': '154', 'url': 'http://fusionstreet.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "rustikal-lecker.com"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'rustikal-lecker.com'))])
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://pfotenabdruck.info/>
{'id': '156', 'url': 'https://pfotenabdruck.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 21 Apr 2020 09:28:05 GMT', 'tableLayout': False, 'ssl_name': 'pfotenabdruck.info', 'ssl_start': '20230609235959Z', 'ssl_expire': '20230609235959Z'}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinolinequinone.org/robots.txt> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativopc.com>
{'id': '160', 'url': 'http://3nativopc.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hilcura.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://klappstuhl51.de> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinolinequinone.org> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://dirkmeineke.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pflegeplatz-willebadessen.de> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dirkmeineke.de/> from <GET http://dirkmeineke.de>
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://kaliner-yoga.de/> from <GET http://kaliner-yoga.de>
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://efggp.de/robots.txt> from <GET http://efggp.de/robots.txt>
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.pfotenabdruck.inforobots.txt> from <GET http://pfoten-abdruck.de/robots.txt>
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dirkmeineke.de/> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://klappstuhl51.de>
{'id': '159', 'url': 'http://klappstuhl51.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thomasludwig.net/robots.txt> (referer: None)
2023-05-02 12:14:00 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:14:00 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:14:00 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:14:00 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pyrroloquinolinequinone.org>
{'id': '164', 'url': 'http://pyrroloquinolinequinone.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pflegeplatz-willebadessen.de>
{'id': '158', 'url': 'http://pflegeplatz-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://jagdgruppe-bad-aibling.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnendiscount.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://rustikal-lecker.com/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://jagdgruppe-bad-aibling.de> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://delta-mb.de/> (referer: None)
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://yuriol.com/> from <GET http://yuriol.com>
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dirkmeineke.de/>
{'id': '163', 'url': 'https://dirkmeineke.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 13 Mar 2023 20:03:35 GMT', 'tableLayout': True, 'ssl_name': 'dirkmeineke.de', 'ssl_start': '20230611205738Z', 'ssl_expire': '20230611205738Z'}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobilashop.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://manwe.net> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holistic-medicine.ch/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobilashop.de> (referer: None)
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://rustikal-lecker.com/> from <GET http://rustikal-lecker.com>
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://go-paps.com/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holistic-medicine.ch> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnendiscount.de> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://jagdgruppe-bad-aibling.de>
{'id': '168', 'url': 'http://jagdgruppe-bad-aibling.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 27 Apr 2023 12:47:45 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://go-paps.com> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://delta-mb.de/>
{'id': '144', 'url': 'https://delta-mb.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'ascon-systems.de', 'ssl_start': '20230706235959Z', 'ssl_expire': '20230706235959Z'}
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://manwe.net>
{'id': '157', 'url': 'http://manwe.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/robots.txt> from <GET http://sememo.de/robots.txt>
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://discounterticket.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yuriol.com/> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.net/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobilashop.de>
{'id': '169', 'url': 'http://casamobilashop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thomasludwig.net> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gukrause.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://holistic-medicine.ch>
{'id': '171', 'url': 'http://holistic-medicine.ch', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gukrause.de> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnendiscount.de>
{'id': '165', 'url': 'http://arbeitsbuehnendiscount.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://victoria-einrichtungen.eu/robots.txt> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://go-paps.com>
{'id': '172', 'url': 'http://go-paps.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 14 Apr 2022 14:49:32 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wikistudien.org/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenpflege-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://yuriol.com/>
{'id': '116', 'url': 'https://yuriol.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 27 Apr 2023 09:59:24 GMT', 'tableLayout': False, 'ssl_name': 'yuriol.com', 'ssl_start': '20230531103008Z', 'ssl_expire': '20230531103008Z'}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://victoria-einrichtungen.eu> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wikistudien.org> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thomasludwig.net>
{'id': '162', 'url': 'http://thomasludwig.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://discounterticket.de> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gukrause.de>
{'id': '175', 'url': 'http://gukrause.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-nebenwirkungen.info/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hotcoconut.eu/robots.txt> from <GET http://hotcoconut.eu/robots.txt>
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rustikal-lecker.com/> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-nebenwirkungen.info> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotcoconut.eu/robots.txt> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.de/robots.txt> (referer: None)
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://victoria-einrichtungen.eu>
{'id': '176', 'url': 'http://victoria-einrichtungen.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hotcoconut.eu/> from <GET http://hotcoconut.eu>
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wikistudien.org>
{'id': '177', 'url': 'http://wikistudien.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://discounterticket.de>
{'id': '170', 'url': 'http://discounterticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.de> (referer: None)
2023-05-02 12:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotcoconut.eu/> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kley-net.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://efggp.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://rustikal-lecker.com/>
{'id': '161', 'url': 'https://rustikal-lecker.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:14:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kaliner-yoga.de/> from <GET https://kaliner-yoga.de/>
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenpflege-willebadessen.de> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kley-net.de> (referer: None)
2023-05-02 12:14:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://efggp.de/> from <GET http://efggp.de>
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://resveratrol-nebenwirkungen.info>
{'id': '179', 'url': 'http://resveratrol-nebenwirkungen.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fabian-klemt.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://christian-kilthau.com/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fabian-klemt.de> (referer: None)
2023-05-02 12:14:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://richard-deutsch.com/robots.txt/> from <GET http://richard-deutsch.com/robots.txt>
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.de>
{'id': '182', 'url': 'http://wasistopc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hotcoconut.eu/>
{'id': '181', 'url': 'https://hotcoconut.eu/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'hotcoconut.eu', 'ssl_start': '20230518232429Z', 'ssl_expire': '20230518232429Z'}
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://altenpflege-willebadessen.de>
{'id': '173', 'url': 'http://altenpflege-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kley-net.de>
{'id': '183', 'url': 'http://kley-net.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 01 Mar 2022 16:42:42 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hilcura.de/> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://stein-und-soehne.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://stein-und-soehne.de> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fabian-klemt.de>
{'id': '185', 'url': 'http://fabian-klemt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 11 Dec 2008 11:42:32 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://erp-ratschlag.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://christian-kilthau.com> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hilcura.de/>
{'id': '109', 'url': 'https://hilcura.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'hilcura.de', 'ssl_start': '20240329235959Z', 'ssl_expire': '20240329235959Z'}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://erp-ratschlag.de> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://stein-und-soehne.de>
{'id': '187', 'url': 'http://stein-und-soehne.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 16 Nov 2022 23:48:49 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lifttaxi.eu/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lift-taxi.de> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lilier.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://richard-deutsch.com/robots.txt/> (referer: None)
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 53 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 61 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 93 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 97 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 104 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 115 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 123 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 131 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 149 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 151 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 153 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 205 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 235 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 242 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 254 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 273 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 280 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 311 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 318 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 348 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 355 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 374 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 382 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 383 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 391 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 399 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 400 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 407 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 408 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 409 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 416 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 417 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 418 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 425 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 426 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 427 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 434 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 435 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 436 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 447 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 448 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 449 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 451 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 452 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 453 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 454 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 482 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 495 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 496 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 502 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 532 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 552 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 553 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 554 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 555 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 556 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 558 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 559 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 560 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 561 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 562 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 563 without any user agent to enforce it on.
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lilier.de> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mitochondriale-medizin.org/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 779: invalid start byte
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kinderpartyfun.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-fragen-online-ktb.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://christian-kilthau.com>
{'id': '186', 'url': 'http://christian-kilthau.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://erp-ratschlag.de>
{'id': '189', 'url': 'http://erp-ratschlag.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Jul 2010 08:53:51 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lift-taxi.de>
{'id': '84', 'url': 'http://lift-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-fragen-online-ktb.de> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lilier.de>
{'id': '190', 'url': 'http://lilier.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 30 Dec 2013 12:30:38 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gracic.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://streamify.tv> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-plus.org/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://richard-deutsch.com> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-plus.org> (referer: None)
2023-05-02 12:14:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.formativmedia.atrobots.txt> from <GET http://formativ-print.at/robots.txt>
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzte-fragen-online-ktb.de>
{'id': '192', 'url': 'http://xn--rzte-fragen-online-ktb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://efggp.de/> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://streamify.tv>
{'id': '124', 'url': 'http://streamify.tv', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dukartstein.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://fuerdiesache.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondriale-medizin.org> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://richard-deutsch.com>
{'id': '180', 'url': 'http://richard-deutsch.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-plus.org>
{'id': '195', 'url': 'http://arginin-plus.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gracic.de> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://efggp.de/>
{'id': '166', 'url': 'https://efggp.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'efggp.de', 'ssl_start': '20230611231725Z', 'ssl_expire': '20230611231725Z'}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dralexandermichalzik.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://himalayaklangschalen.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dralexandermichalzik.de> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mitochondriale-medizin.org>
{'id': '184', 'url': 'http://mitochondriale-medizin.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.exelant.derobots.txt> from <GET http://exelant.de/robots.txt>
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gracic.de>
{'id': '193', 'url': 'http://gracic.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexandermichalzik.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexandermichalzik.de> (referer: None)
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dralexandermichalzik.de>
{'id': '199', 'url': 'http://dralexandermichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fuerdiesache.de> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://himalayaklangschalen.de> (referer: None)
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://wengkbuehle.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://alexandermichalzik.de>
{'id': '202', 'url': 'http://alexandermichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lokomotor.de/robots.txt> (referer: None)
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:14:01 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fuerdiesache.de>
{'id': '194', 'url': 'http://fuerdiesache.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lokomotor.de> (referer: None)
2023-05-02 12:14:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kinderpartyfun.de/> from <GET http://kinderpartyfun.de>
2023-05-02 12:14:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://webmarktplatz24.de/robots.txt> from <GET http://webmarktplatz24.de/robots.txt>
2023-05-02 12:14:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://himalayaklangschalen.de>
{'id': '200', 'url': 'http://himalayaklangschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.kinderpartyfun.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.kinderpartyfun.de'))])
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lokomotor.de>
{'id': '204', 'url': 'http://lokomotor.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://webmarktplatz24.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:14:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://webmarktplatz24.de/> from <GET http://webmarktplatz24.de>
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kopfstandstuhl.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://webmarktplatz24.de/> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.kinderpartyfun.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wengkbuehle.de> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dukartstein.de> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://secstate.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://webmarktplatz24.de/>
{'id': '205', 'url': 'https://webmarktplatz24.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'webmarktplatz24.de', 'ssl_start': '20230523235959Z', 'ssl_expire': '20230523235959Z'}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://secstate.de> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trendsandtechnik.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://osd2000.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wengkbuehle.de>
{'id': '198', 'url': 'http://wengkbuehle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://osd2000.de> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kinderpartyfun.de/> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dukartstein.de>
{'id': '152', 'url': 'http://dukartstein.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cnc-fanpage.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trendsandtechnik.de> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.kaliner-yoga.de/>
{'id': '21', 'url': 'https://www.kaliner-yoga.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'kaliner-yoga.de', 'ssl_start': '20230615235959Z', 'ssl_expire': '20230615235959Z'}
2023-05-02 12:14:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2Frobots.txt&bp-auth=1&action=bpnoaccess> from <GET https://sememo.de/robots.txt>
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://secstate.de>
{'id': '207', 'url': 'http://secstate.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 15 Mar 2010 17:15:22 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cnc-fanpage.de> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lingk-shops.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lingk-shops.de> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://osd2000.de>
{'id': '209', 'url': 'http://osd2000.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 20 Feb 2011 15:04:02 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://maik-wiege.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.kinderpartyfun.de/>
{'id': '178', 'url': 'https://www.kinderpartyfun.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://trendsandtechnik.de>
{'id': '208', 'url': 'http://trendsandtechnik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://maik-wiege.de> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cnc-fanpage.de>
{'id': '210', 'url': 'http://cnc-fanpage.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lifttaxi.eu> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lingk-shops.de>
{'id': '211', 'url': 'http://lingk-shops.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 28 Sep 2021 13:22:44 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://chronisch-untervoegelt.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nrw-zwerge.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 766: invalid start byte
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dinner-ticket.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://maik-wiege.de>
{'id': '213', 'url': 'http://maik-wiege.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lifttaxi.eu>
{'id': '132', 'url': 'http://lifttaxi.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobilashop.com/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobilashop.com> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://johannes-doerfler.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dinner-ticket.de> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.eu/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 762: invalid start byte
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://blass-net.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nrw-zwerge.de> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://blass-net.de> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://krop.at/robots.txt> (referer: None)
2023-05-02 12:14:02 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:14:02 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://peter-flaspoehler.de/robots.txt> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobilashop.com>
{'id': '217', 'url': 'http://casademobilashop.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dinner-ticket.de>
{'id': '214', 'url': 'http://dinner-ticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://johannes-doerfler.de> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://peter-flaspoehler.de> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nrw-zwerge.de>
{'id': '212', 'url': 'http://nrw-zwerge.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://blass-net.de>
{'id': '218', 'url': 'http://blass-net.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 23 May 2018 16:59:30 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronaschutzhilfe.com/robots.txt> (referer: None)
2023-05-02 12:14:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://johannes-doerfler.de>
{'id': '216', 'url': 'http://johannes-doerfler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://peter-flaspoehler.de>
{'id': '220', 'url': 'http://peter-flaspoehler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 04 Mar 2023 14:19:35 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://krop.at> (referer: None)
2023-05-02 12:14:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronaschutzhilfe.com> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://birtekaufmann-photography.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 59 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 61 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 79 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 105 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 113 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 117 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 133 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 138 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 167 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 175 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 183 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 187 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 188 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 191 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 203 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 213 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 227 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 229 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 233 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 235 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 237 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 240 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 242 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 246 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 248 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 253 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 255 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 259 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 263 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 264 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 265 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 268 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 269 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 270 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 272 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 274 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 275 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 278 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 281 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 284 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 289 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 297 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 298 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 302 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 304 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 306 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 310 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 313 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 317 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 320 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 322 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 327 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 328 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 332 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 333 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 338 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 342 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 346 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 350 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 358 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 368 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 376 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 380 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 383 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 384 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 386 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 387 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 391 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 396 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 412 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 429 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 477 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 481 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 483 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 484 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 486 without any user agent to enforce it on.
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yogamaster.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://krop.at>
{'id': '219', 'url': 'http://krop.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kopfstandstuhl.de> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coronaschutzhilfe.com>
{'id': '223', 'url': 'http://coronaschutzhilfe.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2Frobots.txt&bp-auth=1&action=bpnoaccess> (referer: None)
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 9 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 131 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 145 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 149 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 157 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 159 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 166 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 169 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 177 without any user agent to enforce it on.
2023-05-02 12:14:03 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mobilemietstation.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:14:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/> from <GET http://sememo.de>
2023-05-02 12:14:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://fun4-u.info/robots.txt> from <GET http://fun4-u.info/robots.txt>
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila-shop.info/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://steiger-taxi.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kopfstandstuhl.de>
{'id': '197', 'url': 'http://kopfstandstuhl.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila-shop.info> (referer: None)
2023-05-02 12:14:03 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "fun4-u.info"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'fun4-u.info'))])
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://serenafate.com/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://fun4-u.info/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mobilemietstation.de> (referer: None)
2023-05-02 12:14:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://fun4-u.info/> from <GET http://fun4-u.info>
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hotland.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://urbaczek.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://steiger-taxi.de> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila-shop.info>
{'id': '227', 'url': 'http://casamobila-shop.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://fun4-u.info/> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://urbaczek.de> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hotland.de> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://atlantis-magazin.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mobilemietstation.de>
{'id': '224', 'url': 'http://mobilemietstation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://serenafate.com> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://fam-rauch.com/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hyaluronsaeure-kapseln.org/robots.txt> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://steiger-taxi.de>
{'id': '225', 'url': 'http://steiger-taxi.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fam-rauch.com> (referer: None)
2023-05-02 12:14:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.medizinrechtler.inforobots.txt> from <GET http://grad-der-behinderung.de/robots.txt>
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://fun4-u.info/>
{'id': '226', 'url': 'https://fun4-u.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 03 Aug 2019 10:52:54 GMT', 'tableLayout': True, 'ssl_name': 'fun4-u.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sound-producer.com/robots.txt> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://urbaczek.de>
{'id': '230', 'url': 'http://urbaczek.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 05 Feb 2023 15:42:17 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hyaluronsaeure-kapseln.org> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hotland.de>
{'id': '229', 'url': 'http://hotland.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://weihrauchforum.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://serenafate.com>
{'id': '228', 'url': 'http://serenafate.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://birtekaufmann-photography.de> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://herzinfarkt-vorbeugen-herzgesundheit.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://atlantis-magazin.de> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://weihrauchforum.de> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fam-rauch.com>
{'id': '232', 'url': 'http://fam-rauch.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 19 Jun 2011 18:13:24 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://herzinfarkt-vorbeugen-herzgesundheit.de> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hyaluronsaeure-kapseln.org>
{'id': '233', 'url': 'http://hyaluronsaeure-kapseln.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://chronisch-untervoegelt.de> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://birtekaufmann-photography.de>
{'id': '215', 'url': 'http://birtekaufmann-photography.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://atlantis-magazin.de>
{'id': '231', 'url': 'http://atlantis-magazin.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://santakruz.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://weihrauchforum.de>
{'id': '235', 'url': 'http://weihrauchforum.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://herzinfarkt-vorbeugen-herzgesundheit.de>
{'id': '236', 'url': 'http://herzinfarkt-vorbeugen-herzgesundheit.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://santakruz.de> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://felgenfreddy.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://chronisch-untervoegelt.de>
{'id': '188', 'url': 'http://chronisch-untervoegelt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://essenonpaper.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://losmenombak-mentawai.com/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://santakruz.de>
{'id': '240', 'url': 'http://santakruz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 30 Oct 2009 20:47:40 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.info/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essenonpaper.de> (referer: None)
2023-05-02 12:14:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://badischtauchen.de/robots.txt> from <GET http://badischtauchen.de/robots.txt>
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://felgenfreddy.de> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongstrategy.com/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongstrategy.com> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://pharmabox.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://essenonpaper.de>
{'id': '241', 'url': 'http://essenonpaper.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://badischtauchen.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sememo.de/> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pharmabox.de> (referer: None)
2023-05-02 12:14:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://badischtauchen.de/> from <GET http://badischtauchen.de>
2023-05-02 12:14:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.mattner.orgrobots.txt> from <GET http://mattner.org/robots.txt>
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://felgenfreddy.de>
{'id': '237', 'url': 'http://felgenfreddy.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://future-fashion.de/robots.txt> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dejongstrategy.com>
{'id': '243', 'url': 'http://dejongstrategy.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 28 Dec 2014 13:42:08 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://future-fashion.de> (referer: None)
2023-05-02 12:14:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://badischtauchen.de/> (referer: None)
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sememo.de/>
{'id': '174', 'url': 'https://sememo.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'sememo.de', 'ssl_start': '20230710235959Z', 'ssl_expire': '20230710235959Z'}
2023-05-02 12:14:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pharmabox.de>
{'id': '244', 'url': 'http://pharmabox.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 03 Mar 2020 18:24:35 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://x-schreck.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://x-schreck.de> (referer: None)
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://future-fashion.de>
{'id': '247', 'url': 'http://future-fashion.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 05 Jan 2016 16:33:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://meditationkisse.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://badischtauchen.de/>
{'id': '242', 'url': 'https://badischtauchen.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 18 Sep 2021 11:27:27 GMT', 'tableLayout': False, 'ssl_name': 'badischtauchen.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://x-schreck.de>
{'id': '248', 'url': 'http://x-schreck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 06 Oct 2007 18:12:18 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taxi-meile.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 104 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 108 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 112 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 118 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 130 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 132 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 138 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 140 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 146 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 162 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 166 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 168 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 170 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 174 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 176 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 178 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 182 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 184 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 188 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 190 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 192 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 196 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 200 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://meditationkisse.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-lexikon.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://meditationkisse.de>
{'id': '249', 'url': 'http://meditationkisse.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ostsea.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-lexikon.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ostsea.de> (referer: None)
2023-05-02 12:14:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.okv-weiden.de/robots.txt> from <GET http://okv-weiden.de/robots.txt>
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gasthof-rolfes.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sound-producer.com> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bioticon.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/robots.txt> from <GET http://www.okv-weiden.de/robots.txt>
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taxi-meile.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bioticon.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://grothues8.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 9 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 93 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 99 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 123 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 130 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 133 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 140 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 153 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 161 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 162 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 168 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 170 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 171 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 174 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 178 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 184 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 187 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 190 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 206 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 209 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 212 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 239 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 240 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 241 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 244 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 245 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 246 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 247 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 248 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 249 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 251 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 252 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 253 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 254 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 255 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 256 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 258 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 259 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 265 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 268 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 271 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 274 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 280 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 283 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 289 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 298 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 304 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 310 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 311 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 312 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 313 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 314 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 316 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 317 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 318 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 319 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 320 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 321 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 322 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 323 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 325 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 327 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 331 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 332 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 333 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 334 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 335 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 336 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 338 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 340 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 341 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 342 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 343 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 344 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 348 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 349 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 352 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 353 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 357 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 358 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 361 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 362 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 365 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 366 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 369 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 370 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 373 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 374 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 378 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 382 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 386 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 389 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 397 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 398 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 402 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 405 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 406 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 412 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 413 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 414 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 420 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 425 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 428 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 431 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 437 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 438 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 439 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 440 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 441 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 442 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 443 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 444 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 445 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 446 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 447 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 457 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 462 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 466 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 470 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 473 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 474 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 476 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 477 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 478 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 481 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 482 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 483 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 484 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 485 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 488 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 489 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 490 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 491 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 493 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 499 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 501 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 502 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 503 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 504 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 521 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 522 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 530 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 531 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 532 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 533 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 534 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 536 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 538 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 539 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 540 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 542 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 543 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 544 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 545 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 546 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 547 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 549 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 550 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 556 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 558 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 561 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 562 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 563 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 564 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 565 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 566 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 571 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 579 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 580 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 581 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 586 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 589 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 590 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 591 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 592 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 593 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 595 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 596 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 597 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 599 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 600 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 601 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 602 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 606 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 608 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 612 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 614 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 625 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 626 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 627 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 628 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 629 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 630 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 631 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 632 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 633 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 634 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 635 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 639 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 641 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 647 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 649 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 658 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 661 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 667 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 672 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 678 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 679 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 680 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 682 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 689 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 690 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 691 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 692 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 694 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 699 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 700 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 703 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 707 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 709 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 710 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 711 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 714 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 717 without any user agent to enforce it on.
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://klangschalen-schop.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arbeitsbuehnen-lexikon.de>
{'id': '250', 'url': 'http://arbeitsbuehnen-lexikon.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ostsea.de>
{'id': '251', 'url': 'http://ostsea.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 15 Jan 2017 21:15:54 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://sound-producer.com>
{'id': '203', 'url': 'http://sound-producer.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.dknv.de/> from <GET http://dknv.de/robots.txt>
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taxi-meile.de>
{'id': '246', 'url': 'http://taxi-meile.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bioticon.de>
{'id': '253', 'url': 'http://bioticon.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.okv-weiden.de/> from <GET http://okv-weiden.de>
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://klangschalen-schop.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikonblog.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/robots.txt> from <GET http://www.okv-weiden.de/robots.txt>
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://has-wbe.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikonblog.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://michaelbuschke.com/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/> from <GET http://www.okv-weiden.de/>
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/> (referer: None)
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://klangschalen-schop.de>
{'id': '254', 'url': 'http://klangschalen-schop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.dknv.de/> from <GET http://dknv.de>
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://michaelbuschke.com> (referer: None)
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikonblog.de>
{'id': '257', 'url': 'http://biotikonblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://barbaraundjan.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:14:04 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kulturbeutelhamburg.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arztfragenonline.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://has-wbe.de> (referer: None)
2023-05-02 12:14:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.gasthof-rolfes.de/> from <GET http://gasthof-rolfes.de>
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://michaelbuschke.com>
{'id': '258', 'url': 'http://michaelbuschke.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 07 Sep 2019 22:37:35 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://barbaraundjan.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arztfragenonline.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://flugzeuge-weltweit.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yogamaster.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/> (referer: None)
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila.de>
{'id': '260', 'url': 'http://casamobila.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://peking-laufenburg.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://has-wbe.de>
{'id': '255', 'url': 'http://has-wbe.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westsidemusic.com/robots.txt> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://peking-laufenburg.de> (referer: None)
2023-05-02 12:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-fragen-online.de/robots.txt> (referer: None)
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://barbaraundjan.de>
{'id': '259', 'url': 'http://barbaraundjan.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://grothues8.de/Wordpress/> from <GET http://grothues8.de>
2023-05-02 12:14:04 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arztfragenonline.de>
{'id': '261', 'url': 'http://arztfragenonline.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.at/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-fragen-online.de> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yogamaster.de>
{'id': '221', 'url': 'http://yogamaster.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.at> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://was-ist-opc.de>
{'id': '262', 'url': 'http://was-ist-opc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.okv-weiden.de/>
{'id': '252', 'url': 'https://www.okv-weiden.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 02 May 2023 12:14:04 GMT', 'tableLayout': False, 'ssl_name': 'www.okv-weiden.de', 'ssl_start': '20230522235959Z', 'ssl_expire': '20230522235959Z'}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://flugzeuge-weltweit.de> (referer: None)
2023-05-02 12:14:05 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://volki.pb.photographyrobots.txt> from <GET http://galerie-bauer.de/robots.txt>
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nomadpublishing.store/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nomadpublishing.store> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobila24.eu/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westsidemusic.com> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://peking-laufenburg.de>
{'id': '264', 'url': 'http://peking-laufenburg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 24 Aug 2020 11:50:05 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www6.dknv.de/>
{'id': '256', 'url': 'http://www6.dknv.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arzt-fragen-online.de>
{'id': '266', 'url': 'http://arzt-fragen-online.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.at>
{'id': '267', 'url': 'http://biotikon.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://flugzeuge-weltweit.de>
{'id': '263', 'url': 'http://flugzeuge-weltweit.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobila24.eu> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://curcuma-nebenwirkungen.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://grinberg.de/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://curcuma-nebenwirkungen.de> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nomadpublishing.store>
{'id': '270', 'url': 'http://nomadpublishing.store', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://westsidemusic.com>
{'id': '265', 'url': 'http://westsidemusic.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://aus-keil-wird-schmidt.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://grinberg.de/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://grinberg.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobila24.eu>
{'id': '271', 'url': 'http://casademobila24.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://aus-keil-wird-schmidt.de> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://indiaschop.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grinberg.de> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://curcuma-nebenwirkungen.de>
{'id': '273', 'url': 'http://curcuma-nebenwirkungen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tropicanalife.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.gasthof-rolfes.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tropicanalife.de> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.eu/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://holzmoebel-experte.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://aus-keil-wird-schmidt.de>
{'id': '274', 'url': 'http://aus-keil-wird-schmidt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 22 Apr 2008 07:31:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.eu> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://grinberg.de>
{'id': '272', 'url': 'http://grinberg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 28 Jan 2023 20:32:03 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holzmoebel-experte.de> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://indiaschop.de> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzteonlinefragen-unb.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.kg-muellekolk.de/robots.txt> from <GET http://kg-muellekolk.de/robots.txt>
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tropicanalife.de>
{'id': '276', 'url': 'http://tropicanalife.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 28 Mar 2009 22:52:40 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzteonlinefragen-unb.de> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cradle-software.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativopc.eu>
{'id': '277', 'url': 'http://3nativopc.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://strumpfaffen.org/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cradle-software.de> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://holzmoebel-experte.de>
{'id': '278', 'url': 'http://holzmoebel-experte.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://indiaschop.de>
{'id': '275', 'url': 'http://indiaschop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://strumpfaffen.org> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bergium.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzteonlinefragen-unb.de>
{'id': '280', 'url': 'http://xn--rzteonlinefragen-unb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bergium.de> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grothues8.de/Wordpress/> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cradle-software.de>
{'id': '281', 'url': 'http://cradle-software.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://strumpfaffen.org>
{'id': '282', 'url': 'http://strumpfaffen.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 23 Apr 2012 11:57:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://studio-balance.at/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://studio-balance.at> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thebodyasarchive.com/robots.txt> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bergium.de>
{'id': '283', 'url': 'http://bergium.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 07 Nov 2017 13:41:54 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://grothues8.de/Wordpress/>
{'id': '222', 'url': 'http://grothues8.de/Wordpress/', 'status': 200, 'title': 'Linus Tjaard Rabea Hener Grothues – Das ist die Private Homepage von Linus Tjaard Rabea Henner Grothues', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thebodyasarchive.com> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.ch/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://reitanlage-riedmuehle.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:14:05 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.ch> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://studio-balance.at>
{'id': '285', 'url': 'http://studio-balance.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 30 Dec 2020 13:19:59 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-babyzimmer.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-shop-paderborn.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thebodyasarchive.com>
{'id': '286', 'url': 'http://thebodyasarchive.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 27 Dec 2020 18:13:23 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-babyzimmer.de> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.ch>
{'id': '287', 'url': 'http://biotikon.ch', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondrien-funktion.de/robots.txt> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondrien-funktion.de> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.com/robots.txt> (referer: None)
2023-05-02 12:14:05 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-babyzimmer.de>
{'id': '288', 'url': 'http://massivholz-babyzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.com> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://reitanlage-riedmuehle.de> (referer: None)
2023-05-02 12:14:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://format-mehrweg.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://format-mehrweg.de> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cosmecon.eu/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-dielenmoebel.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mitochondrien-funktion.de>
{'id': '290', 'url': 'http://mitochondrien-funktion.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.gasthof-rolfes.de/> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cosmecon.eu> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-dielenmoebel.de> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-alpha-ketoglutarat.com>
{'id': '291', 'url': 'http://arginin-alpha-ketoglutarat.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--kleinegrashpfer-9vb.at/robots.txt> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://reitanlage-riedmuehle.de>
{'id': '284', 'url': 'http://reitanlage-riedmuehle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--kleinegrashpfer-9vb.at> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.eu> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.net> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://format-mehrweg.de>
{'id': '292', 'url': 'http://format-mehrweg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 10 Dec 2019 16:45:18 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://kulturbeutelhamburg.de/> from <GET http://kulturbeutelhamburg.de>
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.gasthof-rolfes.de/>
{'id': '239', 'url': 'http://www.gasthof-rolfes.de/', 'status': 200, 'title': 'Gasthof Rolfes ', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cosmecon.eu>
{'id': '293', 'url': 'http://cosmecon.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 25 Nov 2009 14:28:26 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-dielenmoebel.de>
{'id': '294', 'url': 'http://massivholz-dielenmoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://skiverband-mv.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--kleinegrashpfer-9vb.at>
{'id': '295', 'url': 'http://xn--kleinegrashpfer-9vb.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 13 Apr 2023 07:16:11 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.eu>
{'id': '136', 'url': 'http://govido.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.net>
{'id': '75', 'url': 'http://govido.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://eventation.de/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dinnerticket.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ticila.com/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.kg-muellekolk.de/> from <GET http://kg-muellekolk.de>
2023-05-02 12:14:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://eventation.de/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://eventation.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ticila.com> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://skiverband-mv.de> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-lothar-wester.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://seat-factory.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dinnerticket.de> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://eventation.de> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://seat-factory.de> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://technolog-e.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ticila.com>
{'id': '300', 'url': 'http://ticila.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 05 Jan 2016 16:33:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://skiverband-mv.de>
{'id': '298', 'url': 'http://skiverband-mv.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dark-hawks.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://technolog-e.de> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mcpicchu.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mcpicchu.de> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dinnerticket.de>
{'id': '296', 'url': 'http://dinnerticket.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://trengo.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://eventation.de>
{'id': '299', 'url': 'http://eventation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://seat-factory.de>
{'id': '301', 'url': 'http://seat-factory.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 15 Jul 2015 14:10:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dirkklages.com/robots.txt> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://technolog-e.de>
{'id': '302', 'url': 'http://technolog-e.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 07 Nov 2014 10:40:41 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dirkklages.com> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mcpicchu.de>
{'id': '304', 'url': 'http://mcpicchu.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 23 Jul 2022 22:42:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lederoutfits.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lederoutfits.de> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trengo.de> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dirkklages.com>
{'id': '307', 'url': 'http://dirkklages.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 17 Apr 2021 09:45:46 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lederoutfits.de>
{'id': '308', 'url': 'http://lederoutfits.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Nov 2017 23:18:52 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://trengo.de>
{'id': '306', 'url': 'http://trengo.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-plomari.com/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-datenbank.net/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://teamtrzweb.de> (referer: None)
2023-05-02 12:14:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.nachhilfe-oase.de/robots.txt> from <GET http://123discounter.de/robots.txt>
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-plomari.com> (referer: None)
2023-05-02 12:14:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.nachhilfe-oase.de/> from <GET http://123discounter.de>
2023-05-02 12:14:06 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://teamtrzweb.de>
{'id': '305', 'url': 'http://teamtrzweb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://die-goldene-schallplatte.de/robots.txt> (referer: None)
2023-05-02 12:14:06 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 780: invalid start byte
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dark-hawks.de> (referer: None)
2023-05-02 12:14:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cymeg.de/robots.txt> (referer: None)
2023-05-02 12:14:07 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:07 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ouzo-plomari.com>
{'id': '309', 'url': 'http://ouzo-plomari.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cymeg.de> (referer: None)
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.eu/robots.txt> (referer: None)
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongsblog.de/robots.txt> (referer: None)
2023-05-02 12:14:07 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:14:07 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:14:07 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.eu> (referer: None)
2023-05-02 12:14:07 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:07 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.nachhilfe-oase.de/>
{'id': '311', 'url': 'http://www.nachhilfe-oase.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://die-goldene-schallplatte.de> (referer: None)
2023-05-02 12:14:07 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:07 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dark-hawks.de>
{'id': '279', 'url': 'http://dark-hawks.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:07 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:07 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cymeg.de>
{'id': '313', 'url': 'http://cymeg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 17 Apr 2021 09:45:46 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-shop-paderborn.de> (referer: None)
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://schiefer-lautsprecher.de/robots.txt> (referer: None)
2023-05-02 12:14:07 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:07 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativ-opc.eu>
{'id': '314', 'url': 'http://3nativ-opc.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/> (referer: None)
2023-05-02 12:14:07 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:07 [scrapy.core.scraper] DEBUG: Scraped from <200 http://die-goldene-schallplatte.de>
{'id': '312', 'url': 'http://die-goldene-schallplatte.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:07 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:07 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-shop-paderborn.de>
{'id': '289', 'url': 'http://yoga-shop-paderborn.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:07 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:07 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.kg-muellekolk.de/>
{'id': '268', 'url': 'http://www.kg-muellekolk.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongsblog.de> (referer: None)
2023-05-02 12:14:07 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:07 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dejongsblog.de>
{'id': '310', 'url': 'http://dejongsblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kulturbeutelhamburg.de/> (referer: None)
2023-05-02 12:14:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.info> (referer: None)
2023-05-02 12:14:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-lothar-wester.de> (referer: None)
2023-05-02 12:14:08 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:08 [scrapy.core.scraper] DEBUG: Scraped from <200 https://kulturbeutelhamburg.de/>
{'id': '147', 'url': 'https://kulturbeutelhamburg.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'kulturbeutelhamburg.de', 'ssl_start': '20230605235959Z', 'ssl_expire': '20230605235959Z'}
2023-05-02 12:14:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schiefer-lautsprecher.de> (referer: None)
2023-05-02 12:14:08 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:08 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.info>
{'id': '112', 'url': 'http://govido.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:08 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:08 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-lothar-wester.de>
{'id': '297', 'url': 'http://yoga-lothar-wester.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:08 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:08 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schiefer-lautsprecher.de>
{'id': '315', 'url': 'http://schiefer-lautsprecher.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-datenbank.net> (referer: None)
2023-05-02 12:14:08 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:08 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-datenbank.net>
{'id': '303', 'url': 'http://auto-datenbank.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:09 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.org/robots.txt> (referer: None)
2023-05-02 12:14:09 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:14:09 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ws-gmbh.de/robots.txt> (referer: None)
2023-05-02 12:14:09 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:14:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://photos.hr-photo.derobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:14:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://losmenombak-mentawai.com> (referer: None)
2023-05-02 12:14:12 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:12 [scrapy.core.scraper] DEBUG: Scraped from <200 http://losmenombak-mentawai.com>
{'id': '191', 'url': 'http://losmenombak-mentawai.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.pfotenabdruck.inforobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:14:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.exelant.derobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:14:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.formativmedia.atrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:14:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.org> (referer: None)
2023-05-02 12:14:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ws-gmbh.de> (referer: None)
2023-05-02 12:14:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://govido.org>
{'id': '238', 'url': 'http://govido.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ws-gmbh.de>
{'id': '206', 'url': 'http://ws-gmbh.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.medizinrechtler.inforobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:14:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.mattner.orgrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:14:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://volki.pb.photographyrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:14:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://photos.hr-photo.derobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:14:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://hr-photo.de/robots.txt>: DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:14:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://photos.hr-photo.de> from <GET http://hr-photo.de>
2023-05-02 12:14:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://photos.hr-photo.de/robots.txt> (referer: None)
2023-05-02 12:14:26 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 771: invalid start byte
2023-05-02 12:14:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://photos.hr-photo.de> (referer: None)
2023-05-02 12:14:27 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://photos.hr-photo.de>
{'id': '24', 'url': 'http://photos.hr-photo.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': True, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.pfotenabdruck.inforobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:14:31 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://pfoten-abdruck.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:14:31 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.pfotenabdruck.info> from <GET http://pfoten-abdruck.de>
2023-05-02 12:14:31 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.pfotenabdruck.info/robots.txt> (referer: None)
2023-05-02 12:14:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.pfotenabdruck.info> (referer: None)
2023-05-02 12:14:32 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.pfotenabdruck.info>
{'id': '167', 'url': 'https://www.pfotenabdruck.info', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 21 Apr 2020 09:28:05 GMT', 'tableLayout': False, 'ssl_name': 'pfotenabdruck.info', 'ssl_start': '20230609235959Z', 'ssl_expire': '20230609235959Z'}
2023-05-02 12:14:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://www.formativmedia.atrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:14:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://formativ-print.at/robots.txt>: DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:14:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.exelant.derobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:14:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://exelant.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:14:32 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.formativmedia.at> from <GET http://formativ-print.at>
2023-05-02 12:14:32 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.exelant.de> from <GET http://exelant.de>
2023-05-02 12:14:32 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.exelant.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.exelant.de'))])
2023-05-02 12:14:32 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.exelant.de/robots.txt> (referer: None)
2023-05-02 12:14:32 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:14:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.exelant.de> (referer: None)
2023-05-02 12:14:32 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.exelant.de>
{'id': '201', 'url': 'https://www.exelant.de', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:14:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.formativmedia.at/robots.txt> (referer: None)
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 81 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:14:32 [protego] DEBUG: Rule at line 109 without any user agent to enforce it on.
2023-05-02 12:14:33 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.formativmedia.at/> from <GET http://www.formativmedia.at>
2023-05-02 12:14:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.formativmedia.at/> (referer: None)
2023-05-02 12:14:34 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.formativmedia.at/>
{'id': '196', 'url': 'https://www.formativmedia.at/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'einfachwanda.at', 'ssl_start': '20230515034747Z', 'ssl_expire': '20230515034747Z'}
2023-05-02 12:14:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://www.mattner.orgrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:14:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://mattner.org/robots.txt>: DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:14:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.mattner.org> from <GET http://mattner.org>
2023-05-02 12:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.mattner.org/robots.txt> (referer: None)
2023-05-02 12:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.mattner.org> (referer: None)
2023-05-02 12:14:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.medizinrechtler.inforobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:14:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://grad-der-behinderung.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:14:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.medizinrechtler.info> from <GET http://grad-der-behinderung.de>
2023-05-02 12:14:35 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:35 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.mattner.org>
{'id': '245', 'url': 'http://www.mattner.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:14:35 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.medizinrechtler.info"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.medizinrechtler.info'))])
2023-05-02 12:14:35 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.medizinrechtler.info/robots.txt> (referer: None)
2023-05-02 12:14:35 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.medizinrechtler.info> (referer: None)
2023-05-02 12:14:36 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.medizinrechtler.info>
{'id': '234', 'url': 'https://www.medizinrechtler.info', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': '*.alfahosting-server.de', 'ssl_start': '20240221235959Z', 'ssl_expire': '20240221235959Z'}
2023-05-02 12:14:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://volki.pb.photographyrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:14:36 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://galerie-bauer.de/robots.txt>: DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:14:36 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://volki.pb.photography> from <GET http://galerie-bauer.de>
2023-05-02 12:14:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://volki.pb.photography/robots.txt> (referer: None)
2023-05-02 12:14:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://volki.pb.photography> (referer: None)
2023-05-02 12:14:37 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:14:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://volki.pb.photography>
{'id': '269', 'url': 'https://volki.pb.photography', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': '*.pb.photography', 'ssl_start': '20240222235959Z', 'ssl_expire': '20240222235959Z'}
2023-05-02 12:14:37 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:14:37 [scrapy.extensions.feedexport] INFO: Stored jl feed (314 items) in: baddata_results_20230502_121356.jl
2023-05-02 12:14:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 14,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 14,
 'downloader/request_bytes': 172572,
 'downloader/request_count': 761,
 'downloader/request_method_count/GET': 761,
 'downloader/response_bytes': 2763817,
 'downloader/response_count': 747,
 'downloader/response_status_count/200': 468,
 'downloader/response_status_count/301': 27,
 'downloader/response_status_count/302': 65,
 'downloader/response_status_count/404': 175,
 'downloader/response_status_count/500': 6,
 'downloader/response_status_count/503': 6,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 40.14977,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 14, 37, 110897),
 'httpcompression/response_bytes': 4868946,
 'httpcompression/response_count': 352,
 'item_scraped_count': 314,
 'log_count/DEBUG': 2103,
 'log_count/ERROR': 21,
 'log_count/INFO': 11,
 'log_count/WARNING': 345,
 'memusage/max': 66871296,
 'memusage/startup': 66871296,
 'response_received_count': 646,
 'retry/count': 13,
 'retry/max_reached': 13,
 'retry/reason_count/500 Internal Server Error': 3,
 'retry/reason_count/503 Service Unavailable': 3,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 7,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 7,
 'robotstxt/request_count': 338,
 'robotstxt/response_count': 331,
 'robotstxt/response_status_count/200': 151,
 'robotstxt/response_status_count/404': 175,
 'robotstxt/response_status_count/500': 3,
 'robotstxt/response_status_count/503': 2,
 'scheduler/dequeued': 367,
 'scheduler/dequeued/memory': 367,
 'scheduler/enqueued': 367,
 'scheduler/enqueued/memory': 367,
 'start_time': datetime.datetime(2023, 5, 2, 12, 13, 56, 961127)}
2023-05-02 12:14:37 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:16:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:16:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:16:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:16:16 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:16:16 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:16:16 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:16:16 [scrapy.extensions.telnet] INFO: Telnet Password: 9ee013f59d85bdd7
2023-05-02 12:16:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:16:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:16:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:16:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:16:16 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:16:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:16:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:16:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:16:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/> (referer: None)
2023-05-02 12:16:17 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:16:17 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.info/>
{'id': '0', 'url': 'http://wasistopc.info/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:16:17 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:16:17 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_121615.jl
2023-05-02 12:16:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 438,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 628,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.204127,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 16, 17, 40605),
 'httpcompression/response_bytes': 87,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66711552,
 'memusage/startup': 66711552,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 12, 16, 16, 836478)}
2023-05-02 12:16:17 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:17:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:17:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:17:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:17:10 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:17:10 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:17:10 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:17:10 [scrapy.extensions.telnet] INFO: Telnet Password: eb67c1af1eb2149e
2023-05-02 12:17:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:17:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:17:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:17:10 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:17:10 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:17:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:17:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:17:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:17:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/> (referer: None)
2023-05-02 12:17:11 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:17:11 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.info/>
{'id': '0', 'url': 'http://wasistopc.info/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:17:11 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:17:11 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_121710.jl
2023-05-02 12:17:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 438,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 628,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.193894,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 17, 11, 115172),
 'httpcompression/response_bytes': 87,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66744320,
 'memusage/startup': 66744320,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 12, 17, 10, 921278)}
2023-05-02 12:17:11 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:17:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:17:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:17:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:17:18 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:17:18 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:17:18 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:17:18 [scrapy.extensions.telnet] INFO: Telnet Password: 18b85da1c30e307b
2023-05-02 12:17:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:17:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:17:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:17:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:17:19 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:17:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:17:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:17:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:17:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/> (referer: None)
2023-05-02 12:17:19 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:17:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.info/>
{'id': '0', 'url': 'http://wasistopc.info/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:17:19 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:17:19 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_121718.jl
2023-05-02 12:17:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 438,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 628,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.197431,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 17, 19, 285032),
 'httpcompression/response_bytes': 87,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66928640,
 'memusage/startup': 66928640,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 12, 17, 19, 87601)}
2023-05-02 12:17:19 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:50:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:50:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:50:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:50:30 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:50:30 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:50:30 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:50:30 [scrapy.extensions.telnet] INFO: Telnet Password: 4a090679168adb69
2023-05-02 12:50:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:50:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:50:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:50:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:50:30 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:50:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:50:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:50:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:50:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/> (referer: None)
2023-05-02 12:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wasistopc.info/> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body()
TypeError: 'bytes' object is not callable
2023-05-02 12:50:31 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:50:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 438,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 569,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.199643,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 50, 31, 166098),
 'httpcompression/response_bytes': 87,
 'httpcompression/response_count': 1,
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'memusage/max': 66564096,
 'memusage/startup': 66564096,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2023, 5, 2, 12, 50, 30, 966455)}
2023-05-02 12:50:31 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:51:07 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:51:07 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:51:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:51:07 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:51:07 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:51:07 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:51:07 [scrapy.extensions.telnet] INFO: Telnet Password: 456039fc6de745e9
2023-05-02 12:51:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:51:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:51:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:51:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:51:07 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:51:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:51:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:51:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:51:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/> (referer: None)
2023-05-02 12:51:07 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wasistopc.info/> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.text
AttributeError: 'bytes' object has no attribute 'text'
2023-05-02 12:51:07 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:51:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 438,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 569,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.214643,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 51, 7, 801725),
 'httpcompression/response_bytes': 87,
 'httpcompression/response_count': 1,
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'memusage/max': 66711552,
 'memusage/startup': 66711552,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2023, 5, 2, 12, 51, 7, 587082)}
2023-05-02 12:51:07 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:54:01 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:54:01 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:54:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:54:01 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:54:01 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:54:01 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:54:01 [scrapy.extensions.telnet] INFO: Telnet Password: a3ab3aeb70407bf0
2023-05-02 12:54:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:54:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:54:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:54:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:54:01 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:54:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:54:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:54:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:54:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/> (referer: None)
2023-05-02 12:54:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.info/>
{'id': '0', 'url': 'http://wasistopc.info/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:01 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:54:01 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_125400.jl
2023-05-02 12:54:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 438,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 628,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.259438,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 54, 1, 937925),
 'httpcompression/response_bytes': 87,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66527232,
 'memusage/startup': 66527232,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 12, 54, 1, 678487)}
2023-05-02 12:54:01 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:54:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:54:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:54:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:54:45 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:54:45 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:54:45 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:54:45 [scrapy.extensions.telnet] INFO: Telnet Password: a29804d42d4a49c9
2023-05-02 12:54:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:54:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:54:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:54:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:54:45 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:54:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:54:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://lisaglauer.com/robots.txt> from <GET http://lisaglauer.de/robots.txt>
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfs-europe.de/robots.txt> from <GET http://pfs-europe.de/robots.txt>
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.be/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mgvliederkranz-asbach.de/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lahres.com/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sprachlos-ev-beratung.de/robots.txt> from <GET http://sprachlos-ev-beratung.de/robots.txt>
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hausegger.net/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://kaffeeundservice.at/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-wohnzimmer.de/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.be> (referer: None)
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mgvliederkranz-asbach.de> (referer: None)
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lahres.com> (referer: None)
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://docs.moodle.org:443/robots.txt> from <GET http://docs.moodle.org/robots.txt>
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hausegger.net> (referer: None)
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://kaffeeundservice.at/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://kaffeeundservice.at/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mixable.media/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "pfs-europe.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'pfs-europe.de'))])
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mind-holiday.com/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:54:45 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET http://lifttaxi.com> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfs-europe.de/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://efwe-art.at/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mixable.media> (referer: None)
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://docs.moodle.org:443/2x/pl/Flash> from <GET http://docs.moodle.org/2x/pl/Flash>
2023-05-02 12:54:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://steigertaxi.de/robots.txt> (referer: None)
2023-05-02 12:54:45 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://kaliner-yoga.de/robots.txt> from <GET http://kaliner-yoga.de/robots.txt>
2023-05-02 12:54:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfs-europe.de/> from <GET http://pfs-europe.de>
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.be>
{'id': '7', 'url': 'http://biotikon.be', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://bayern-immobilie.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lifttaxi.com/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mgvliederkranz-asbach.de>
{'id': '11', 'url': 'http://mgvliederkranz-asbach.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 28 Jan 2009 10:53:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://photos.hr-photo.derobots.txt> from <GET http://hr-photo.de/robots.txt>
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-teamblog.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://karo-stimme.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfs-europe.de/> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-wohnzimmer.de> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gbv-grosskarolinenfeld.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lahres.com>
{'id': '15', 'url': 'http://lahres.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 25 Dec 2019 08:39:49 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://coaching-im-alltag.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://efwe-art.at> (referer: None)
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hausegger.net>
{'id': '12', 'url': 'http://hausegger.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 12 Sep 2012 11:18:42 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mind-holiday.com> (referer: None)
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mixable.media>
{'id': '17', 'url': 'http://mixable.media', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-teamblog.de> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://steigertaxi.de> (referer: None)
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://pfs-europe.de/>
{'id': '3', 'url': 'https://pfs-europe.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 03 Aug 2019 10:52:54 GMT', 'tableLayout': True, 'ssl_name': 'fun4-u.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bayern-immobilie.de> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lifttaxi.com/> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.moodle.org:443/2x/pl/Flash> (referer: None)
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-wohnzimmer.de>
{'id': '13', 'url': 'http://massivholz-wohnzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://karo-stimme.de> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gbv-grosskarolinenfeld.de> (referer: None)
2023-05-02 12:54:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://efwe-art.at> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:54:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://mind-holiday.com> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://billardcafe-suedpark.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.dienstleistung-solar.de/robots.txt> from <GET http://dienstleistung-solar.de/robots.txt>
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon-teamblog.de>
{'id': '23', 'url': 'http://biotikon-teamblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://steigertaxi.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coaching-im-alltag.de> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.info> (referer: None)
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET http://www.purpur.eatbu.com/?lang=de> from <GET http://billardcafe-suedpark.de>
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.com/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaukus.org/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://bayern-immobilie.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kilthau.tech/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://seelenfeder.at/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://lifttaxi.com/> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://docs.moodle.org:443/2x/pl/Flash>
{'id': '0', 'url': 'https://docs.moodle.org:443/2x/pl/Flash', 'status': 200, 'title': 'Flash – MoodleDocs', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': True, 'lastModified': 'Mon, 24 Apr 2023 19:09:27 GMT', 'tableLayout': False, 'ssl_name': 'sni.cloudflaressl.com', 'ssl_start': '20240501235959Z', 'ssl_expire': '20240501235959Z'}
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.com> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.purpur.eatbu.com/robots.txt> (referer: None)
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET http://kaukus.org/index2.html> from <GET http://kaukus.org>
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 463 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 469 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 472 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 507 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 513 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://byzickl.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://seelenfeder.at> (referer: None)
2023-05-02 12:54:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://karo-stimme.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:54:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://gbv-grosskarolinenfeld.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '25', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://coaching-im-alltag.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.info>
{'id': '31', 'url': 'http://wasistopc.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaffeeundservice.at> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.purpur.eatbu.com/?lang=de> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kaukus.org/index2.html> (referer: None)
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.wassersportcenter-heiligenhafen.de/> from <GET http://wassersportcenter-heiligenhafen.de>
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kilthau.tech> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://byzickl.de> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gartengestaltung-brandner.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 781: invalid start byte
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 1 times): 503 Service Unavailable
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativ-opc.com>
{'id': '33', 'url': 'http://3nativ-opc.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essen-auf-raedern-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://seelenfeder.at> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa9 in position 43799: invalid start byte
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.sprachlos-ev-beratung.de/robots.txt> from <GET https://sprachlos-ev-beratung.de/robots.txt>
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://illger.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://drey.info/robots.txt> from <GET http://drey.info/robots.txt>
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> from <GET http://www.wassersportcenter-heiligenhafen.de/robots.txt>
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dobbrunz.com/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://illger.de> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-star.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://magnonics.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cellhomoeostasis.com/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 195 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 199 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 230 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 231 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 257 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 262 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 285 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 303 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 324 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 339 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 460 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 463 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 464 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 465 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 467 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 469 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 472 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 505 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 506 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 507 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 508 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 511 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 512 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 513 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 548 without any user agent to enforce it on.
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://diekicktipper.de/robots.txt> from <GET http://diekicktipper.de/robots.txt>
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dobbrunz.com> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://magnonics.de> (referer: None)
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cellhomoeostasis.com> (referer: None)
2023-05-02 12:54:46 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kaffeeundservice.at>
{'id': '14', 'url': 'http://kaffeeundservice.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.purpur.eatbu.com/?lang=de>
{'id': '30', 'url': 'http://www.purpur.eatbu.com/?lang=de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kaukus.org/index2.html>
{'id': '32', 'url': 'http://kaukus.org/index2.html', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2009 20:14:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kilthau.tech>
{'id': '34', 'url': 'http://kilthau.tech', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://byzickl.de>
{'id': '36', 'url': 'http://byzickl.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://lisaglauer.com/> from <GET http://lisaglauer.de>
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.wassersportcenter-heiligenhafen.de/> from <GET http://www.wassersportcenter-heiligenhafen.de/>
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ts-it-service.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 2 times): 503 Service Unavailable
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:54:46 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hgwimmobilien.de/robots.txt> (referer: None)
2023-05-02 12:54:46 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:54:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essen-auf-raedern-willebadessen.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schoolscout24.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gartengestaltung-brandner.de> (referer: None)
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://illger.de>
{'id': '37', 'url': 'http://illger.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 07 Nov 2011 11:18:19 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dobbrunz.com>
{'id': '40', 'url': 'http://dobbrunz.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 03 Jul 2016 18:53:33 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://magnonics.de>
{'id': '42', 'url': 'http://magnonics.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 26 Aug 2009 08:33:56 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cellhomoeostasis.com>
{'id': '41', 'url': 'http://cellhomoeostasis.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://drey.info/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.dienstleistung-solar.de/> from <GET http://dienstleistung-solar.de>
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.wassersportcenter-heiligenhafen.de/> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schoolscout24.de> (referer: None)
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://essen-auf-raedern-willebadessen.de>
{'id': '27', 'url': 'http://essen-auf-raedern-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://drey.info/> from <GET http://drey.info>
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gesundheitsforum-norderstedt.com/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://malttec.net/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dj-sb.com/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hgwimmobilien.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ts-it-service.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://gartengestaltung-brandner.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 781: invalid start byte
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.wassersportcenter-heiligenhafen.de/>
{'id': '16', 'url': 'https://www.wassersportcenter-heiligenhafen.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'wassersportcenter-heiligenhafen.de', 'ssl_start': '20230719235959Z', 'ssl_expire': '20230719235959Z'}
2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schoolscout24.de>
{'id': '46', 'url': 'http://schoolscout24.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 03 May 2011 19:33:17 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-flurmoebel.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kanzlei-sauerwein.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 1 times): 503 Service Unavailable
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://drey.info/> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://diekicktipper.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET https://teamtrzweb.de> from <GET http://malttec.net>
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fmz-frankershausen.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gesundheitsforum-norderstedt.com> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dj-sb.com> (referer: None)
2023-05-02 12:54:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hgwimmobilien.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ts-it-service.de>
{'id': '45', 'url': 'http://ts-it-service.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://diekicktipper.de/> from <GET http://diekicktipper.de>
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sprachlos-ev-beratung.de/> from <GET http://sprachlos-ev-beratung.de>
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kanzlei-sauerwein.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yogashop-paderborn.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-flurmoebel.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.net/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lisaglauer.com/> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-bueromoebel.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.net> (referer: None)
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://drey.info/>
{'id': '38', 'url': 'https://drey.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 17 Mar 2014 08:12:11 GMT', 'tableLayout': False, 'ssl_name': 'drey.info', 'ssl_start': '20230705235959Z', 'ssl_expire': '20230705235959Z'}
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://goting-kliff53.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-bueromoebel.de> (referer: None)
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/robots.txt> (failed 2 times): 503 Service Unavailable
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 121 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:54:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://gesundheitsforum-norderstedt.com> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 5688: invalid start byte
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dj-sb.com>
{'id': '47', 'url': 'http://dj-sb.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 28 Dec 2014 13:42:08 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kanzlei-sauerwein.de>
{'id': '51', 'url': 'http://kanzlei-sauerwein.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 03 Jan 2010 02:56:47 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://teamtrzweb.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-online-fragen-ktb.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://diekicktipper.de/> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wetzel.work/robots.txt> (referer: None)
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 213 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 214 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 216 without any user agent to enforce it on.
2023-05-02 12:54:47 [protego] DEBUG: Rule at line 234 without any user agent to enforce it on.
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://goting-kliff53.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-pilavas.com/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://buehnen-taxi.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenheim-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-flurmoebel.de>
{'id': '50', 'url': 'http://massivholz-flurmoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lisaglauer.com/>
{'id': '2', 'url': 'http://lisaglauer.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.net>
{'id': '55', 'url': 'http://biotikon.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.dienstleistung-solar.de/> (failed 1 times): 503 Service Unavailable
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wetzel.work> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-online-fragen-ktb.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-bueromoebel.de>
{'id': '59', 'url': 'http://massivholz-bueromoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://diekicktipper.de/>
{'id': '44', 'url': 'https://diekicktipper.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'diekicktipper.de', 'ssl_start': '20230527235959Z', 'ssl_expire': '20230527235959Z'}
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-star.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bce-clan.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.sprachlos-ev-beratung.de/> from <GET https://sprachlos-ev-beratung.de/>
2023-05-02 12:54:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://goting-kliff53.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf6 in position 145: invalid start byte
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-pilavas.com> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.info/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://bavarian-starlights.com/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://buehnen-taxi.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://xn--edelstahlscheckkartenhlle-0wc.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bauelemente-riede.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wetzel.work>
{'id': '56', 'url': 'http://wetzel.work', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 14 Aug 2017 17:23:07 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzte-online-fragen-ktb.de>
{'id': '61', 'url': 'http://xn--rzte-online-fragen-ktb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.info> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenheim-willebadessen.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bauelemente-riede.de> (referer: None)
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.dienstleistung-solar.de/> (failed 2 times): 503 Service Unavailable
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://www.dienstleistung-solar.de/> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-makler.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bce-clan.de> (referer: None)
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-star.de>
{'id': '39', 'url': 'http://yoga-star.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ciber-man.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://da365.de/robots.txt> from <GET http://da365.de/robots.txt>
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://buehnentaxi.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fmz-frankershausen.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://ouzo-pilavas.com> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:54:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://buehnen-taxi.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--edelstahlscheckkartenhlle-0wc.de> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ciber-man.de> (referer: None)
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://was-ist-opc.info>
{'id': '65', 'url': 'http://was-ist-opc.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/robots.txt> from <GET http://cyberfiber.me/robots.txt>
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://xn--holzmbel-experte-qwb.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://altenheim-willebadessen.de>
{'id': '58', 'url': 'http://altenheim-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bauelemente-riede.de>
{'id': '69', 'url': 'http://bauelemente-riede.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 16 May 2018 20:42:31 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://style-and-smile.com/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://schluesselvereinzler.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-markt.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 776: invalid start byte
2023-05-02 12:54:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mobile-mietstation.de/robots.txt> (referer: None)
2023-05-02 12:54:47 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 774: invalid start byte
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-makler.de> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://huckbros.de/robots.txt> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/robots.txt> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://buehnentaxi.de> (referer: None)
2023-05-02 12:54:48 [dubdev] ERROR: HttpError on https://www.dienstleistung-solar.de/
2023-05-02 12:54:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bce-clan.de>
{'id': '64', 'url': 'http://bce-clan.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--holzmbel-experte-qwb.de> (referer: None)
2023-05-02 12:54:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fmz-frankershausen.de>
{'id': '20', 'url': 'http://fmz-frankershausen.de', 'status': 200, 'title': 'Fanfaren- und Musikzug Frankershausen – Freiwillige Feuerwehr Frankershausen 1956 e.V.', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--edelstahlscheckkartenhlle-0wc.de>
{'id': '67', 'url': 'http://xn--edelstahlscheckkartenhlle-0wc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ciber-man.de>
{'id': '70', 'url': 'http://ciber-man.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 20 Jan 2013 06:17:41 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://huckbros.de> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://openlate.de/robots.txt> (referer: None)
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:54:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kaliner-yoga.de/robots.txt> from <GET https://kaliner-yoga.de/robots.txt>
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schluesselvereinzler.de> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yogashop-paderborn.de> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-markt.de> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mobile-mietstation.de> (referer: None)
2023-05-02 12:54:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.bavarian-starlights.com/> from <GET http://bavarian-starlights.com>
2023-05-02 12:54:48 [scrapy.core.scraper] ERROR: Spider error processing <GET http://arbeitsbuehnen-makler.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:54:48 [scrapy.core.scraper] ERROR: Spider error processing <GET http://buehnentaxi.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:54:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--holzmbel-experte-qwb.de>
{'id': '74', 'url': 'http://xn--holzmbel-experte-qwb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-jugendzimmer.de/robots.txt> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-qigong-master.de/robots.txt> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://teamtrzweb.de> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ignaz-schwarzbach.eu/robots.txt> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila.info/robots.txt> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://www.bavarian-starlights.com/robots.txt> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://marco-k.com/robots.txt> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gzlw.de/robots.txt> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-qigong-master.de> (referer: None)
2023-05-02 12:54:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://huckbros.de>
{'id': '78', 'url': 'http://huckbros.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Feb 2023 08:29:22 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gzlw.de> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://no-flush-niacin.com/robots.txt> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-jugendzimmer.de> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ignaz-schwarzbach.eu> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila.info> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://marco-k.com> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.sprachlos-ev-beratung.de/> (referer: None)
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kindz.de/robots.txt> (referer: None)
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:54:48 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://da365.de/robots.txt> (referer: None)
2023-05-02 12:54:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schluesselvereinzler.de>
{'id': '76', 'url': 'http://schluesselvereinzler.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yogashop-paderborn.de>
{'id': '52', 'url': 'http://yogashop-paderborn.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:48 [scrapy.core.scraper] ERROR: Spider error processing <GET http://arbeitsbuehnen-markt.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 776: invalid start byte
2023-05-02 12:54:48 [scrapy.core.scraper] ERROR: Spider error processing <GET http://mobile-mietstation.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 774: invalid start byte
2023-05-02 12:54:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://no-flush-niacin.com> (referer: None)
2023-05-02 12:54:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://teamtrzweb.de>
{'id': '49', 'url': 'https://teamtrzweb.de', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'biergartenteam.de', 'ssl_start': '20230609051948Z', 'ssl_expire': '20230609051948Z'}
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://openlate.de> (referer: None)
2023-05-02 12:54:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://da365.de/> from <GET http://da365.de>
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.bavarian-starlights.com/> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.net/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lift-taxi.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kindz.de> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-kinderzimmer.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taichichuan-qigong-master.de>
{'id': '79', 'url': 'http://taichichuan-qigong-master.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gzlw.de>
{'id': '85', 'url': 'http://gzlw.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 28 Jan 2009 09:42:45 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-jugendzimmer.de>
{'id': '80', 'url': 'http://massivholz-jugendzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ignaz-schwarzbach.eu>
{'id': '81', 'url': 'http://ignaz-schwarzbach.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 04 Dec 2022 16:28:01 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila.info>
{'id': '83', 'url': 'http://casamobila.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://marco-k.com>
{'id': '82', 'url': 'http://marco-k.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 23 Jul 2022 22:42:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.sprachlos-ev-beratung.de/>
{'id': '10', 'url': 'https://www.sprachlos-ev-beratung.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'sprachlos-ev-beratung.de', 'ssl_start': '20230711235959Z', 'ssl_expire': '20230711235959Z'}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://no-flush-niacin.com>
{'id': '86', 'url': 'http://no-flush-niacin.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-kinderzimmer.de> (referer: None)
2023-05-02 12:54:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://kaliner-yoga.de/> from <GET http://kaliner-yoga.de>
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ganzheitlichesheilen.eu/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobila24.com/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://boncomputa.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:54:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://openlate.de>
{'id': '54', 'url': 'http://openlate.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.bavarian-starlights.com/>
{'id': '66', 'url': 'http://www.bavarian-starlights.com/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kindz.de>
{'id': '26', 'url': 'http://kindz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmedmichalzik.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmichalzik.eu/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ganzheitlichesheilen.eu> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobila24.com> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.net> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lift-taxi.de> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hgwimmobilien.org/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://da365.de/> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://boncomputa.de> (referer: None)
2023-05-02 12:54:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-kinderzimmer.de>
{'id': '87', 'url': 'http://massivholz-kinderzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rztefragenonline-unb.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmedmichalzik.de> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://drmichalzik.eu> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nows.tk/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.org/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rztefragenonline-unb.de> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://style-and-smile.com> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nows.tk> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://westside-linedance.com/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://edelstahlscheckkartenhuelle.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.org> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wwb1.net/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-alexander-michalzik.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-friedrichwipfel.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://doc-blog.org/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tibetanische-klangschalen.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westside-linedance.com> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://strukturplatten.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://wvm-branchenportal.com/robots.txt> from <GET http://wvm-branchenportal.com/robots.txt>
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wwb1.net> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hgwimmobilien.org> (referer: None)
2023-05-02 12:54:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ganzheitlichesheilen.eu>
{'id': '90', 'url': 'http://ganzheitlichesheilen.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-nektar.com/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobila24.com>
{'id': '89', 'url': 'http://casademobila24.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://govido.net> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:54:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://lift-taxi.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://da365.de/>
{'id': '62', 'url': 'https://da365.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'da365.de', 'ssl_start': '20230708235959Z', 'ssl_expire': '20230708235959Z'}
2023-05-02 12:54:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://boncomputa.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:54:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://drmedmichalzik.de>
{'id': '91', 'url': 'http://drmedmichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://drmichalzik.eu>
{'id': '93', 'url': 'http://drmichalzik.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-alexander-michalzik.de> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobilashop.eu/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://doc-blog.org> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taichichuan-friedrichwipfel.de> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://sulden-apresski.com/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://strukturplatten.de> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://edelstahlscheckkartenhuelle.de> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://impax.de/robots.txt> (referer: None)
2023-05-02 12:54:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kaliner-yoga.de/> from <GET https://kaliner-yoga.de/>
2023-05-02 12:54:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rztefragenonline-unb.de>
{'id': '95', 'url': 'http://xn--rztefragenonline-unb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://style-and-smile.com>
{'id': '77', 'url': 'http://style-and-smile.com', 'status': 200, 'title': 'WordPress 5.3.0 – Eine weitere WordPress-Website', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tibetanische-klangschalen.de> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-nektar.com> (referer: None)
2023-05-02 12:54:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nows.tk>
{'id': '94', 'url': 'http://nows.tk', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 06 Jun 2018 13:57:15 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobilashop.eu> (referer: None)
2023-05-02 12:54:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sulden-apresski.com> (referer: None)
2023-05-02 12:54:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.org>
{'id': '97', 'url': 'http://biotikon.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://westside-linedance.com>
{'id': '99', 'url': 'http://westside-linedance.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 29 Jan 2020 19:02:17 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wwb1.net>
{'id': '102', 'url': 'http://wwb1.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 30 Sep 2019 13:35:47 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hgwimmobilien.org> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://impax.de> (referer: None)
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dr-alexander-michalzik.de>
{'id': '105', 'url': 'http://dr-alexander-michalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://doc-blog.org>
{'id': '104', 'url': 'http://doc-blog.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taichichuan-friedrichwipfel.de>
{'id': '103', 'url': 'http://taichichuan-friedrichwipfel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://strukturplatten.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 734: invalid continuation byte
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://edelstahlscheckkartenhuelle.de>
{'id': '101', 'url': 'http://edelstahlscheckkartenhuelle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tibetanische-klangschalen.de>
{'id': '107', 'url': 'http://tibetanische-klangschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://ouzo-nektar.com> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hilcura.com/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobilashop.eu>
{'id': '110', 'url': 'http://casademobilashop.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://sulden-apresski.com>
{'id': '111', 'url': 'http://sulden-apresski.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': True, 'lastModified': 'Tue, 13 Mar 2018 16:25:36 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wvm-branchenportal.com/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://marcodesigner.com/robots.txt> from <GET http://marcodesigner.com/robots.txt>
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ateliereichner.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nevaton.eu/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mastershaft.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://impax.de>
{'id': '100', 'url': 'http://impax.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://wvm-branchenportal.com/> from <GET http://wvm-branchenportal.com>
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-esszimmer.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ateliereichner.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tibetanischeklagschalen.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wvm-branchenportal.com/> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://carcal.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-schlafzimmer.com/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-esszimmer.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "marcodesigner.com"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'marcodesigner.com'))])
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.info/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://heueu.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-schlafzimmer.com> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://heueu.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.org/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://djmirco.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mastershaft.de> (referer: None)
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ateliereichner.de>
{'id': '114', 'url': 'http://ateliereichner.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://el-toque-latino.com/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.org> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dr-michalzik.at/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://wahnfriet.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://wvm-branchenportal.com/>
{'id': '108', 'url': 'https://wvm-branchenportal.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 14 Oct 2020 22:24:24 GMT', 'tableLayout': False, 'ssl_name': 'wvm-branchenportal.com', 'ssl_start': '20240307235959Z', 'ssl_expire': '20240307235959Z'}
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nowikow.org/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tibetanischeklagschalen.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://el-toque-latino.com> (referer: None)
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-esszimmer.de>
{'id': '118', 'url': 'http://massivholz-esszimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.info/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://carcal.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://djmirco.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://biotikon.es/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-schlafzimmer.com>
{'id': '122', 'url': 'http://massivholz-schlafzimmer.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://heueu.de>
{'id': '125', 'url': 'http://heueu.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 12 Sep 2009 08:57:13 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://mastershaft.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://marcodesigner.com/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.info> (referer: None)
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-alpha-ketoglutarat.org>
{'id': '126', 'url': 'http://arginin-alpha-ketoglutarat.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.info> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dr-michalzik.at> (referer: None)
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tibetanischeklagschalen.de>
{'id': '121', 'url': 'http://tibetanischeklagschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://el-toque-latino.com>
{'id': '129', 'url': 'http://el-toque-latino.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 07 Sep 2007 23:41:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-online-fragen.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://marcodesigner.com/> from <GET http://marcodesigner.com>
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexander-michalzik.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wahnfriet.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://die-goldene-cd.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nowikow.org> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nevaton.eu> (referer: None)
2023-05-02 12:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://carcal.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 1476: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.es> (referer: None)
2023-05-02 12:54:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hilcura.de/> from <GET http://hilcura.com>
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://aufklebershop-geislingen.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://djmirco.de>
{'id': '127', 'url': 'http://djmirco.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-online-fragen.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronamitarbeiterschutz.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexander-michalzik.de> (referer: None)
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.info>
{'id': '130', 'url': 'http://biotikon.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kaliner-yoga.de/> (referer: None)
2023-05-02 12:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://govido.info> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lifttaxi.eu/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://aufklebershop-geislingen.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dr-michalzik.at> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> from <GET http://xn--wohlfhlwerkstatt-nzb.de/robots.txt>
2023-05-02 12:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wahnfriet.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 765: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://springblade.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://nowikow.org> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nevaton.eu>
{'id': '98', 'url': 'http://nevaton.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinoline-quinone.org/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://die-goldene-cd.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://edv-frenzel.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.eu/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 762: invalid start byte
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://marcodesigner.com/> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronamitarbeiterschutz.de> (referer: None)
2023-05-02 12:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://biotikon.es> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:54:51 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arzt-online-fragen.de>
{'id': '134', 'url': 'http://arzt-online-fragen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://alexander-michalzik.de>
{'id': '133', 'url': 'http://alexander-michalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.kaliner-yoga.de/>
{'id': '21', 'url': 'https://www.kaliner-yoga.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'kaliner-yoga.de', 'ssl_start': '20230615235959Z', 'ssl_expire': '20230615235959Z'}
2023-05-02 12:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://aufklebershop-geislingen.de>
{'id': '135', 'url': 'http://aufklebershop-geislingen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Jul 2010 08:21:56 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tribulus-saponine.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://delta-mb.de/robots.txt> from <GET http://delta-mb.de/robots.txt>
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-dosierung.de/robots.txt> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinoline-quinone.org> (referer: None)
2023-05-02 12:54:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lifttaxi.eu> (referer: None)
2023-05-02 12:54:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://die-goldene-cd.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/robots.txt?doing_wp_cron=1683032092.0083971023559570312500> from <GET https://cyberfiber.me/robots.txt>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-dosierung.de> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tribulus-saponine.de> (referer: None)
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://marcodesigner.com/>
{'id': '115', 'url': 'https://marcodesigner.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'die-rohrreinigung.com', 'ssl_start': '20210714235959Z', 'ssl_expire': '20210714235959Z'}
2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coronamitarbeiterschutz.de>
{'id': '137', 'url': 'http://coronamitarbeiterschutz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> (referer: None)
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://springblade.de> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://patcy.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-blog.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://edv-frenzel.de> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.health/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://huckbros.com/robots.txt> from <GET http://huckbros.com/robots.txt>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://formergy.biz/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pyrroloquinoline-quinone.org>
{'id': '140', 'url': 'http://pyrroloquinoline-quinone.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://lifttaxi.eu> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 764: invalid start byte
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> from <GET http://xn--wohlfhlwerkstatt-nzb.de>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon-blog.de> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.health> (referer: None)
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://resveratrol-dosierung.de>
{'id': '145', 'url': 'http://resveratrol-dosierung.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tribulus-saponine.de>
{'id': '142', 'url': 'http://tribulus-saponine.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://pixxpress.com/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://crazybulls.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fusionstreet.com/robots.txt> (referer: None)
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://springblade.de>
{'id': '139', 'url': 'http://springblade.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfotenabdruck.info/robots.txt> from <GET http://pfotenabdruck.info/robots.txt>
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://edv-frenzel.de>
{'id': '141', 'url': 'http://edv-frenzel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://huckbros.com/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pixxpress.com> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://patcy.de> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://crazybulls.de> (referer: None)
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon-blog.de>
{'id': '149', 'url': 'http://biotikon-blog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yuriol.com/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://huckbros.com/> from <GET http://huckbros.com>
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.health>
{'id': '150', 'url': 'http://biotikon.health', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://formergy.biz> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.com/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://pfotenabdruck.info/robots.txt> (referer: None)
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pixxpress.com>
{'id': '155', 'url': 'http://pixxpress.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 26 Aug 2011 20:30:29 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://rustikal-lecker.com/robots.txt> from <GET http://rustikal-lecker.com/robots.txt>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://huckbros.com/> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.com> (referer: None)
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://pfotenabdruck.info/> from <GET http://pfotenabdruck.info>
2023-05-02 12:54:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://patcy.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb4 in position 748: invalid start byte
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://crazybulls.de>
{'id': '153', 'url': 'http://crazybulls.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 07 Dec 2022 21:55:50 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.xn--wohlfhlwerkstatt-nzb.de/> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fusionstreet.com> (referer: None)
2023-05-02 12:54:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://formergy.biz> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://klappstuhl51.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pfotenabdruck.info/> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pflegeplatz-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinolinequinone.org/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dirkmeineke.de/robots.txt> from <GET http://dirkmeineke.de/robots.txt>
2023-05-02 12:54:52 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "rustikal-lecker.com"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'rustikal-lecker.com'))])
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://huckbros.com/>
{'id': '151', 'url': 'https://huckbros.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 14 Feb 2023 08:29:22 GMT', 'tableLayout': False, 'ssl_name': 'huckbros.com', 'ssl_start': '20230715235959Z', 'ssl_expire': '20230715235959Z'}
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hilcura.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pyrroloquinolinequinone.org> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://abandonedzone.com/robots.txt> (referer: None)
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://delta-mb.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativopc.com>
{'id': '160', 'url': 'http://3nativopc.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnendiscount.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www6.xn--wohlfhlwerkstatt-nzb.de/>
{'id': '138', 'url': 'http://www6.xn--wohlfhlwerkstatt-nzb.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fusionstreet.com>
{'id': '154', 'url': 'http://fusionstreet.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://pfotenabdruck.info/>
{'id': '156', 'url': 'https://pfotenabdruck.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 21 Apr 2020 09:28:05 GMT', 'tableLayout': False, 'ssl_name': 'pfotenabdruck.info', 'ssl_start': '20230609235959Z', 'ssl_expire': '20230609235959Z'}
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://delta-mb.de/> from <GET http://delta-mb.de>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://klappstuhl51.de> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://abandonedzone.com> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://cyberfiber.me/robots.txt?doing_wp_cron=1683032092.0083971023559570312500> (referer: None)
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://efggp.de/robots.txt> from <GET http://efggp.de/robots.txt>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://manwe.net/robots.txt> (referer: None)
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pyrroloquinolinequinone.org>
{'id': '164', 'url': 'http://pyrroloquinolinequinone.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://cyberfiber.me/> from <GET http://cyberfiber.me>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pflegeplatz-willebadessen.de> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://rustikal-lecker.com/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.pfotenabdruck.inforobots.txt> from <GET http://pfoten-abdruck.de/robots.txt>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://dirkmeineke.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://jagdgruppe-bad-aibling.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobilashop.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://rustikal-lecker.com/> from <GET http://rustikal-lecker.com>
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dirkmeineke.de/> from <GET http://dirkmeineke.de>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://jagdgruppe-bad-aibling.de> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobilashop.de> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnendiscount.de> (referer: None)
2023-05-02 12:54:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://klappstuhl51.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 804: invalid continuation byte
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://abandonedzone.com>
{'id': '146', 'url': 'http://abandonedzone.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://go-paps.com/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holistic-medicine.ch/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dirkmeineke.de/> (referer: None)
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://yuriol.com/> from <GET http://yuriol.com>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://streamify.tv/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://go-paps.com> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holistic-medicine.ch> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thomasludwig.net/robots.txt> (referer: None)
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:54:52 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://discounterticket.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://pflegeplatz-willebadessen.de>
{'id': '158', 'url': 'http://pflegeplatz-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenpflege-willebadessen.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://manwe.net> (referer: None)
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://rustikal-lecker.com/> (referer: None)
2023-05-02 12:54:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://jagdgruppe-bad-aibling.de>
{'id': '168', 'url': 'http://jagdgruppe-bad-aibling.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 27 Apr 2023 12:47:45 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobilashop.de>
{'id': '169', 'url': 'http://casamobilashop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://arbeitsbuehnendiscount.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://gukrause.de/robots.txt> (referer: None)
2023-05-02 12:54:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/robots.txt> from <GET http://sememo.de/robots.txt>
2023-05-02 12:54:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://victoria-einrichtungen.eu/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://delta-mb.de/> (referer: None)
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dirkmeineke.de/>
{'id': '163', 'url': 'https://dirkmeineke.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 13 Mar 2023 20:03:35 GMT', 'tableLayout': True, 'ssl_name': 'dirkmeineke.de', 'ssl_start': '20230611205738Z', 'ssl_expire': '20230611205738Z'}
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://discounterticket.de> (referer: None)
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://go-paps.com>
{'id': '172', 'url': 'http://go-paps.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 14 Apr 2022 14:49:32 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://holistic-medicine.ch>
{'id': '171', 'url': 'http://holistic-medicine.ch', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://manwe.net>
{'id': '157', 'url': 'http://manwe.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://victoria-einrichtungen.eu> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://altenpflege-willebadessen.de> (referer: None)
2023-05-02 12:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://rustikal-lecker.com/> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://efggp.de/robots.txt> (referer: None)
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://delta-mb.de/>
{'id': '144', 'url': 'https://delta-mb.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'ascon-systems.de', 'ssl_start': '20230706235959Z', 'ssl_expire': '20230706235959Z'}
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thomasludwig.net> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wikistudien.org/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-nebenwirkungen.info/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gukrause.de> (referer: None)
2023-05-02 12:54:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://efggp.de/> from <GET http://efggp.de>
2023-05-02 12:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://discounterticket.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 772: invalid start byte
2023-05-02 12:54:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hotcoconut.eu/robots.txt> from <GET http://hotcoconut.eu/robots.txt>
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://yuriol.com/> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wikistudien.org> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.de/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://resveratrol-nebenwirkungen.info> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kley-net.de/robots.txt> (referer: None)
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://victoria-einrichtungen.eu>
{'id': '176', 'url': 'http://victoria-einrichtungen.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fabian-klemt.de/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wasistopc.de> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kley-net.de> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hilcura.de/> (referer: None)
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://altenpflege-willebadessen.de>
{'id': '173', 'url': 'http://altenpflege-willebadessen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://christian-kilthau.com/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://stein-und-soehne.de/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotcoconut.eu/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fabian-klemt.de> (referer: None)
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thomasludwig.net>
{'id': '162', 'url': 'http://thomasludwig.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gukrause.de>
{'id': '175', 'url': 'http://gukrause.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://yuriol.com/>
{'id': '116', 'url': 'https://yuriol.com/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 27 Apr 2023 09:59:24 GMT', 'tableLayout': False, 'ssl_name': 'yuriol.com', 'ssl_start': '20230531103008Z', 'ssl_expire': '20230531103008Z'}
2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wikistudien.org>
{'id': '177', 'url': 'http://wikistudien.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://resveratrol-nebenwirkungen.info>
{'id': '179', 'url': 'http://resveratrol-nebenwirkungen.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://hotcoconut.eu/> from <GET http://hotcoconut.eu>
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://stein-und-soehne.de> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.eu> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://christian-kilthau.com> (referer: None)
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://wasistopc.de>
{'id': '182', 'url': 'http://wasistopc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://kley-net.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x96 in position 51472: invalid start byte
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hilcura.de/>
{'id': '109', 'url': 'https://hilcura.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'hilcura.de', 'ssl_start': '20240329235959Z', 'ssl_expire': '20240329235959Z'}
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hotcoconut.eu/> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mitochondriale-medizin.org/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 779: invalid start byte
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://efggp.de/> (referer: None)
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fabian-klemt.de>
{'id': '185', 'url': 'http://fabian-klemt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 11 Dec 2008 11:42:32 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://stein-und-soehne.de>
{'id': '187', 'url': 'http://stein-und-soehne.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 16 Nov 2022 23:48:49 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://govido.eu> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 762: invalid start byte
2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 http://christian-kilthau.com>
{'id': '186', 'url': 'http://christian-kilthau.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-fragen-online-ktb.de/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://erp-ratschlag.de/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lilier.de/robots.txt> (referer: None)
2023-05-02 12:54:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://hotcoconut.eu/>
{'id': '181', 'url': 'https://hotcoconut.eu/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'hotcoconut.eu', 'ssl_start': '20230518232429Z', 'ssl_expire': '20230518232429Z'}
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzte-fragen-online-ktb.de> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://cyberfiber.me/> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gracic.de/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://erp-ratschlag.de> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lilier.de> (referer: None)
2023-05-02 12:54:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://chronisch-untervoegelt.de/robots.txt> (referer: None)
2023-05-02 12:54:53 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://efggp.de/>
{'id': '166', 'url': 'https://efggp.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'efggp.de', 'ssl_start': '20230611231725Z', 'ssl_expire': '20230611231725Z'}
2023-05-02 12:54:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.formativmedia.atrobots.txt> from <GET http://formativ-print.at/robots.txt>
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-plus.org/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gracic.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://fuerdiesache.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kulturbeutelhamburg.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kinderpartyfun.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzte-fragen-online-ktb.de>
{'id': '192', 'url': 'http://xn--rzte-fragen-online-ktb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://cyberfiber.me/>
{'id': '73', 'url': 'https://cyberfiber.me/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'cyberfiber.me', 'ssl_start': '20220206235959Z', 'ssl_expire': '20220206235959Z'}
2023-05-02 12:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET http://erp-ratschlag.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 57: invalid continuation byte
2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lilier.de>
{'id': '190', 'url': 'http://lilier.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 30 Dec 2013 12:30:38 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondriale-medizin.org> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-plus.org> (referer: None)
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gracic.de>
{'id': '193', 'url': 'http://gracic.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dralexandermichalzik.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://kopfstandstuhl.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.exelant.derobots.txt> from <GET http://exelant.de/robots.txt>
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://himalayaklangschalen.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexandermichalzik.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dralexandermichalzik.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fuerdiesache.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://alexandermichalzik.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET http://mitochondriale-medizin.org> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 779: invalid start byte
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-plus.org>
{'id': '195', 'url': 'http://arginin-plus.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://webmarktplatz24.de/robots.txt> from <GET http://webmarktplatz24.de/robots.txt>
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://wengkbuehle.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dralexandermichalzik.de>
{'id': '199', 'url': 'http://dralexandermichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET http://fuerdiesache.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://alexandermichalzik.de>
{'id': '202', 'url': 'http://alexandermichalzik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://himalayaklangschalen.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://streamify.tv> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lokomotor.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:54 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://secstate.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lokomotor.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://secstate.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trendsandtechnik.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://osd2000.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://osd2000.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wengkbuehle.de> (referer: None)
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://himalayaklangschalen.de>
{'id': '200', 'url': 'http://himalayaklangschalen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cnc-fanpage.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://streamify.tv>
{'id': '124', 'url': 'http://streamify.tv', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://webmarktplatz24.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lingk-shops.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.kinderpartyfun.de/> from <GET http://kinderpartyfun.de>
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://chronisch-untervoegelt.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trendsandtechnik.de> (referer: None)
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lokomotor.de>
{'id': '204', 'url': 'http://lokomotor.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://secstate.de>
{'id': '207', 'url': 'http://secstate.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 15 Mar 2010 17:15:22 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://osd2000.de>
{'id': '209', 'url': 'http://osd2000.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 20 Feb 2011 15:04:02 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wengkbuehle.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 767: invalid start byte
2023-05-02 12:54:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://webmarktplatz24.de/> from <GET http://webmarktplatz24.de>
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lingk-shops.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dukartstein.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cnc-fanpage.de> (referer: None)
2023-05-02 12:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET http://chronisch-untervoegelt.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://trendsandtechnik.de>
{'id': '208', 'url': 'http://trendsandtechnik.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nrw-zwerge.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 766: invalid start byte
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://webmarktplatz24.de/> (referer: None)
2023-05-02 12:54:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2Frobots.txt&bp-auth=1&action=bpnoaccess> from <GET https://sememo.de/robots.txt>
2023-05-02 12:54:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://richard-deutsch.com/robots.txt/> from <GET http://richard-deutsch.com/robots.txt>
2023-05-02 12:54:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://maik-wiege.de/robots.txt> (referer: None)
2023-05-02 12:54:54 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://lingk-shops.de>
{'id': '211', 'url': 'http://lingk-shops.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 28 Sep 2021 13:22:44 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cnc-fanpage.de>
{'id': '210', 'url': 'http://cnc-fanpage.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.kinderpartyfun.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.kinderpartyfun.de'))])
2023-05-02 12:54:55 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://kulturbeutelhamburg.de/> from <GET http://kulturbeutelhamburg.de>
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://webmarktplatz24.de/>
{'id': '205', 'url': 'https://webmarktplatz24.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'webmarktplatz24.de', 'ssl_start': '20230523235959Z', 'ssl_expire': '20230523235959Z'}
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobilashop.com/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dinner-ticket.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://johannes-doerfler.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nrw-zwerge.de> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobilashop.com> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://kopfstandstuhl.de> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://blass-net.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://blass-net.de> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://peter-flaspoehler.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://maik-wiege.de> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://richard-deutsch.com/robots.txt/> (referer: None)
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 35 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 53 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 61 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 93 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 97 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 104 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 115 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 123 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 131 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 141 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 149 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 151 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 153 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 155 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 205 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 235 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 242 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 254 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 273 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 280 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 299 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 311 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 318 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 330 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 348 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 355 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 374 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 381 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 382 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 383 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 391 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 399 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 400 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 401 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 407 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 408 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 409 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 416 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 417 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 418 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 425 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 426 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 427 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 434 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 435 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 436 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 447 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 448 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 449 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 451 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 452 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 453 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 454 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 468 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 482 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 495 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 496 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 497 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 502 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 532 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 551 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 552 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 553 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 554 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 555 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 556 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 557 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 558 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 559 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 560 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 561 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 562 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 563 without any user agent to enforce it on.
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://krop.at/robots.txt> (referer: None)
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://peter-flaspoehler.de> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.kinderpartyfun.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://nrw-zwerge.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 766: invalid start byte
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://johannes-doerfler.de> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dinner-ticket.de> (referer: None)
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobilashop.com>
{'id': '217', 'url': 'http://casademobilashop.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://kopfstandstuhl.de>
{'id': '197', 'url': 'http://kopfstandstuhl.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronaschutzhilfe.com/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yogamaster.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://blass-net.de>
{'id': '218', 'url': 'http://blass-net.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 23 May 2018 16:59:30 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://maik-wiege.de>
{'id': '213', 'url': 'http://maik-wiege.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coronaschutzhilfe.com> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://krop.at> (referer: None)
2023-05-02 12:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://peter-flaspoehler.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf6 in position 445: invalid start byte
2023-05-02 12:54:55 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://fun4-u.info/robots.txt> from <GET http://fun4-u.info/robots.txt>
2023-05-02 12:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://johannes-doerfler.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dinner-ticket.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.kinderpartyfun.de/> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://richard-deutsch.com> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila-shop.info/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "fun4-u.info"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'fun4-u.info'))])
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila-shop.info> (referer: None)
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://coronaschutzhilfe.com>
{'id': '223', 'url': 'http://coronaschutzhilfe.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://steiger-taxi.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mobilemietstation.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://fun4-u.info/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://serenafate.com/robots.txt> (referer: None)
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://krop.at>
{'id': '219', 'url': 'http://krop.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://fun4-u.info/> from <GET http://fun4-u.info>
2023-05-02 12:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.kinderpartyfun.de/> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 770: invalid start byte
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hotland.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://richard-deutsch.com>
{'id': '180', 'url': 'http://richard-deutsch.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://urbaczek.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://atlantis-magazin.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://birtekaufmann-photography.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 33 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 41 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 45 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 47 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 51 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 57 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 59 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 61 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 63 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 71 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 79 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 101 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 105 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 107 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 113 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 117 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 129 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 133 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 138 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 147 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 167 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 175 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 183 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 187 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 188 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 191 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 203 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 213 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 217 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 220 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 222 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 224 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 227 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 229 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 233 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 235 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 237 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 240 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 242 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 246 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 248 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 250 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 253 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 255 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 259 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 260 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 261 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 263 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 264 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 265 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 268 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 269 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 270 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 272 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 274 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 275 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 277 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 278 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 281 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 284 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 286 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 289 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 291 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 292 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 295 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 297 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 298 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 301 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 302 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 304 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 306 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 309 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 310 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 313 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 315 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 317 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 320 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 322 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 327 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 328 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 332 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 333 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 337 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 338 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 342 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 346 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 350 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 358 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 368 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 376 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 377 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 380 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 383 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 384 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 385 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 386 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 387 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 390 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 391 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 392 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 393 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 394 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 395 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 396 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 412 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 429 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 477 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 479 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 481 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 483 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 484 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 486 without any user agent to enforce it on.
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://steiger-taxi.de> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mobilemietstation.de> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://serenafate.com> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grothues8.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:55 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://losmenombak-mentawai.com/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dukartstein.de> (referer: None)
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila-shop.info>
{'id': '227', 'url': 'http://casamobila-shop.info', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://fun4-u.info/> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://urbaczek.de> (referer: None)
2023-05-02 12:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://steiger-taxi.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://mobilemietstation.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://serenafate.com>
{'id': '228', 'url': 'http://serenafate.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://fam-rauch.com/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hyaluronsaeure-kapseln.org/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.medizinrechtler.inforobots.txt> from <GET http://grad-der-behinderung.de/robots.txt>
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://herzinfarkt-vorbeugen-herzgesundheit.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://weihrauchforum.de/robots.txt> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://fam-rauch.com> (referer: None)
2023-05-02 12:54:55 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dukartstein.de>
{'id': '152', 'url': 'http://dukartstein.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hyaluronsaeure-kapseln.org> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://herzinfarkt-vorbeugen-herzgesundheit.de> (referer: None)
2023-05-02 12:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://weihrauchforum.de> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://atlantis-magazin.de> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hotland.de> (referer: None)
2023-05-02 12:54:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://fun4-u.info/>
{'id': '226', 'url': 'https://fun4-u.info/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 03 Aug 2019 10:52:54 GMT', 'tableLayout': True, 'ssl_name': 'fun4-u.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://urbaczek.de>
{'id': '230', 'url': 'http://urbaczek.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 05 Feb 2023 15:42:17 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://felgenfreddy.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kulturbeutelhamburg.de/> (referer: None)
2023-05-02 12:54:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://fam-rauch.com>
{'id': '232', 'url': 'http://fam-rauch.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 19 Jun 2011 18:13:24 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hyaluronsaeure-kapseln.org>
{'id': '233', 'url': 'http://hyaluronsaeure-kapseln.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://herzinfarkt-vorbeugen-herzgesundheit.de>
{'id': '236', 'url': 'http://herzinfarkt-vorbeugen-herzgesundheit.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://weihrauchforum.de>
{'id': '235', 'url': 'http://weihrauchforum.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://atlantis-magazin.de>
{'id': '231', 'url': 'http://atlantis-magazin.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hotland.de>
{'id': '229', 'url': 'http://hotland.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grothues8.de> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://felgenfreddy.de> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://birtekaufmann-photography.de> (referer: None)
2023-05-02 12:54:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://kulturbeutelhamburg.de/>
{'id': '147', 'url': 'https://kulturbeutelhamburg.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'kulturbeutelhamburg.de', 'ssl_start': '20230605235959Z', 'ssl_expire': '20230605235959Z'}
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sound-producer.com/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://santakruz.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://badischtauchen.de/robots.txt> from <GET http://badischtauchen.de/robots.txt>
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://essenonpaper.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yogamaster.de> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ws-gmbh.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gasthof-rolfes.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:54:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://grothues8.de>
{'id': '222', 'url': 'http://grothues8.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.scraper] ERROR: Spider error processing <GET http://felgenfreddy.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://birtekaufmann-photography.de>
{'id': '215', 'url': 'http://birtekaufmann-photography.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongstrategy.com/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://santakruz.de> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2Frobots.txt&bp-auth=1&action=bpnoaccess> (referer: None)
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 7 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 9 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 17 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 37 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 125 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 127 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 131 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 135 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 139 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 143 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 145 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 149 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 157 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 159 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 163 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 166 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 169 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 177 without any user agent to enforce it on.
2023-05-02 12:54:56 [protego] DEBUG: Rule at line 179 without any user agent to enforce it on.
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://essenonpaper.de> (referer: None)
2023-05-02 12:54:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yogamaster.de>
{'id': '221', 'url': 'http://yogamaster.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/> from <GET http://sememo.de>
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://pharmabox.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongstrategy.com> (referer: None)
2023-05-02 12:54:56 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.mattner.orgrobots.txt> from <GET http://mattner.org/robots.txt>
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://future-fashion.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pharmabox.de> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://x-schreck.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://meditationkisse.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://santakruz.de>
{'id': '240', 'url': 'http://santakruz.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Fri, 30 Oct 2009 20:47:40 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://badischtauchen.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://x-schreck.de> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://future-fashion.de> (referer: None)
2023-05-02 12:54:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://essenonpaper.de>
{'id': '241', 'url': 'http://essenonpaper.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dejongstrategy.com>
{'id': '243', 'url': 'http://dejongstrategy.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 28 Dec 2014 13:42:08 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://badischtauchen.de/> from <GET http://badischtauchen.de>
2023-05-02 12:54:56 [scrapy.core.scraper] ERROR: Spider error processing <GET http://pharmabox.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x96 in position 41434: invalid start byte
2023-05-02 12:54:56 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.okv-weiden.de/robots.txt> from <GET http://okv-weiden.de/robots.txt>
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://arbeitsbuehnen-lexikon.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ostsea.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://klangschalen-schop.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bioticon.de/robots.txt> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://meditationkisse.de> (referer: None)
2023-05-02 12:54:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://x-schreck.de>
{'id': '248', 'url': 'http://x-schreck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 06 Oct 2007 18:12:18 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://badischtauchen.de/> (referer: None)
2023-05-02 12:54:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ostsea.de> (referer: None)
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://future-fashion.de>
{'id': '247', 'url': 'http://future-fashion.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 05 Jan 2016 16:33:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.dknv.de/> from <GET http://dknv.de/robots.txt>
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bioticon.de> (referer: None)
2023-05-02 12:54:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/robots.txt> from <GET http://www.okv-weiden.de/robots.txt>
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://meditationkisse.de>
{'id': '249', 'url': 'http://meditationkisse.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arbeitsbuehnen-lexikon.de> (referer: None)
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://badischtauchen.de/>
{'id': '242', 'url': 'https://badischtauchen.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': True, 'lastModified': 'Sat, 18 Sep 2021 11:27:27 GMT', 'tableLayout': False, 'ssl_name': 'badischtauchen.de', 'ssl_start': '20230729235959Z', 'ssl_expire': '20230729235959Z'}
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://michaelbuschke.com/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikonblog.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://klangschalen-schop.de> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://has-wbe.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taxi-meile.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 24 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 26 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 34 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 38 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 40 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 46 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 48 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 50 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 52 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 60 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 62 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 64 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 72 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 94 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 98 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 100 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 104 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 106 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 108 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 110 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 112 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 114 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 116 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 118 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 120 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 122 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 124 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 126 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 128 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 130 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 132 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 134 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 136 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 138 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 140 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 142 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 144 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 146 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 148 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 150 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 152 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 154 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 156 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 158 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 160 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 162 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 164 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 166 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 168 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 170 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 172 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 174 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 176 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 178 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 180 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 182 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 184 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 186 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 188 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 190 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 192 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 194 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 196 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 198 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 200 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 202 without any user agent to enforce it on.
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ostsea.de>
{'id': '251', 'url': 'http://ostsea.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 15 Jan 2017 21:15:54 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bioticon.de>
{'id': '253', 'url': 'http://bioticon.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://arbeitsbuehnen-lexikon.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 778: invalid start byte
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikonblog.de> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://michaelbuschke.com> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/> (referer: None)
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:54:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www6.dknv.de/> from <GET http://dknv.de>
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://klangschalen-schop.de>
{'id': '254', 'url': 'http://klangschalen-schop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arztfragenonline.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casamobila.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.okv-weiden.de/> from <GET http://okv-weiden.de>
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://was-ist-opc.de> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arztfragenonline.de> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casamobila.de> (referer: None)
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikonblog.de>
{'id': '257', 'url': 'http://biotikonblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/robots.txt> from <GET http://www.okv-weiden.de/robots.txt>
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://flugzeuge-weltweit.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://peking-laufenburg.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://michaelbuschke.com> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 3438: invalid start byte
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://has-wbe.de> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westsidemusic.com/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://peking-laufenburg.de> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://barbaraundjan.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.
2023-05-02 12:54:57 [protego] DEBUG: Rule at line 16 without any user agent to enforce it on.
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-fragen-online.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.okv-weiden.de/> from <GET http://www.okv-weiden.de/>
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://was-ist-opc.de>
{'id': '262', 'url': 'http://was-ist-opc.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arzt-fragen-online.de> (referer: None)
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arztfragenonline.de>
{'id': '261', 'url': 'http://arztfragenonline.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casamobila.de>
{'id': '260', 'url': 'http://casamobila.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.at/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://volki.pb.photographyrobots.txt> from <GET http://galerie-bauer.de/robots.txt>
2023-05-02 12:54:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess> from <GET https://sememo.de/>
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://barbaraundjan.de> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://taxi-meile.de> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.at> (referer: None)
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://has-wbe.de>
{'id': '255', 'url': 'http://has-wbe.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www6.dknv.de/> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://nomadpublishing.store/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://westsidemusic.com> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://flugzeuge-weltweit.de> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sound-producer.com> (referer: None)
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://peking-laufenburg.de>
{'id': '264', 'url': 'http://peking-laufenburg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 24 Aug 2020 11:50:05 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arzt-fragen-online.de>
{'id': '266', 'url': 'http://arzt-fragen-online.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://barbaraundjan.de>
{'id': '259', 'url': 'http://barbaraundjan.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.okv-weiden.de/> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://casademobila24.eu/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nomadpublishing.store> (referer: None)
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://taxi-meile.de>
{'id': '246', 'url': 'http://taxi-meile.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.at>
{'id': '267', 'url': 'http://biotikon.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://casademobila24.eu> (referer: None)
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www6.dknv.de/>
{'id': '256', 'url': 'http://www6.dknv.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://westsidemusic.com>
{'id': '265', 'url': 'http://westsidemusic.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://flugzeuge-weltweit.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 3327: invalid continuation byte
2023-05-02 12:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://sound-producer.com>
{'id': '203', 'url': 'http://sound-producer.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://curcuma-nebenwirkungen.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://aus-keil-wird-schmidt.de/robots.txt> (referer: None)
2023-05-02 12:54:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://grinberg.de/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:54:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.okv-weiden.de/>
{'id': '252', 'url': 'https://www.okv-weiden.de/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 02 May 2023 12:54:57 GMT', 'tableLayout': False, 'ssl_name': 'www.okv-weiden.de', 'ssl_start': '20230522235959Z', 'ssl_expire': '20230522235959Z'}
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://nomadpublishing.store>
{'id': '270', 'url': 'http://nomadpublishing.store', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://casademobila24.eu>
{'id': '271', 'url': 'http://casademobila24.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://curcuma-nebenwirkungen.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://indiaschop.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://tropicanalife.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://aus-keil-wird-schmidt.de> (referer: None)
2023-05-02 12:54:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://grinberg.de/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://grinberg.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://tropicanalife.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.eu/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://holzmoebel-experte.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativopc.eu> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://curcuma-nebenwirkungen.de>
{'id': '273', 'url': 'http://curcuma-nebenwirkungen.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://grinberg.de> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://aus-keil-wird-schmidt.de>
{'id': '274', 'url': 'http://aus-keil-wird-schmidt.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 22 Apr 2008 07:31:46 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://holzmoebel-experte.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzteonlinefragen-unb.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cradle-software.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://indiaschop.de> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tropicanalife.de>
{'id': '276', 'url': 'http://tropicanalife.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 28 Mar 2009 22:52:40 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://strumpfaffen.org/robots.txt> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativopc.eu>
{'id': '277', 'url': 'http://3nativopc.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--rzteonlinefragen-unb.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://govido.org/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://losmenombak-mentawai.com> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cradle-software.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://strumpfaffen.org> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://gasthof-rolfes.de> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://grinberg.de>
{'id': '272', 'url': 'http://grinberg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 28 Jan 2023 20:32:03 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bergium.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://studio-balance.at/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thebodyasarchive.com/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.ch/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bergium.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-babyzimmer.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://studio-balance.at> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://holzmoebel-experte.de>
{'id': '278', 'url': 'http://holzmoebel-experte.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess>
{'id': '174', 'url': 'https://sememo.de/wp-login.php?itsec-hb-token=log&redirect_to=https%3A%2F%2Fsememo.de%2F&bp-auth=1&action=bpnoaccess', 'status': 200, 'title': 'Anmelden ‹ sememo — WordPress', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'sememo.de', 'ssl_start': '20230710235959Z', 'ssl_expire': '20230710235959Z'}
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://indiaschop.de>
{'id': '275', 'url': 'http://indiaschop.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-shop-paderborn.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-babyzimmer.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thebodyasarchive.com> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biotikon.ch> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--rzteonlinefragen-unb.de>
{'id': '280', 'url': 'http://xn--rzteonlinefragen-unb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://losmenombak-mentawai.com> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 777: invalid start byte
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cradle-software.de>
{'id': '281', 'url': 'http://cradle-software.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://strumpfaffen.org>
{'id': '282', 'url': 'http://strumpfaffen.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 23 Apr 2012 11:57:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://gasthof-rolfes.de>
{'id': '239', 'url': 'http://gasthof-rolfes.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://bergium.de>
{'id': '283', 'url': 'http://bergium.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 07 Nov 2017 13:41:54 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://studio-balance.at> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdc in position 36326: invalid continuation byte
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondrien-funktion.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-babyzimmer.de>
{'id': '288', 'url': 'http://massivholz-babyzimmer.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thebodyasarchive.com>
{'id': '286', 'url': 'http://thebodyasarchive.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sun, 27 Dec 2020 18:13:23 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://biotikon.ch>
{'id': '287', 'url': 'http://biotikon.ch', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://format-mehrweg.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://massivholz-dielenmoebel.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cosmecon.eu/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.com/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mitochondrien-funktion.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://reitanlage-riedmuehle.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 30 without any user agent to enforce it on.
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 36 without any user agent to enforce it on.
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 42 without any user agent to enforce it on.
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 49 without any user agent to enforce it on.
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://format-mehrweg.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cosmecon.eu> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://arginin-alpha-ketoglutarat.com> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://massivholz-dielenmoebel.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--kleinegrashpfer-9vb.at/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://skiverband-mv.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://xn--kleinegrashpfer-9vb.at> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mitochondrien-funktion.de>
{'id': '290', 'url': 'http://mitochondrien-funktion.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://eventation.de/robots.txt> (failed 1 times): 500 Internal Server Error
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ws-gmbh.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ticila.com/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://yoga-lothar-wester.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://format-mehrweg.de>
{'id': '292', 'url': 'http://format-mehrweg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 10 Dec 2019 16:45:18 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cosmecon.eu>
{'id': '293', 'url': 'http://cosmecon.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 25 Nov 2009 14:28:26 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://seat-factory.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ticila.com> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dinnerticket.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://arginin-alpha-ketoglutarat.com>
{'id': '291', 'url': 'http://arginin-alpha-ketoglutarat.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://massivholz-dielenmoebel.de>
{'id': '294', 'url': 'http://massivholz-dielenmoebel.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://technolog-e.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://seat-factory.de> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://xn--kleinegrashpfer-9vb.at>
{'id': '295', 'url': 'http://xn--kleinegrashpfer-9vb.at', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 13 Apr 2023 07:16:11 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://reitanlage-riedmuehle.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://skiverband-mv.de> (referer: None)
2023-05-02 12:54:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://eventation.de/robots.txt> (failed 2 times): 500 Internal Server Error
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (500) <GET http://eventation.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://technolog-e.de> (referer: None)
2023-05-02 12:54:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.kg-muellekolk.de/robots.txt> from <GET http://kg-muellekolk.de/robots.txt>
2023-05-02 12:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://ws-gmbh.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://ticila.com>
{'id': '300', 'url': 'http://ticila.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 05 Jan 2016 16:33:20 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://mcpicchu.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dinnerticket.de> (referer: None)
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://seat-factory.de>
{'id': '301', 'url': 'http://seat-factory.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Wed, 15 Jul 2015 14:10:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 http://reitanlage-riedmuehle.de>
{'id': '284', 'url': 'http://reitanlage-riedmuehle.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://dirkklages.com/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://trengo.de/robots.txt> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-shop-paderborn.de> (referer: None)
2023-05-02 12:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://mcpicchu.de> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-datenbank.net/robots.txt> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://eventation.de> (referer: None)
2023-05-02 12:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://skiverband-mv.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 35: invalid continuation byte
2023-05-02 12:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://technolog-e.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 342: invalid continuation byte
2023-05-02 12:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://dinnerticket.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 768: invalid start byte
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://lederoutfits.de/robots.txt> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dirkklages.com> (referer: None)
2023-05-02 12:54:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.nachhilfe-oase.de/robots.txt> from <GET http://123discounter.de/robots.txt>
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://lederoutfits.de> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://trengo.de> (referer: None)
2023-05-02 12:54:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-shop-paderborn.de>
{'id': '289', 'url': 'http://yoga-shop-paderborn.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://ouzo-plomari.com/robots.txt> (referer: None)
2023-05-02 12:54:59 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:54:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://mcpicchu.de>
{'id': '304', 'url': 'http://mcpicchu.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 23 Jul 2022 22:42:14 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://eventation.de>
{'id': '299', 'url': 'http://eventation.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dirkklages.com>
{'id': '307', 'url': 'http://dirkklages.com', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 17 Apr 2021 09:45:46 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://lederoutfits.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdc in position 3819: invalid continuation byte
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://govido.org> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://yoga-lothar-wester.de> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://cymeg.de/robots.txt> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/robots.txt> (referer: None)
2023-05-02 12:54:59 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://die-goldene-schallplatte.de/robots.txt> (referer: None)
2023-05-02 12:54:59 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 780: invalid start byte
2023-05-02 12:54:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://trengo.de>
{'id': '306', 'url': 'http://trengo.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://teamtrzweb.de> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ouzo-plomari.com> (referer: None)
2023-05-02 12:54:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.nachhilfe-oase.de/> from <GET http://123discounter.de>
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cymeg.de> (referer: None)
2023-05-02 12:54:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://govido.org> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:54:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:54:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://yoga-lothar-wester.de>
{'id': '297', 'url': 'http://yoga-lothar-wester.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongsblog.de/robots.txt> (referer: None)
2023-05-02 12:54:59 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:59 [protego] DEBUG: Rule at line 4 without any user agent to enforce it on.
2023-05-02 12:54:59 [protego] DEBUG: Rule at line 6 without any user agent to enforce it on.
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.eu/robots.txt> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/robots.txt> (referer: None)
2023-05-02 12:54:59 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://3nativ-opc.eu> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://die-goldene-schallplatte.de> (referer: None)
2023-05-02 12:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.nachhilfe-oase.de/> (referer: None)
2023-05-02 12:54:59 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://teamtrzweb.de>
{'id': '305', 'url': 'http://teamtrzweb.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-datenbank.net> (referer: None)
2023-05-02 12:55:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://schiefer-lautsprecher.de/robots.txt> (referer: None)
2023-05-02 12:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dark-hawks.de/robots.txt> (referer: None)
2023-05-02 12:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dejongsblog.de> (referer: None)
2023-05-02 12:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/robots.txt> (referer: None)
2023-05-02 12:55:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://photos.hr-photo.derobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET http://ouzo-plomari.com> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 769: invalid start byte
2023-05-02 12:55:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://cymeg.de>
{'id': '313', 'url': 'http://cymeg.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Sat, 17 Apr 2021 09:45:46 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://3nativ-opc.eu>
{'id': '314', 'url': 'http://3nativ-opc.eu', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Sun, 01 Nov 2015 18:42:59 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET http://die-goldene-schallplatte.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 780: invalid start byte
2023-05-02 12:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.nachhilfe-oase.de/>
{'id': '311', 'url': 'http://www.nachhilfe-oase.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:55:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-datenbank.net>
{'id': '303', 'url': 'http://auto-datenbank.net', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:55:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dejongsblog.de>
{'id': '310', 'url': 'http://dejongsblog.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schiefer-lautsprecher.de> (referer: None)
2023-05-02 12:55:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://schiefer-lautsprecher.de>
{'id': '315', 'url': 'http://schiefer-lautsprecher.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:55:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.kg-muellekolk.de/> from <GET http://kg-muellekolk.de>
2023-05-02 12:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://dark-hawks.de> (referer: None)
2023-05-02 12:55:01 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://dark-hawks.de>
{'id': '279', 'url': 'http://dark-hawks.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/robots.txt> (referer: None)
2023-05-02 12:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.kg-muellekolk.de/> (referer: None)
2023-05-02 12:55:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.kg-muellekolk.de/>
{'id': '268', 'url': 'http://www.kg-muellekolk.de/', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:55:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.pfotenabdruck.inforobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:55:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.exelant.derobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:55:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.formativmedia.atrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:55:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.medizinrechtler.inforobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:55:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.mattner.orgrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:55:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://volki.pb.photographyrobots.txt> (failed 1 times): DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:55:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://photos.hr-photo.derobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:55:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://hr-photo.de/robots.txt>: DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: photos.hr-photo.derobots.txt.
2023-05-02 12:55:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://photos.hr-photo.de> from <GET http://hr-photo.de>
2023-05-02 12:55:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://photos.hr-photo.de/robots.txt> (referer: None)
2023-05-02 12:55:15 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 771: invalid start byte
2023-05-02 12:55:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://photos.hr-photo.de> (referer: None)
2023-05-02 12:55:15 [scrapy.core.scraper] ERROR: Spider error processing <GET http://photos.hr-photo.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 771: invalid start byte
2023-05-02 12:55:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.pfotenabdruck.inforobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:55:22 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://pfoten-abdruck.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.pfotenabdruck.inforobots.txt.
2023-05-02 12:55:22 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.pfotenabdruck.info> from <GET http://pfoten-abdruck.de>
2023-05-02 12:55:22 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.pfotenabdruck.info/robots.txt> (referer: None)
2023-05-02 12:55:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.pfotenabdruck.info> (referer: None)
2023-05-02 12:55:22 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.pfotenabdruck.info>
{'id': '167', 'url': 'https://www.pfotenabdruck.info', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Tue, 21 Apr 2020 09:28:05 GMT', 'tableLayout': False, 'ssl_name': 'pfotenabdruck.info', 'ssl_start': '20230609235959Z', 'ssl_expire': '20230609235959Z'}
2023-05-02 12:55:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://www.formativmedia.atrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:55:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://formativ-print.at/robots.txt>: DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.formativmedia.atrobots.txt.
2023-05-02 12:55:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.formativmedia.at> from <GET http://formativ-print.at>
2023-05-02 12:55:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.exelant.derobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:55:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://exelant.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.exelant.derobots.txt.
2023-05-02 12:55:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.exelant.de> from <GET http://exelant.de>
2023-05-02 12:55:25 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.exelant.de"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.exelant.de'))])
2023-05-02 12:55:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.exelant.de/robots.txt> (referer: None)
2023-05-02 12:55:25 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:55:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.exelant.de> (referer: None)
2023-05-02 12:55:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.exelant.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 763: invalid start byte
2023-05-02 12:55:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.formativmedia.at/robots.txt> (referer: None)
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 44 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 54 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 56 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 58 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 65 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 67 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 73 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 75 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 77 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 78 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 80 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 81 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 82 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 83 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 85 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 86 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 87 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 88 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 89 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 92 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 102 without any user agent to enforce it on.
2023-05-02 12:55:26 [protego] DEBUG: Rule at line 109 without any user agent to enforce it on.
2023-05-02 12:55:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.medizinrechtler.inforobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:55:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://grad-der-behinderung.de/robots.txt>: DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.medizinrechtler.inforobots.txt.
2023-05-02 12:55:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.medizinrechtler.info> from <GET http://grad-der-behinderung.de>
2023-05-02 12:55:27 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "www.medizinrechtler.info"; VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.medizinrechtler.info'))])
2023-05-02 12:55:27 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.medizinrechtler.info/robots.txt> (referer: None)
2023-05-02 12:55:27 [scrapy.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/robotstxt.py", line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode("utf-8")
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:55:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.medizinrechtler.info> (referer: None)
2023-05-02 12:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.medizinrechtler.info> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/dubdev/dubdev/spiders/spider.py", line 124, in parse
    content = response.body.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 773: invalid start byte
2023-05-02 12:55:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://www.mattner.orgrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:55:28 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://mattner.org/robots.txt>: DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.mattner.orgrobots.txt.
2023-05-02 12:55:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.mattner.org> from <GET http://mattner.org>
2023-05-02 12:55:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.formativmedia.at/> from <GET http://www.formativmedia.at>
2023-05-02 12:55:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.mattner.org/robots.txt> (referer: None)
2023-05-02 12:55:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.mattner.org> (referer: None)
2023-05-02 12:55:29 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.mattner.org>
{'id': '245', 'url': 'http://www.mattner.org', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:55:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://volki.pb.photographyrobots.txt> (failed 2 times): DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:55:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://galerie-bauer.de/robots.txt>: DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1693, in _inlineCallbacks
    result = context.run(
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/python/failure.py", line 518, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 52, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 892, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/endpoints.py", line 1022, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: volki.pb.photographyrobots.txt.
2023-05-02 12:55:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://volki.pb.photography> from <GET http://galerie-bauer.de>
2023-05-02 12:55:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.formativmedia.at/> (referer: None)
2023-05-02 12:55:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://volki.pb.photography/robots.txt> (referer: None)
2023-05-02 12:55:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.formativmedia.at/>
{'id': '196', 'url': 'https://www.formativmedia.at/', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'einfachwanda.at', 'ssl_start': '20230515034747Z', 'ssl_expire': '20230515034747Z'}
2023-05-02 12:55:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://volki.pb.photography> (referer: None)
2023-05-02 12:55:30 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:55:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://volki.pb.photography>
{'id': '269', 'url': 'https://volki.pb.photography', 'status': 200, 'title': None, 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': '*.pb.photography', 'ssl_start': '20240222235959Z', 'ssl_expire': '20240222235959Z'}
2023-05-02 12:55:30 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:55:30 [scrapy.extensions.feedexport] INFO: Stored jl feed (242 items) in: baddata_results_20230502_125444.jl
2023-05-02 12:55:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 14,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 14,
 'downloader/request_bytes': 172470,
 'downloader/request_count': 760,
 'downloader/request_method_count/GET': 760,
 'downloader/response_bytes': 2707019,
 'downloader/response_count': 746,
 'downloader/response_status_count/200': 468,
 'downloader/response_status_count/301': 25,
 'downloader/response_status_count/302': 67,
 'downloader/response_status_count/404': 174,
 'downloader/response_status_count/500': 6,
 'downloader/response_status_count/503': 6,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 44.534911,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 55, 30, 329021),
 'httpcompression/response_bytes': 4841507,
 'httpcompression/response_count': 352,
 'item_scraped_count': 242,
 'log_count/DEBUG': 1720,
 'log_count/ERROR': 93,
 'log_count/INFO': 11,
 'log_count/WARNING': 221,
 'memusage/max': 66621440,
 'memusage/startup': 66621440,
 'response_received_count': 645,
 'retry/count': 13,
 'retry/max_reached': 13,
 'retry/reason_count/500 Internal Server Error': 3,
 'retry/reason_count/503 Service Unavailable': 3,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 7,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 7,
 'robotstxt/request_count': 337,
 'robotstxt/response_count': 330,
 'robotstxt/response_status_count/200': 151,
 'robotstxt/response_status_count/404': 174,
 'robotstxt/response_status_count/500': 3,
 'robotstxt/response_status_count/503': 2,
 'scheduler/dequeued': 366,
 'scheduler/dequeued/memory': 366,
 'scheduler/enqueued': 366,
 'scheduler/enqueued/memory': 366,
 'spider_exceptions/UnicodeDecodeError': 72,
 'start_time': datetime.datetime(2023, 5, 2, 12, 54, 45, 794110)}
2023-05-02 12:55:30 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:57:45 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:57:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:57:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:57:45 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:57:45 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:57:45 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:57:45 [scrapy.extensions.telnet] INFO: Telnet Password: 86f4bbb2a92a9409
2023-05-02 12:57:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:57:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:57:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:57:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:57:45 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:57:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:57:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:57:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 12:57:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 12:57:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:57:45 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '0', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:57:45 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:57:45 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_125744.jl
2023-05-02 12:57:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 0.236856,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 57, 45, 996815),
 'httpcompression/response_bytes': 1523,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66646016,
 'memusage/startup': 66646016,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 12, 57, 45, 759959)}
2023-05-02 12:57:45 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:58:33 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:58:33 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:58:33 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:58:33 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:58:33 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:58:33 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:58:33 [scrapy.extensions.telnet] INFO: Telnet Password: b2d8f129e6b6ab52
2023-05-02 12:58:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:58:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:58:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:58:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:58:33 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:58:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:58:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:58:33 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 12:58:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 12:58:33 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:58:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '0', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:58:33 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:58:33 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_125832.jl
2023-05-02 12:58:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 0.196767,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 58, 33, 658813),
 'httpcompression/response_bytes': 1523,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66621440,
 'memusage/startup': 66621440,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 12, 58, 33, 462046)}
2023-05-02 12:58:33 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:58:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:58:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:58:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:58:50 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:58:50 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:58:50 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:58:50 [scrapy.extensions.telnet] INFO: Telnet Password: 1de7ad21cf24a01b
2023-05-02 12:58:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:58:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:58:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:58:50 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:58:50 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:58:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:58:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:58:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 12:58:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 12:58:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:58:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '0', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:58:50 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:58:50 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_125849.jl
2023-05-02 12:58:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 0.244952,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 58, 50, 787526),
 'httpcompression/response_bytes': 1523,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66625536,
 'memusage/startup': 66625536,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 12, 58, 50, 542574)}
2023-05-02 12:58:50 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 12:59:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 12:59:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 12:59:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 12:59:34 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 12:59:34 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 12:59:34 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 12:59:34 [scrapy.extensions.telnet] INFO: Telnet Password: 517446bf8359e7bd
2023-05-02 12:59:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 12:59:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 12:59:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 12:59:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 12:59:34 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:59:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 12:59:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:59:34 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 12:59:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 12:59:34 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 12:59:34 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '0', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 12:59:34 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 12:59:34 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_125933.jl
2023-05-02 12:59:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 0.194171,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 12, 59, 34, 453584),
 'httpcompression/response_bytes': 1523,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66547712,
 'memusage/startup': 66547712,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 12, 59, 34, 259413)}
2023-05-02 12:59:34 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 13:02:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 13:02:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 13:02:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 13:02:10 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 13:02:10 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 13:02:10 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 13:02:10 [scrapy.extensions.telnet] INFO: Telnet Password: eae4f6562ff4d7cb
2023-05-02 13:02:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 13:02:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 13:02:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 13:02:10 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 13:02:10 [scrapy.core.engine] INFO: Spider opened
2023-05-02 13:02:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 13:02:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 13:02:11 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 13:02:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 13:02:11 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 13:02:11 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '0', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 13:02:11 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 13:02:11 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_130210.jl
2023-05-02 13:02:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 0.250206,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 13, 2, 11, 264468),
 'httpcompression/response_bytes': 1523,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66613248,
 'memusage/startup': 66613248,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 13, 2, 11, 14262)}
2023-05-02 13:02:11 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 13:02:55 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 13:02:55 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 13:02:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 13:02:55 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 13:02:55 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 13:02:55 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 13:02:55 [scrapy.extensions.telnet] INFO: Telnet Password: cb5a8683712fa186
2023-05-02 13:02:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 13:02:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 13:02:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 13:02:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 13:02:55 [scrapy.core.engine] INFO: Spider opened
2023-05-02 13:02:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 13:02:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 13:02:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 13:02:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 13:02:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 13:02:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '0', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 13:02:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 13:02:56 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_130254.jl
2023-05-02 13:02:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 0.235823,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 13, 2, 56, 67884),
 'httpcompression/response_bytes': 1523,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66596864,
 'memusage/startup': 66596864,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 13, 2, 55, 832061)}
2023-05-02 13:02:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-02 13:18:59 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 13:18:59 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 13:18:59 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 13:18:59 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 13:18:59 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 13:18:59 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 13:18:59 [scrapy.extensions.telnet] INFO: Telnet Password: 384c8faecd83d528
2023-05-02 13:18:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 13:19:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 13:19:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 13:19:00 [twisted] CRITICAL: Unhandled error in Deferred:
2023-05-02 13:19:00 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1697, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 122, in crawl
    self.engine = self._create_engine()
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 136, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/engine.py", line 79, in __init__
    self.scraper = Scraper(crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/scraper.py", line 101, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/middleware.py", line 68, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/middleware.py", line 44, in from_settings
    mw = create_instance(mwcls, settings, crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/misc.py", line 171, in create_instance
    instance = objcls(*args, **kwargs)
TypeError: __init__() missing 4 required positional arguments: 'bad_title_file_path', 'domain_parking_file_path', 'maintenance_file_path', and 'flash_file_path'
2023-05-02 13:21:24 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 13:21:24 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 13:21:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 13:21:24 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 13:21:24 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 13:21:24 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 13:21:24 [scrapy.extensions.telnet] INFO: Telnet Password: 5cdbf048a55c9754
2023-05-02 13:21:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 13:21:24 [twisted] CRITICAL: Unhandled error in Deferred:
2023-05-02 13:21:24 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1697, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 121, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 133, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spiders/__init__.py", line 53, in from_crawler
    spider = cls(*args, **kwargs)
TypeError: __init__() missing 4 required positional arguments: 'bad_title_file_path', 'domain_parking_file_path', 'maintenance_file_path', and 'flash_file_path'
2023-05-02 13:21:59 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 13:21:59 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 13:21:59 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 13:21:59 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 13:21:59 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 13:21:59 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 13:21:59 [scrapy.extensions.telnet] INFO: Telnet Password: d75e1f89d534fb2e
2023-05-02 13:21:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 13:21:59 [twisted] CRITICAL: Unhandled error in Deferred:
2023-05-02 13:21:59 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1697, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 121, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 133, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spiders/__init__.py", line 53, in from_crawler
    spider = cls(*args, **kwargs)
TypeError: __init__() missing 4 required positional arguments: 'bad_title_file_path', 'domain_parking_file_path', 'maintenance_file_path', and 'flash_file_path'
2023-05-02 13:23:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 13:23:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 13:23:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 13:23:09 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 13:23:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 13:23:09 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 13:23:09 [scrapy.extensions.telnet] INFO: Telnet Password: 7f0826f897be6ebb
2023-05-02 13:23:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 13:23:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 13:23:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 13:23:09 [twisted] CRITICAL: Unhandled error in Deferred:
2023-05-02 13:23:09 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1697, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 122, in crawl
    self.engine = self._create_engine()
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 136, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/engine.py", line 79, in __init__
    self.scraper = Scraper(crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/scraper.py", line 101, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/middleware.py", line 68, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/middleware.py", line 44, in from_settings
    mw = create_instance(mwcls, settings, crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/misc.py", line 171, in create_instance
    instance = objcls(*args, **kwargs)
TypeError: __init__() missing 4 required positional arguments: 'bad_title_file_path', 'domain_parking_file_path', 'maintenance_file_path', and 'flash_file_path'
2023-05-02 13:25:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 13:25:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 13:25:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 13:25:30 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 13:25:30 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 13:25:30 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 13:25:30 [scrapy.extensions.telnet] INFO: Telnet Password: 1edf8badaa6fac10
2023-05-02 13:25:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 13:25:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 13:25:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 13:25:31 [twisted] CRITICAL: Unhandled error in Deferred:
2023-05-02 13:25:31 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1697, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 122, in crawl
    self.engine = self._create_engine()
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 136, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/engine.py", line 79, in __init__
    self.scraper = Scraper(crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/scraper.py", line 101, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/middleware.py", line 68, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/middleware.py", line 44, in from_settings
    mw = create_instance(mwcls, settings, crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/misc.py", line 171, in create_instance
    instance = objcls(*args, **kwargs)
TypeError: __init__() missing 4 required positional arguments: 'bad_title_file_path', 'domain_parking_file_path', 'maintenance_file_path', and 'flash_file_path'
2023-05-02 13:31:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 13:31:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 13:31:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 13:31:56 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 13:31:56 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 13:31:56 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 13:31:56 [scrapy.extensions.telnet] INFO: Telnet Password: f9a119dcffd76aac
2023-05-02 13:31:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 13:31:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 13:31:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 13:31:56 [twisted] CRITICAL: Unhandled error in Deferred:
2023-05-02 13:31:56 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/twisted/internet/defer.py", line 1697, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 122, in crawl
    self.engine = self._create_engine()
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/crawler.py", line 136, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/engine.py", line 79, in __init__
    self.scraper = Scraper(crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/scraper.py", line 101, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/middleware.py", line 68, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/middleware.py", line 44, in from_settings
    mw = create_instance(mwcls, settings, crawler)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/misc.py", line 171, in create_instance
    instance = objcls(*args, **kwargs)
TypeError: __init__() missing 4 required positional arguments: 'bad_title_file_path', 'domain_parking_file_path', 'maintenance_file_path', and 'flash_file_path'
2023-05-02 13:32:09 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-02 13:32:09 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-02 13:32:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-02 13:32:09 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-02 13:32:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-02 13:32:09 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-02 13:32:09 [scrapy.extensions.telnet] INFO: Telnet Password: cc55ff3a46948c49
2023-05-02 13:32:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-02 13:32:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-02 13:32:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-02 13:32:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-02 13:32:09 [scrapy.core.engine] INFO: Spider opened
2023-05-02 13:32:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-02 13:32:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 13:32:09 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-02 13:32:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-02 13:32:09 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-02 13:32:09 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '0', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-02 13:32:09 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 13:32:09 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230502_133208.jl
2023-05-02 13:32:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 0.199453,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 2, 13, 32, 9, 987825),
 'httpcompression/response_bytes': 1523,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66748416,
 'memusage/startup': 66748416,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 2, 13, 32, 9, 788372)}
2023-05-02 13:32:09 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 11:23:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: dubdev)
2023-05-03 11:23:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 11:23:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'dubdev',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'dubdev.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['dubdev.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 11:23:12 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 11:23:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 11:23:12 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 11:23:12 [scrapy.extensions.telnet] INFO: Telnet Password: 1f8d0bcba2c92f73
2023-05-03 11:23:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 11:23:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 11:23:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 11:23:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 11:23:12 [scrapy.core.engine] INFO: Spider opened
2023-05-03 11:23:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 11:23:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 11:23:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-03 11:23:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-03 11:23:13 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 11:23:13 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '0', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 11:23:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 11:23:13 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_112312.jl
2023-05-03 11:23:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 0.227399,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 11, 23, 13, 170569),
 'httpcompression/response_bytes': 1523,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66592768,
 'memusage/startup': 66592768,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 3, 11, 23, 12, 943170)}
2023-05-03 11:23:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 11:28:03 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 11:28:03 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 11:28:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 11:28:03 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 11:28:03 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 11:28:03 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 11:28:03 [scrapy.extensions.telnet] INFO: Telnet Password: 205f346e39173585
2023-05-03 11:28:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 11:28:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 11:28:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 11:28:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 11:28:03 [scrapy.core.engine] INFO: Spider opened
2023-05-03 11:28:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 11:28:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 11:28:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-03 11:28:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-03 11:28:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 11:28:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '0', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 11:28:03 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 11:28:03 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_112802.jl
2023-05-03 11:28:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 0.199874,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 11, 28, 3, 519872),
 'httpcompression/response_bytes': 1523,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66572288,
 'memusage/startup': 66572288,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 3, 11, 28, 3, 319998)}
2023-05-03 11:28:03 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:01:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:01:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:01:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:01:22 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:01:22 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:01:22 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:01:22 [scrapy.extensions.telnet] INFO: Telnet Password: 8177c082fb9a8400
2023-05-03 12:01:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:01:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:01:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:01:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:01:23 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:01:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:01:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:01:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://auto-service-beck.de/robots.txt> (referer: None)
2023-05-03 12:01:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://auto-service-beck.de> (referer: None)
2023-05-03 12:01:24 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:01:24 [scrapy.core.scraper] DEBUG: Scraped from <200 http://auto-service-beck.de>
{'id': '0', 'url': 'http://auto-service-beck.de', 'status': 200, 'title': None, 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': 'Thu, 17 Nov 2016 15:58:57 GMT', 'tableLayout': True, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:01:24 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:01:24 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_120119.jl
2023-05-03 12:01:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 0.264316,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 1, 24, 112172),
 'httpcompression/response_bytes': 1523,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66617344,
 'memusage/startup': 66617344,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 5, 3, 12, 1, 23, 847856)}
2023-05-03 12:01:24 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:01:50 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:01:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:01:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:01:50 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:01:50 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:01:50 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:01:50 [scrapy.extensions.telnet] INFO: Telnet Password: d18a60b3daeaa6c6
2023-05-03 12:01:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:01:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:01:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:01:50 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:01:50 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:01:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:01:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:01:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:01:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:01:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:01:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:01:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:01:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:01:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:01:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hawk-systems.de>
{'id': 'hawk-systems.de', 'url': 'http://hawk-systems.de', 'status': 200, 'title': 'Hawk-Systems', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 03 Jan 2019 11:59:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:01:50 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:01:50 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thechaosteam.de>
{'id': 'thechaosteam.de', 'url': 'http://thechaosteam.de', 'status': 200, 'title': 'Page Redirection', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 11 Jan 2016 01:01:06 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:01:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:01:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:01:52 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:01:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.nancyglor.de/>
{'id': 'nancyglor.de', 'url': 'https://www.nancyglor.de/', 'status': 200, 'title': 'Klimapositive Photographie | Leipzig | für Unternehmen und Selbständige', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'nancyglor.de', 'ssl_start': '20230717235959Z', 'ssl_expire': '20230717235959Z'}
2023-05-03 12:01:52 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:01:52 [scrapy.extensions.feedexport] INFO: Stored jl feed (3 items) in: baddata_results_20230503_120149.jl
2023-05-03 12:01:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1756,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 270078,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 2.176126,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 1, 52, 827970),
 'httpcompression/response_bytes': 1678152,
 'httpcompression/response_count': 3,
 'item_scraped_count': 3,
 'log_count/DEBUG': 14,
 'log_count/INFO': 11,
 'log_count/WARNING': 3,
 'memusage/max': 66531328,
 'memusage/startup': 66531328,
 'response_received_count': 7,
 'robotstxt/request_count': 4,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 5, 3, 12, 1, 50, 651844)}
2023-05-03 12:01:52 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:02:02 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:02:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:02:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:02:02 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:02:02 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:02:02 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:02:02 [scrapy.extensions.telnet] INFO: Telnet Password: 6d419574e8f79425
2023-05-03 12:02:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:02:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:02:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:02:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:02:02 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:02:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:02:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:02:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:02:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:02:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:02:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:02:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:02:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:02:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:02:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hawk-systems.de>
{'id': 'hawk-systems.de', 'url': 'http://hawk-systems.de', 'status': 200, 'title': 'Hawk-Systems', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 03 Jan 2019 11:59:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:02:02 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:02:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thechaosteam.de>
{'id': 'thechaosteam.de', 'url': 'http://thechaosteam.de', 'status': 200, 'title': 'Page Redirection', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 11 Jan 2016 01:01:06 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:02:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:02:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:02:03 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:02:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.nancyglor.de/>
{'id': 'nancyglor.de', 'url': 'https://www.nancyglor.de/', 'status': 200, 'title': 'Klimapositive Photographie | Leipzig | für Unternehmen und Selbständige', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'nancyglor.de', 'ssl_start': '20230717235959Z', 'ssl_expire': '20230717235959Z'}
2023-05-03 12:02:03 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:02:03 [scrapy.extensions.feedexport] INFO: Stored jl feed (3 items) in: baddata_results_20230503_120201.jl
2023-05-03 12:02:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1756,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 269669,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 0.566883,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 2, 3, 179976),
 'httpcompression/response_bytes': 1678152,
 'httpcompression/response_count': 3,
 'item_scraped_count': 3,
 'log_count/DEBUG': 14,
 'log_count/INFO': 11,
 'log_count/WARNING': 3,
 'memusage/max': 66453504,
 'memusage/startup': 66453504,
 'response_received_count': 7,
 'robotstxt/request_count': 4,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 5, 3, 12, 2, 2, 613093)}
2023-05-03 12:02:03 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:03:55 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:03:55 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:03:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:03:55 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:03:55 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:03:55 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:03:55 [scrapy.extensions.telnet] INFO: Telnet Password: bf8e60642ec4e0a2
2023-05-03 12:03:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:03:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:03:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:03:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:03:55 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:03:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:03:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:03:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:03:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:03:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:03:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:03:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:03:55 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:03:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:03:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hawk-systems.de>
{'id': 'hawk-systems.de', 'url': 'http://hawk-systems.de', 'status': 200, 'title': 'Hawk-Systems', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 03 Jan 2019 11:59:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:03:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:03:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thechaosteam.de>
{'id': 'thechaosteam.de', 'url': 'http://thechaosteam.de', 'status': 200, 'title': 'Page Redirection', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 11 Jan 2016 01:01:06 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:03:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:03:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:03:56 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:03:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.nancyglor.de/>
{'id': 'nancyglor.de', 'url': 'https://www.nancyglor.de/', 'status': 200, 'title': 'Klimapositive Photographie | Leipzig | für Unternehmen und Selbständige', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'nancyglor.de', 'ssl_start': '20230717235959Z', 'ssl_expire': '20230717235959Z'}
2023-05-03 12:03:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:03:56 [scrapy.extensions.feedexport] INFO: Stored jl feed (3 items) in: baddata_results_20230503_120354.jl
2023-05-03 12:03:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1756,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 269671,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 0.561446,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 3, 56, 395485),
 'httpcompression/response_bytes': 1678152,
 'httpcompression/response_count': 3,
 'item_scraped_count': 3,
 'log_count/DEBUG': 14,
 'log_count/INFO': 11,
 'log_count/WARNING': 3,
 'memusage/max': 66842624,
 'memusage/startup': 66842624,
 'response_received_count': 7,
 'robotstxt/request_count': 4,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 5, 3, 12, 3, 55, 834039)}
2023-05-03 12:03:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:04:57 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:04:57 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:04:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:04:57 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:04:57 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:04:57 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:04:57 [scrapy.extensions.telnet] INFO: Telnet Password: 4d9cc1649383a534
2023-05-03 12:04:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:04:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:04:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:04:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:04:57 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:04:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:04:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:04:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:04:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:04:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:04:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:04:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:04:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:04:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:04:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hawk-systems.de>
{'id': 'hawk-systems.de', 'url': 'http://hawk-systems.de', 'status': 200, 'title': 'Hawk-Systems', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 03 Jan 2019 11:59:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:04:57 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:04:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thechaosteam.de>
{'id': 'thechaosteam.de', 'url': 'http://thechaosteam.de', 'status': 200, 'title': 'Page Redirection', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 11 Jan 2016 01:01:06 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:04:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:04:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:04:58 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:04:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.nancyglor.de/>
{'id': 'nancyglor.de', 'url': 'https://www.nancyglor.de/', 'status': 200, 'title': 'Klimapositive Photographie | Leipzig | für Unternehmen und Selbständige', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'nancyglor.de', 'ssl_start': '20230717235959Z', 'ssl_expire': '20230717235959Z'}
2023-05-03 12:04:58 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:04:58 [scrapy.extensions.feedexport] INFO: Stored jl feed (3 items) in: baddata_results_20230503_120456.jl
2023-05-03 12:04:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1756,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 269669,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 0.527878,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 4, 58, 273613),
 'httpcompression/response_bytes': 1678152,
 'httpcompression/response_count': 3,
 'item_scraped_count': 3,
 'log_count/DEBUG': 14,
 'log_count/INFO': 11,
 'log_count/WARNING': 3,
 'memusage/max': 66920448,
 'memusage/startup': 66920448,
 'response_received_count': 7,
 'robotstxt/request_count': 4,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 5, 3, 12, 4, 57, 745735)}
2023-05-03 12:04:58 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:05:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:05:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:05:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:05:10 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:05:10 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:05:10 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:05:10 [scrapy.extensions.telnet] INFO: Telnet Password: bcf21bff4a092ef1
2023-05-03 12:05:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:05:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:05:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:05:10 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:05:10 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:05:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:05:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:05:10 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:05:10 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:05:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:05:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:05:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:05:10 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:05:10 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:05:10 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hawk-systems.de>
{'id': 'hawk-systems.de', 'url': 'http://hawk-systems.de', 'status': 200, 'title': 'Hawk-Systems', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 03 Jan 2019 11:59:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:05:10 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thechaosteam.de>
{'id': 'thechaosteam.de', 'url': 'http://thechaosteam.de', 'status': 200, 'title': 'Page Redirection', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 11 Jan 2016 01:01:06 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:05:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:05:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:05:10 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:05:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.nancyglor.de/>
{'id': 'nancyglor.de', 'url': 'https://www.nancyglor.de/', 'status': 200, 'title': 'Klimapositive Photographie | Leipzig | für Unternehmen und Selbständige', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'nancyglor.de', 'ssl_start': '20230717235959Z', 'ssl_expire': '20230717235959Z'}
2023-05-03 12:05:10 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:05:10 [scrapy.extensions.feedexport] INFO: Stored jl feed (3 items) in: baddata_results_20230503_120509.jl
2023-05-03 12:05:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1756,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 269666,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 0.504998,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 5, 10, 957592),
 'httpcompression/response_bytes': 1678152,
 'httpcompression/response_count': 3,
 'item_scraped_count': 3,
 'log_count/DEBUG': 14,
 'log_count/INFO': 11,
 'log_count/WARNING': 2,
 'memusage/max': 66613248,
 'memusage/startup': 66613248,
 'response_received_count': 7,
 'robotstxt/request_count': 4,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 5, 3, 12, 5, 10, 452594)}
2023-05-03 12:05:10 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:05:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:05:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:05:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:05:36 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:05:36 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:05:36 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:05:36 [scrapy.extensions.telnet] INFO: Telnet Password: 7fe3153cc0789665
2023-05-03 12:05:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:05:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:05:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:05:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:05:37 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:05:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:05:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:05:37 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:05:37 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:05:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:05:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:05:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:05:37 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:05:37 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:05:37 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hawk-systems.de>
{'id': 'hawk-systems.de', 'url': 'http://hawk-systems.de', 'status': 200, 'title': 'Hawk-Systems', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 03 Jan 2019 11:59:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:05:37 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:05:37 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thechaosteam.de>
{'id': 'thechaosteam.de', 'url': 'http://thechaosteam.de', 'status': 200, 'title': 'Page Redirection', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 11 Jan 2016 01:01:06 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:05:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:05:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:05:37 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:05:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.nancyglor.de/>
{'id': 'nancyglor.de', 'url': 'https://www.nancyglor.de/', 'status': 200, 'title': 'Klimapositive Photographie | Leipzig | für Unternehmen und Selbständige', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'nancyglor.de', 'ssl_start': '20230717235959Z', 'ssl_expire': '20230717235959Z'}
2023-05-03 12:05:37 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:05:37 [scrapy.extensions.feedexport] INFO: Stored jl feed (3 items) in: baddata_results_20230503_120536.jl
2023-05-03 12:05:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1756,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 269671,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 0.493709,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 5, 37, 664237),
 'httpcompression/response_bytes': 1678152,
 'httpcompression/response_count': 3,
 'item_scraped_count': 3,
 'log_count/DEBUG': 14,
 'log_count/INFO': 11,
 'log_count/WARNING': 3,
 'memusage/max': 66527232,
 'memusage/startup': 66527232,
 'response_received_count': 7,
 'robotstxt/request_count': 4,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 5, 3, 12, 5, 37, 170528)}
2023-05-03 12:05:37 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:05:59 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:05:59 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:05:59 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:05:59 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:05:59 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:05:59 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:05:59 [scrapy.extensions.telnet] INFO: Telnet Password: ff3d5d16ab71d327
2023-05-03 12:05:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:05:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:05:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:05:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:05:59 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:05:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:05:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:05:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:05:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:05:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:05:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:06:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:06:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:06:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:06:00 [scrapy.core.scraper] ERROR: Spider error processing <GET http://thechaosteam.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/bulkseospider/bulkseospider/spiders/spider.py", line 120, in parse
    hrefs = response.xpath('//a/@href').getall().extract()
AttributeError: 'list' object has no attribute 'extract'
2023-05-03 12:06:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:06:00 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hawk-systems.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/bulkseospider/bulkseospider/spiders/spider.py", line 120, in parse
    hrefs = response.xpath('//a/@href').getall().extract()
AttributeError: 'list' object has no attribute 'extract'
2023-05-03 12:06:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:06:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:06:00 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:06:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.nancyglor.de/> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/bulkseospider/bulkseospider/spiders/spider.py", line 120, in parse
    hrefs = response.xpath('//a/@href').getall().extract()
AttributeError: 'list' object has no attribute 'extract'
2023-05-03 12:06:00 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:06:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1756,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 269639,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 0.496035,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 6, 0, 389879),
 'httpcompression/response_bytes': 1678152,
 'httpcompression/response_count': 3,
 'log_count/DEBUG': 11,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'log_count/WARNING': 3,
 'memusage/max': 67010560,
 'memusage/startup': 67010560,
 'response_received_count': 7,
 'robotstxt/request_count': 4,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/AttributeError': 3,
 'start_time': datetime.datetime(2023, 5, 3, 12, 5, 59, 893844)}
2023-05-03 12:06:00 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:07:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:07:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:07:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:07:12 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:07:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:07:12 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:07:12 [scrapy.extensions.telnet] INFO: Telnet Password: e614bb0894d8e70f
2023-05-03 12:07:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:07:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:07:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:07:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:07:12 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:07:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:07:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:07:12 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:07:12 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:07:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:07:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:07:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:07:12 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:07:12 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:07:12 [scrapy.core.scraper] ERROR: Spider error processing <GET http://thechaosteam.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/bulkseospider/bulkseospider/spiders/spider.py", line 120, in parse
    hrefs = response.xpath('//a/@href').getall().extract()
AttributeError: 'list' object has no attribute 'extract'
2023-05-03 12:07:12 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:07:12 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hawk-systems.de> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/bulkseospider/bulkseospider/spiders/spider.py", line 120, in parse
    hrefs = response.xpath('//a/@href').getall().extract()
AttributeError: 'list' object has no attribute 'extract'
2023-05-03 12:07:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:07:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:07:13 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:07:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.nancyglor.de/> (referer: None)
Traceback (most recent call last):
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/home/pino/scrapySpiders/bulkseospider/bulkseospider/spiders/spider.py", line 120, in parse
    hrefs = response.xpath('//a/@href').getall().extract()
AttributeError: 'list' object has no attribute 'extract'
2023-05-03 12:07:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:07:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1756,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 269669,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 0.527074,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 7, 13, 43398),
 'httpcompression/response_bytes': 1678152,
 'httpcompression/response_count': 3,
 'log_count/DEBUG': 11,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'log_count/WARNING': 3,
 'memusage/max': 67010560,
 'memusage/startup': 67010560,
 'response_received_count': 7,
 'robotstxt/request_count': 4,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/AttributeError': 3,
 'start_time': datetime.datetime(2023, 5, 3, 12, 7, 12, 516324)}
2023-05-03 12:07:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:07:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:07:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:07:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:07:16 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:07:16 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:07:16 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:07:16 [scrapy.extensions.telnet] INFO: Telnet Password: e79b1392c1bcf930
2023-05-03 12:07:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:07:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:07:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:07:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:07:16 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:07:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:07:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:07:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:07:16 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:07:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:07:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:07:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:07:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:07:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:07:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thechaosteam.de>
{'id': 'thechaosteam.de', 'url': 'http://thechaosteam.de', 'status': 200, 'title': 'Page Redirection', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 11 Jan 2016 01:01:06 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:07:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:07:16 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hawk-systems.de>
{'id': 'hawk-systems.de', 'url': 'http://hawk-systems.de', 'status': 200, 'title': 'Hawk-Systems', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 03 Jan 2019 11:59:37 GMT', 'tableLayout': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:07:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:07:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:07:16 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:07:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.nancyglor.de/>
{'id': 'nancyglor.de', 'url': 'https://www.nancyglor.de/', 'status': 200, 'title': 'Klimapositive Photographie | Leipzig | für Unternehmen und Selbständige', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'ssl_name': 'nancyglor.de', 'ssl_start': '20230717235959Z', 'ssl_expire': '20230717235959Z'}
2023-05-03 12:07:16 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:07:16 [scrapy.extensions.feedexport] INFO: Stored jl feed (3 items) in: baddata_results_20230503_120715.jl
2023-05-03 12:07:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1756,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 269666,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 0.518918,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 7, 16, 940136),
 'httpcompression/response_bytes': 1678152,
 'httpcompression/response_count': 3,
 'item_scraped_count': 3,
 'log_count/DEBUG': 14,
 'log_count/INFO': 11,
 'log_count/WARNING': 3,
 'memusage/max': 66719744,
 'memusage/startup': 66719744,
 'response_received_count': 7,
 'robotstxt/request_count': 4,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 5, 3, 12, 7, 16, 421218)}
2023-05-03 12:07:16 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:15:41 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:15:41 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:15:41 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:15:41 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:15:41 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:15:41 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:15:41 [scrapy.extensions.telnet] INFO: Telnet Password: 9da95a00eae34d7d
2023-05-03 12:15:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:15:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:15:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:15:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:15:41 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:15:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:15:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:15:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:15:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:15:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:15:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:15:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:15:42 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:15:42 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:15:42 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hawk-systems.de>
{'id': 'hawk-systems.de', 'url': 'http://hawk-systems.de', 'status': 200, 'title': 'Hawk-Systems', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 03 Jan 2019 11:59:37 GMT', 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:15:42 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:15:42 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thechaosteam.de>
{'id': 'thechaosteam.de', 'url': 'http://thechaosteam.de', 'status': 200, 'title': 'Page Redirection', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 11 Jan 2016 01:01:06 GMT', 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:15:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:15:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:15:42 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:15:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.nancyglor.de/>
{'id': 'nancyglor.de', 'url': 'https://www.nancyglor.de/', 'status': 200, 'title': 'Klimapositive Photographie | Leipzig | für Unternehmen und Selbständige', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'ssl_name': 'nancyglor.de', 'ssl_start': '20230717235959Z', 'ssl_expire': '20230717235959Z'}
2023-05-03 12:15:42 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:15:42 [scrapy.extensions.feedexport] INFO: Stored jl feed (3 items) in: baddata_results_20230503_121541.jl
2023-05-03 12:15:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1756,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 269669,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 0.506407,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 15, 42, 445039),
 'httpcompression/response_bytes': 1678152,
 'httpcompression/response_count': 3,
 'item_scraped_count': 3,
 'log_count/DEBUG': 14,
 'log_count/INFO': 11,
 'log_count/WARNING': 3,
 'memusage/max': 67018752,
 'memusage/startup': 67018752,
 'response_received_count': 7,
 'robotstxt/request_count': 4,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 2,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2023, 5, 3, 12, 15, 41, 938632)}
2023-05-03 12:15:42 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:16:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:16:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:16:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:16:44 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:16:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:16:44 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:16:44 [scrapy.extensions.telnet] INFO: Telnet Password: 405329701fd15380
2023-05-03 12:16:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:16:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:16:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:16:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:16:44 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:16:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:16:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:16:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/robots.txt> from <GET http://www.herold.at/robots.txt>
2023-05-03 12:16:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/robots.txt> (referer: None)
2023-05-03 12:16:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/> from <GET http://www.herold.at>
2023-05-03 12:16:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/> (referer: None)
2023-05-03 12:16:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://thechaosteam.de/robots.txt> (referer: None)
2023-05-03 12:16:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://hawk-systems.de/robots.txt> (referer: None)
2023-05-03 12:16:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://thechaosteam.de> (referer: None)
2023-05-03 12:16:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://hawk-systems.de> (referer: None)
2023-05-03 12:16:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:16:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:16:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.herold.at/>
{'id': 'test', 'url': 'https://www.herold.at/', 'status': 200, 'title': 'Herold | Firmensuche - Bewertungen - Telefonnummern', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'ssl_name': '*.herold.at', 'ssl_start': '20230603235959Z', 'ssl_expire': '20230603235959Z'}
2023-05-03 12:16:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.nancyglor.de/> from <GET http://nancyglor.de>
2023-05-03 12:16:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:16:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://thechaosteam.de>
{'id': 'thechaosteam.de', 'url': 'http://thechaosteam.de', 'status': 200, 'title': 'Page Redirection', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Mon, 11 Jan 2016 01:01:06 GMT', 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:16:44 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:16:44 [scrapy.core.scraper] DEBUG: Scraped from <200 http://hawk-systems.de>
{'id': 'hawk-systems.de', 'url': 'http://hawk-systems.de', 'status': 200, 'title': 'Hawk-Systems', 'redirectHTTPS': False, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': 'Thu, 03 Jan 2019 11:59:37 GMT', 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'ssl_name': False, 'ssl_start': False, 'ssl_expire': False}
2023-05-03 12:16:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/robots.txt> (referer: None)
2023-05-03 12:16:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.nancyglor.de/> (referer: None)
2023-05-03 12:16:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:16:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.nancyglor.de/>
{'id': 'nancyglor.de', 'url': 'https://www.nancyglor.de/', 'status': 200, 'title': 'Klimapositive Photographie | Leipzig | für Unternehmen und Selbständige', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': True, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'ssl_name': 'nancyglor.de', 'ssl_start': '20230717235959Z', 'ssl_expire': '20230717235959Z'}
2023-05-03 12:16:45 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:16:45 [scrapy.extensions.feedexport] INFO: Stored jl feed (4 items) in: baddata_results_20230503_121643.jl
2023-05-03 12:16:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2628,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 290846,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 7,
 'downloader/response_status_count/301': 3,
 'downloader/response_status_count/404': 2,
 'elapsed_time_seconds': 0.50806,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 16, 45, 120868),
 'httpcompression/response_bytes': 1824479,
 'httpcompression/response_count': 5,
 'item_scraped_count': 4,
 'log_count/DEBUG': 19,
 'log_count/INFO': 11,
 'log_count/WARNING': 4,
 'memusage/max': 66789376,
 'memusage/startup': 66789376,
 'response_received_count': 9,
 'robotstxt/request_count': 5,
 'robotstxt/response_count': 5,
 'robotstxt/response_status_count/200': 3,
 'robotstxt/response_status_count/404': 2,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2023, 5, 3, 12, 16, 44, 612808)}
2023-05-03 12:16:45 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:17:14 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:17:14 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:17:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:17:14 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:17:14 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:17:14 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:17:14 [scrapy.extensions.telnet] INFO: Telnet Password: c81586e3d51c27b0
2023-05-03 12:17:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:17:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:17:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:17:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:17:14 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:17:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:17:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:17:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/robots.txt> from <GET http://www.herold.at/robots.txt>
2023-05-03 12:17:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/robots.txt> (referer: None)
2023-05-03 12:17:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/> from <GET http://www.herold.at>
2023-05-03 12:17:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/> (referer: None)
2023-05-03 12:17:14 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:17:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.herold.at/>
{'id': 'test', 'url': 'https://www.herold.at/', 'status': 200, 'title': 'Herold | Firmensuche - Bewertungen - Telefonnummern', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'ssl_name': '*.herold.at', 'ssl_start': '20230603235959Z', 'ssl_expire': '20230603235959Z'}
2023-05-03 12:17:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:17:14 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_121713.jl
2023-05-03 12:17:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 872,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 21175,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/301': 2,
 'elapsed_time_seconds': 0.16919,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 17, 14, 398678),
 'httpcompression/response_bytes': 146327,
 'httpcompression/response_count': 2,
 'item_scraped_count': 1,
 'log_count/DEBUG': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66715648,
 'memusage/startup': 66715648,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2023, 5, 3, 12, 17, 14, 229488)}
2023-05-03 12:17:14 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:18:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:18:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:18:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:18:31 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:18:31 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:18:31 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:18:31 [scrapy.extensions.telnet] INFO: Telnet Password: a126bcdcf4ecbb5b
2023-05-03 12:18:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:18:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:18:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:18:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:18:31 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:18:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:18:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:18:31 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/robots.txt> from <GET http://www.herold.at/robots.txt>
2023-05-03 12:18:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/robots.txt> (referer: None)
2023-05-03 12:18:31 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/> from <GET http://www.herold.at>
2023-05-03 12:18:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/> (referer: None)
2023-05-03 12:18:32 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:18:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.herold.at/>
{'id': 'test', 'url': 'https://www.herold.at/', 'status': 200, 'title': 'Herold | Firmensuche - Bewertungen - Telefonnummern', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'smXing': False, 'ssl_name': '*.herold.at', 'ssl_start': '20230603235959Z', 'ssl_expire': '20230603235959Z'}
2023-05-03 12:18:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:18:32 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_121831.jl
2023-05-03 12:18:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 872,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 21175,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/301': 2,
 'elapsed_time_seconds': 0.159049,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 18, 32, 101520),
 'httpcompression/response_bytes': 146327,
 'httpcompression/response_count': 2,
 'item_scraped_count': 1,
 'log_count/DEBUG': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66936832,
 'memusage/startup': 66936832,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2023, 5, 3, 12, 18, 31, 942471)}
2023-05-03 12:18:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:18:53 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:18:53 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:18:53 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:18:53 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:18:53 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:18:53 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:18:53 [scrapy.extensions.telnet] INFO: Telnet Password: 0c9c3680045c40f2
2023-05-03 12:18:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:18:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:18:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:18:53 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:18:53 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:18:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:18:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:18:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/robots.txt> from <GET http://www.herold.at/robots.txt>
2023-05-03 12:18:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/robots.txt> (referer: None)
2023-05-03 12:18:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/> from <GET http://www.herold.at>
2023-05-03 12:18:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/> (referer: None)
2023-05-03 12:18:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:18:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.herold.at/>
{'id': 'test', 'url': 'https://www.herold.at/', 'status': 200, 'title': 'Herold | Firmensuche - Bewertungen - Telefonnummern', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'smXing': False, 'smPinterest': False, 'ssl_name': '*.herold.at', 'ssl_start': '20230603235959Z', 'ssl_expire': '20230603235959Z'}
2023-05-03 12:18:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:18:53 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_121852.jl
2023-05-03 12:18:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 872,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 21175,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/301': 2,
 'elapsed_time_seconds': 0.159265,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 18, 53, 839743),
 'httpcompression/response_bytes': 146327,
 'httpcompression/response_count': 2,
 'item_scraped_count': 1,
 'log_count/DEBUG': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66588672,
 'memusage/startup': 66588672,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2023, 5, 3, 12, 18, 53, 680478)}
2023-05-03 12:18:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:19:08 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:19:08 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:19:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:19:08 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:19:08 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:19:08 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:19:08 [scrapy.extensions.telnet] INFO: Telnet Password: de71066bbd72befd
2023-05-03 12:19:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:19:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:19:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:19:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:19:08 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:19:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:19:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:19:08 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/robots.txt> from <GET http://www.herold.at/robots.txt>
2023-05-03 12:19:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/robots.txt> (referer: None)
2023-05-03 12:19:08 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/> from <GET http://www.herold.at>
2023-05-03 12:19:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/> (referer: None)
2023-05-03 12:19:08 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:19:08 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.herold.at/>
{'id': 'test', 'url': 'https://www.herold.at/', 'status': 200, 'title': 'Herold | Firmensuche - Bewertungen - Telefonnummern', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'smXing': False, 'smPinterest': False, 'ssl_name': '*.herold.at', 'ssl_start': '20230603235959Z', 'ssl_expire': '20230603235959Z'}
2023-05-03 12:19:08 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:19:08 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_121907.jl
2023-05-03 12:19:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 872,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 21175,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/301': 2,
 'elapsed_time_seconds': 0.167037,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 19, 8, 708304),
 'httpcompression/response_bytes': 146327,
 'httpcompression/response_count': 2,
 'item_scraped_count': 1,
 'log_count/DEBUG': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 67014656,
 'memusage/startup': 67014656,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2023, 5, 3, 12, 19, 8, 541267)}
2023-05-03 12:19:08 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:19:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:19:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:19:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:19:44 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:19:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:19:44 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:19:44 [scrapy.extensions.telnet] INFO: Telnet Password: c3d1b6265e91e939
2023-05-03 12:19:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:19:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:19:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:19:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:19:44 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:19:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:19:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:19:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/robots.txt> from <GET http://www.herold.at/robots.txt>
2023-05-03 12:19:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/robots.txt> (referer: None)
2023-05-03 12:19:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/> from <GET http://www.herold.at>
2023-05-03 12:19:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/> (referer: None)
2023-05-03 12:19:45 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:19:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.herold.at/>
{'id': 'test', 'url': 'https://www.herold.at/', 'status': 200, 'title': 'Herold | Firmensuche - Bewertungen - Telefonnummern', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'smXing': False, 'smPinterest': False, 'ssl_name': '*.herold.at', 'ssl_start': '20230603235959Z', 'ssl_expire': '20230603235959Z'}
2023-05-03 12:19:45 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:19:45 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_121943.jl
2023-05-03 12:19:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 872,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 21175,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/301': 2,
 'elapsed_time_seconds': 0.463746,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 19, 45, 208761),
 'httpcompression/response_bytes': 146327,
 'httpcompression/response_count': 2,
 'item_scraped_count': 1,
 'log_count/DEBUG': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66617344,
 'memusage/startup': 66617344,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2023, 5, 3, 12, 19, 44, 745015)}
2023-05-03 12:19:45 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:19:52 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:19:52 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:19:52 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:19:52 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:19:52 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:19:52 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:19:52 [scrapy.extensions.telnet] INFO: Telnet Password: 2feca40b195b244c
2023-05-03 12:19:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:19:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:19:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:19:53 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:19:53 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:19:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:19:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:19:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/robots.txt> from <GET http://www.herold.at/robots.txt>
2023-05-03 12:19:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/robots.txt> (referer: None)
2023-05-03 12:19:53 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/> from <GET http://www.herold.at>
2023-05-03 12:19:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/> (referer: None)
2023-05-03 12:19:53 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:19:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.herold.at/>
{'id': 'test', 'url': 'https://www.herold.at/', 'status': 200, 'title': 'Herold | Firmensuche - Bewertungen - Telefonnummern', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'smXing': False, 'smPinterest': False, 'ssl_name': '*.herold.at', 'ssl_start': '20230603235959Z', 'ssl_expire': '20230603235959Z'}
2023-05-03 12:19:53 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:19:53 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_121952.jl
2023-05-03 12:19:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 872,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 21175,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/301': 2,
 'elapsed_time_seconds': 0.162589,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 19, 53, 411317),
 'httpcompression/response_bytes': 146327,
 'httpcompression/response_count': 2,
 'item_scraped_count': 1,
 'log_count/DEBUG': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66486272,
 'memusage/startup': 66486272,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2023, 5, 3, 12, 19, 53, 248728)}
2023-05-03 12:19:53 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:20:24 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:20:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:20:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:20:25 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:20:25 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:20:25 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:20:25 [scrapy.extensions.telnet] INFO: Telnet Password: fa052110d7da0ada
2023-05-03 12:20:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:20:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:20:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:20:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:20:25 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:20:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:20:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:20:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/robots.txt> from <GET http://www.herold.at/robots.txt>
2023-05-03 12:20:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/robots.txt> (referer: None)
2023-05-03 12:20:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/> from <GET http://www.herold.at>
2023-05-03 12:20:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/> (referer: None)
2023-05-03 12:20:25 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:20:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.herold.at/>
{'id': 'test', 'url': 'https://www.herold.at/', 'status': 200, 'title': 'Herold | Firmensuche - Bewertungen - Telefonnummern', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': False, 'smInstagram': False, 'smTwitter': False, 'smYoutube': False, 'smLinkedin': False, 'smXing': False, 'smPinterest': False, 'ssl_name': '*.herold.at', 'ssl_start': '20230603235959Z', 'ssl_expire': '20230603235959Z'}
2023-05-03 12:20:25 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:20:25 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_122024.jl
2023-05-03 12:20:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 872,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 21175,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/301': 2,
 'elapsed_time_seconds': 0.181916,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 20, 25, 394444),
 'httpcompression/response_bytes': 146327,
 'httpcompression/response_count': 2,
 'item_scraped_count': 1,
 'log_count/DEBUG': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66637824,
 'memusage/startup': 66637824,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2023, 5, 3, 12, 20, 25, 212528)}
2023-05-03 12:20:25 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:20:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:20:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:20:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:20:48 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:20:48 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:20:48 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:20:48 [scrapy.extensions.telnet] INFO: Telnet Password: cea2d8e0a86f08dd
2023-05-03 12:20:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:20:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:20:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:20:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:20:48 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:20:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:20:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:20:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/robots.txt> from <GET http://www.herold.at/robots.txt>
2023-05-03 12:20:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/robots.txt> (referer: None)
2023-05-03 12:20:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/> from <GET http://www.herold.at>
2023-05-03 12:20:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/> (referer: None)
2023-05-03 12:20:48 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:20:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.herold.at/>
{'id': 'test', 'url': 'https://www.herold.at/', 'status': 200, 'title': 'Herold | Firmensuche - Bewertungen - Telefonnummern', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': True, 'smInstagram': False, 'smTwitter': False, 'smYoutube': True, 'smLinkedin': False, 'smXing': True, 'smPinterest': True, 'ssl_name': '*.herold.at', 'ssl_start': '20230603235959Z', 'ssl_expire': '20230603235959Z'}
2023-05-03 12:20:48 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:20:48 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_122047.jl
2023-05-03 12:20:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 872,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 21175,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/301': 2,
 'elapsed_time_seconds': 0.182577,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 20, 48, 684402),
 'httpcompression/response_bytes': 146327,
 'httpcompression/response_count': 2,
 'item_scraped_count': 1,
 'log_count/DEBUG': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66506752,
 'memusage/startup': 66506752,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2023, 5, 3, 12, 20, 48, 501825)}
2023-05-03 12:20:48 [scrapy.core.engine] INFO: Spider closed (finished)
2023-05-03 12:23:32 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: bulkseospider)
2023-05-03 12:23:32 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.4.0-x86_64-with-glibc2.29
2023-05-03 12:23:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'bulkseospider',
 'CONCURRENT_REQUESTS': 32,
 'DOWNLOAD_TIMEOUT': '20',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'test.log',
 'NEWSPIDER_MODULE': 'bulkseospider.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_TIMES': '1',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['bulkseospider.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-05-03 12:23:32 [asyncio] DEBUG: Using selector: EpollSelector
2023-05-03 12:23:32 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-05-03 12:23:32 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2023-05-03 12:23:32 [scrapy.extensions.telnet] INFO: Telnet Password: 3726d9876987c3b0
2023-05-03 12:23:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-05-03 12:23:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-05-03 12:23:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-05-03 12:23:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-05-03 12:23:32 [scrapy.core.engine] INFO: Spider opened
2023-05-03 12:23:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-05-03 12:23:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-03 12:23:32 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/robots.txt> from <GET http://www.herold.at/robots.txt>
2023-05-03 12:23:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/robots.txt> (referer: None)
2023-05-03 12:23:32 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.herold.at/> from <GET http://www.herold.at>
2023-05-03 12:23:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.herold.at/> (referer: None)
2023-05-03 12:23:33 [py.warnings] WARNING: /home/pino/scrapySpiders/venv/lib/python3.8/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.
  super().__init__(text=text, type=st, root=root, **kwargs)

2023-05-03 12:23:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.herold.at/>
{'id': 'test', 'url': 'https://www.herold.at/', 'status': 200, 'title': 'Herold | Firmensuche - Bewertungen - Telefonnummern', 'redirectHTTPS': True, 'badTitle': False, 'Domainparking': False, 'Maintainance': False, 'foundFlash': False, 'lastModified': None, 'tableLayout': False, 'smFacebook': True, 'smInstagram': False, 'smTwitter': False, 'smYoutube': True, 'smLinkedin': True, 'smXing': True, 'smPinterest': True, 'ssl_name': '*.herold.at', 'ssl_start': '20230603235959Z', 'ssl_expire': '20230603235959Z'}
2023-05-03 12:23:33 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-03 12:23:33 [scrapy.extensions.feedexport] INFO: Stored jl feed (1 items) in: baddata_results_20230503_122332.jl
2023-05-03 12:23:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 872,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 21175,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/301': 2,
 'elapsed_time_seconds': 0.174632,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 5, 3, 12, 23, 33, 116517),
 'httpcompression/response_bytes': 146327,
 'httpcompression/response_count': 2,
 'item_scraped_count': 1,
 'log_count/DEBUG': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'memusage/max': 66736128,
 'memusage/startup': 66736128,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2023, 5, 3, 12, 23, 32, 941885)}
2023-05-03 12:23:33 [scrapy.core.engine] INFO: Spider closed (finished)
